Premade Datasets
1. http://research.microsoft.com/en-us/um/redmond/projects/mctest/index.html
MCTest is a freely available set of 660 stories and associated questions intended for research on the machine comprehension of text.
2. http://www.gutenberg.org/wiki/Gutenberg:Offline_Catalogs
Gutenberge has a lot of books
3. https://catalog.ldc.upenn.edu/LDC2006T13
Web 1T 5-gram Version 1, contributed by Google Inc., contains English word n-grams and their observed frequency counts. 
The length of the n-grams ranges from unigrams (single words) to five-grams. 
This data is expected to be useful for statistical language modeling, e.g., for machine translation or speech recognition, etc.
4. http://www.iesl.cs.umass.edu/data
A lot of datasets
5. http://webdatacommons.org/webtables/
A subset of the HTML tables on the Web contains relational data which can be useful for various applications. 
The Web Data Commons project has extracted two large corpora of relational Web tables from the Common Crawl and offers them for public download. 
This page provides an overview of the corpora as well as their use cases.
6. http://statmt.org/ngrams/
Unpruend Unpruned 5-gram counts and language models trained on 9 billion web pages -- Large amounts of raw data in many languages 
7. https://en.wikipedia.org/wiki/Wikipedia:Database_download
Wikipedia Database Download
8. https://aws.amazon.com/ko/datasets/google-books-ngrams/
A data set containing Google Books n-gram corpora.
9. https://aws.amazon.com/ko/public-datasets/common-crawl/
The Common Crawl corpus includes web crawl data collected over 8 years. 
Common Crawl offers the largest, most comprehensive, open repository of web crawl data on the cloud.
10. http://commoncrawl.org/the-data/tutorials/
착한 아이들 ㅋㅋ
11. https://wikireverse.org/data
The full dataset of 36 million links can be downloaded as a torrent. 
The download is a tarball containing 4 tab-delimited files. 
The data is 1.1 GB when compressed and 5.4 GB when extracted.
12. https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html
This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts.
13. https://www.uow.edu.au/~dlee/corpora.htm
several dozens of english corpus
14. http://research.google.com/research-outreach.html#/research-outreach/research-datasets
Google Datasets
15. http://www.cs.cornell.edu/home/llee/data/
Collection of Cornell Datasets
16. https://github.com/rkadlec/ubuntu-ranking-dataset-creator
Ubuntu Dialogue Datasets
17. http://ebiquity.umbc.edu/resource/html/id/351
The UMBC webBase corpus (http://ebiq.org/r/351) is a dataset containing a collection of English paragraphs with over three billion words 
processed from the February 2007 crawl from the Stanford WebBase project (http://bit.ly/WebBase). Compressed, it is about 13GB in size.



Movie Subtitles Datasets (BE AWARE OF COPYRIGHTS!!!)

http://www.opensubtitles.org/en/search
https://subscene.com/
http://www.moviesubtitles.org/
http://www.divxsubtitles.net/
http://www.subs4free.com/

https://videoconverter.iskysoft.com/video-tips/download-subtitles.html (15 Best Subtitle Software and Top 10 Subtitle Download Sites)



Q&A Datasets
https://www.researchgate.net/post/What_are_the_datasets_available_for_question_answering_system
https://archive.org/details/stackexchange
https://rajpurkar.github.io/SQuAD-explorer/
https://www.quora.com/Datasets-How-can-I-get-corpus-of-a-question-answering-website-like-Quora-or-Yahoo-Answers-or-Stack-Overflow-for-analyzing-answer-quality
http://jmcauley.ucsd.edu/data/amazon/qa/


A lot of Datasets
https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/
https://github.com/caesar0301/awesome-public-datasets#natural-language


Miscellaneous
https://github.com/deepmind/rc-data
http://u.cs.biu.ac.il/~koppel/BlogCorpus.htm
http://wiki.dbpedia.org/Downloads2015-10
https://aws.amazon.com/ko/datasets/google-books-ngrams/