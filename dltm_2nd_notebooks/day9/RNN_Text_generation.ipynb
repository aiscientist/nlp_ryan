{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b', 'a', 'y', 'o', 'I', ' ', 'm'}\n"
     ]
    }
   ],
   "source": [
    "text = \"I am a boy\"\n",
    "print(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "corpus = \"\"\n",
    "with open('shakespeare.txt', 'r') as f:\n",
    "    corpus += f.read()\n",
    "corpus = corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w', 'v', '3', \"'\", 'p', 'f', 'u', ' ', 'r', 't', 'k', ':', '&', 'e', '!', 'x', 'g', 'l', 'b', '-', 'a', 'q', ';', '.', 'c', '\\n', 'y', 'i', 'd', 'h', 's', 'o', 'z', 'n', ',', '$', 'j', '?', 'm']\n"
     ]
    }
   ],
   "source": [
    "# Construct character vocabulary\n",
    "vocab = list(set(corpus))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def vocab_encoding(corpus, vocab):\n",
    "    output = np.zeros((len(corpus), len(vocab)))\n",
    "    \n",
    "    cnt = 0\n",
    "    for char in corpus:\n",
    "        v = [0.0] * len(vocab)\n",
    "        v[vocab.index(char)] = 1.0\n",
    "        output[cnt, :] = v\n",
    "        cnt += 1\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = vocab_encoding(corpus=corpus, vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Model structure parameters\n",
    "in_size = len(vocab) # Size of input vectors at each time step\n",
    "hidden_size = 64 # Size of hidden state vector\n",
    "num_layers = 1 # Number of hidden layers\n",
    "out_size = len(vocab) # Size of output vectors at each time step\n",
    "\n",
    "learning_rate = 0.001 # Learning rate\n",
    "\n",
    "# Data and train parameters\n",
    "batch_size = 64 # Training batch size\n",
    "time_steps = 50 # (Maximum) number of time steps in each batch\n",
    "num_epochs = 10000\n",
    "display_interval = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Make the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.1. Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Placeholder for inputs: shape [batch_size, timesteps, in_size]\n",
    "X = tf.placeholder(tf.float32, shape=[None, None, in_size], name='input_X')\n",
    "\n",
    "# Placeholder for outputs: shape [batch_size, timesteps, in_size]\n",
    "Y = tf.placeholder(tf.float32, shape=[None, None, out_size], name='target_Y')\n",
    "\n",
    "# Placeholder for initial state\n",
    "state_size = num_layers * 2 * hidden_size\n",
    "hidden_init = tf.placeholder(tf.float32, shape=[None, state_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.2. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x131af2390>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "hidden_cells = [rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=False) for i in range(num_layers)]\n",
    "hidden = rnn.MultiRNNCell(hidden_cells, state_is_tuple=False)\n",
    "\n",
    "outputs, hidden_new_state = tf.nn.dynamic_rnn(cell=hidden, \n",
    "                                              inputs=X, \n",
    "                                              initial_state=hidden_init, \n",
    "                                              dtype=tf.float32)\n",
    "\n",
    "W = tf.get_variable(name='weights', \n",
    "                    shape=[hidden_size, out_size], \n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable(name='biases', \n",
    "                    shape=[out_size], \n",
    "                    initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "outputs_reshaped = tf.reshape(outputs, [-1, hidden_size])\n",
    "logits = tf.nn.xw_plus_b(x=outputs_reshaped, weights=W, biases=b, name='logits')\n",
    "\n",
    "batch_time_shape = tf.shape(outputs)\n",
    "outputs_activated = tf.reshape(tensor=tf.nn.softmax(logits), \n",
    "                               shape=[batch_time_shape[0], batch_time_shape[1], out_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_X:0\", shape=(?, ?, 39), dtype=float32)\n",
      "Tensor(\"target_Y:0\", shape=(?, ?, 39), dtype=float32)\n",
      "Tensor(\"rnn/transpose:0\", shape=(?, ?, 64), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"logits:0\", shape=(?, 39), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "print(outputs)\n",
    "print(outputs_reshaped)\n",
    "\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.3. Cost and training operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y_batch_flatten = tf.reshape(Y, [-1, out_size])\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_batch_flatten))\n",
    "train_op = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. Run the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Declare the session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialize all variables\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.1. Make batches and train them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_x = np.zeros([batch_size, time_steps, in_size])\n",
    "batch_y = np.zeros([batch_size, time_steps, in_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 \tcost:  3.66473\n",
      "epoch:  20 \tcost:  3.66031\n",
      "epoch:  40 \tcost:  3.6536\n",
      "epoch:  60 \tcost:  3.63497\n",
      "epoch:  80 \tcost:  3.59763\n",
      "epoch:  100 \tcost:  3.49742\n",
      "epoch:  120 \tcost:  3.2099\n",
      "epoch:  140 \tcost:  3.1362\n",
      "epoch:  160 \tcost:  3.08786\n",
      "epoch:  180 \tcost:  3.0691\n",
      "epoch:  200 \tcost:  3.05458\n",
      "epoch:  220 \tcost:  3.02659\n",
      "epoch:  240 \tcost:  2.97358\n",
      "epoch:  260 \tcost:  2.93546\n",
      "epoch:  280 \tcost:  2.90337\n",
      "epoch:  300 \tcost:  2.88383\n",
      "epoch:  320 \tcost:  2.8073\n",
      "epoch:  340 \tcost:  2.73717\n",
      "epoch:  360 \tcost:  2.71905\n",
      "epoch:  380 \tcost:  2.63051\n",
      "epoch:  400 \tcost:  2.63928\n",
      "epoch:  420 \tcost:  2.59724\n",
      "epoch:  440 \tcost:  2.56152\n",
      "epoch:  460 \tcost:  2.57654\n",
      "epoch:  480 \tcost:  2.531\n",
      "epoch:  500 \tcost:  2.47867\n",
      "epoch:  520 \tcost:  2.47099\n",
      "epoch:  540 \tcost:  2.49559\n",
      "epoch:  560 \tcost:  2.43446\n",
      "epoch:  580 \tcost:  2.4354\n",
      "epoch:  600 \tcost:  2.38725\n",
      "epoch:  620 \tcost:  2.40215\n",
      "epoch:  640 \tcost:  2.38854\n",
      "epoch:  660 \tcost:  2.40684\n",
      "epoch:  680 \tcost:  2.36739\n",
      "epoch:  700 \tcost:  2.33774\n",
      "epoch:  720 \tcost:  2.34472\n",
      "epoch:  740 \tcost:  2.35047\n",
      "epoch:  760 \tcost:  2.31186\n",
      "epoch:  780 \tcost:  2.3225\n",
      "epoch:  800 \tcost:  2.3398\n",
      "epoch:  820 \tcost:  2.30304\n",
      "epoch:  840 \tcost:  2.31881\n",
      "epoch:  860 \tcost:  2.30023\n",
      "epoch:  880 \tcost:  2.28904\n",
      "epoch:  900 \tcost:  2.27179\n",
      "epoch:  920 \tcost:  2.29017\n",
      "epoch:  940 \tcost:  2.29783\n",
      "epoch:  960 \tcost:  2.27673\n",
      "epoch:  980 \tcost:  2.27058\n",
      "epoch:  1000 \tcost:  2.23376\n",
      "epoch:  1020 \tcost:  2.2232\n",
      "epoch:  1040 \tcost:  2.20301\n",
      "epoch:  1060 \tcost:  2.24142\n",
      "epoch:  1080 \tcost:  2.22869\n",
      "epoch:  1100 \tcost:  2.22624\n",
      "epoch:  1120 \tcost:  2.23577\n",
      "epoch:  1140 \tcost:  2.16538\n",
      "epoch:  1160 \tcost:  2.18568\n",
      "epoch:  1180 \tcost:  2.1869\n",
      "epoch:  1200 \tcost:  2.15897\n",
      "epoch:  1220 \tcost:  2.22197\n",
      "epoch:  1240 \tcost:  2.18892\n",
      "epoch:  1260 \tcost:  2.14984\n",
      "epoch:  1280 \tcost:  2.16838\n",
      "epoch:  1300 \tcost:  2.20359\n",
      "epoch:  1320 \tcost:  2.14289\n",
      "epoch:  1340 \tcost:  2.16973\n",
      "epoch:  1360 \tcost:  2.18557\n",
      "epoch:  1380 \tcost:  2.13588\n",
      "epoch:  1400 \tcost:  2.14797\n",
      "epoch:  1420 \tcost:  2.12819\n",
      "epoch:  1440 \tcost:  2.14294\n",
      "epoch:  1460 \tcost:  2.15826\n",
      "epoch:  1480 \tcost:  2.13978\n",
      "epoch:  1500 \tcost:  2.18648\n",
      "epoch:  1520 \tcost:  2.1272\n",
      "epoch:  1540 \tcost:  2.11897\n",
      "epoch:  1560 \tcost:  2.14737\n",
      "epoch:  1580 \tcost:  2.11469\n",
      "epoch:  1600 \tcost:  2.09246\n",
      "epoch:  1620 \tcost:  2.11182\n",
      "epoch:  1640 \tcost:  2.13062\n",
      "epoch:  1660 \tcost:  2.12671\n",
      "epoch:  1680 \tcost:  2.14128\n",
      "epoch:  1700 \tcost:  2.10066\n",
      "epoch:  1720 \tcost:  2.11988\n",
      "epoch:  1740 \tcost:  2.09968\n",
      "epoch:  1760 \tcost:  2.0614\n",
      "epoch:  1780 \tcost:  2.08229\n",
      "epoch:  1800 \tcost:  2.09422\n",
      "epoch:  1820 \tcost:  2.0572\n",
      "epoch:  1840 \tcost:  2.09026\n",
      "epoch:  1860 \tcost:  2.131\n",
      "epoch:  1880 \tcost:  2.09226\n",
      "epoch:  1900 \tcost:  2.07138\n",
      "epoch:  1920 \tcost:  2.08775\n",
      "epoch:  1940 \tcost:  2.06057\n",
      "epoch:  1960 \tcost:  2.11953\n",
      "epoch:  1980 \tcost:  2.0639\n",
      "epoch:  2000 \tcost:  2.02818\n",
      "epoch:  2020 \tcost:  2.06021\n",
      "epoch:  2040 \tcost:  2.06152\n",
      "epoch:  2060 \tcost:  2.04344\n",
      "epoch:  2080 \tcost:  2.05719\n",
      "epoch:  2100 \tcost:  2.08068\n",
      "epoch:  2120 \tcost:  2.08095\n",
      "epoch:  2140 \tcost:  2.08459\n",
      "epoch:  2160 \tcost:  1.99483\n",
      "epoch:  2180 \tcost:  2.07492\n",
      "epoch:  2200 \tcost:  2.07539\n",
      "epoch:  2220 \tcost:  2.05353\n",
      "epoch:  2240 \tcost:  2.00435\n",
      "epoch:  2260 \tcost:  2.02018\n",
      "epoch:  2280 \tcost:  2.01512\n",
      "epoch:  2300 \tcost:  2.08836\n",
      "epoch:  2320 \tcost:  2.02459\n",
      "epoch:  2340 \tcost:  2.00335\n",
      "epoch:  2360 \tcost:  2.01657\n",
      "epoch:  2380 \tcost:  2.03669\n",
      "epoch:  2400 \tcost:  2.04541\n",
      "epoch:  2420 \tcost:  2.03704\n",
      "epoch:  2440 \tcost:  2.05089\n",
      "epoch:  2460 \tcost:  2.06748\n",
      "epoch:  2480 \tcost:  2.01556\n",
      "epoch:  2500 \tcost:  2.00546\n",
      "epoch:  2520 \tcost:  2.00519\n",
      "epoch:  2540 \tcost:  2.05213\n",
      "epoch:  2560 \tcost:  1.94399\n",
      "epoch:  2580 \tcost:  2.03277\n",
      "epoch:  2600 \tcost:  2.01903\n",
      "epoch:  2620 \tcost:  2.02654\n",
      "epoch:  2640 \tcost:  1.98944\n",
      "epoch:  2660 \tcost:  2.0007\n",
      "epoch:  2680 \tcost:  1.94833\n",
      "epoch:  2700 \tcost:  1.98293\n",
      "epoch:  2720 \tcost:  2.00083\n",
      "epoch:  2740 \tcost:  1.99919\n",
      "epoch:  2760 \tcost:  1.98617\n",
      "epoch:  2780 \tcost:  1.97921\n",
      "epoch:  2800 \tcost:  2.03254\n",
      "epoch:  2820 \tcost:  1.99965\n",
      "epoch:  2840 \tcost:  2.01957\n",
      "epoch:  2860 \tcost:  2.03704\n",
      "epoch:  2880 \tcost:  1.97359\n",
      "epoch:  2900 \tcost:  2.02672\n",
      "epoch:  2920 \tcost:  2.02559\n",
      "epoch:  2940 \tcost:  1.96671\n",
      "epoch:  2960 \tcost:  1.96302\n",
      "epoch:  2980 \tcost:  1.92749\n",
      "epoch:  3000 \tcost:  1.96946\n",
      "epoch:  3020 \tcost:  1.97951\n",
      "epoch:  3040 \tcost:  1.98463\n",
      "epoch:  3060 \tcost:  1.96013\n",
      "epoch:  3080 \tcost:  2.02518\n",
      "epoch:  3100 \tcost:  1.95858\n",
      "epoch:  3120 \tcost:  1.98025\n",
      "epoch:  3140 \tcost:  1.95386\n",
      "epoch:  3160 \tcost:  1.99601\n",
      "epoch:  3180 \tcost:  1.96854\n",
      "epoch:  3200 \tcost:  1.96625\n",
      "epoch:  3220 \tcost:  1.96455\n",
      "epoch:  3240 \tcost:  1.98425\n",
      "epoch:  3260 \tcost:  1.97641\n",
      "epoch:  3280 \tcost:  1.95453\n",
      "epoch:  3300 \tcost:  1.99764\n",
      "epoch:  3320 \tcost:  1.95418\n",
      "epoch:  3340 \tcost:  1.94615\n",
      "epoch:  3360 \tcost:  1.95177\n",
      "epoch:  3380 \tcost:  1.95457\n",
      "epoch:  3400 \tcost:  1.90483\n",
      "epoch:  3420 \tcost:  1.94033\n",
      "epoch:  3440 \tcost:  1.9267\n",
      "epoch:  3460 \tcost:  1.97781\n",
      "epoch:  3480 \tcost:  1.90801\n",
      "epoch:  3500 \tcost:  1.95361\n",
      "epoch:  3520 \tcost:  1.95933\n",
      "epoch:  3540 \tcost:  1.94979\n",
      "epoch:  3560 \tcost:  1.91947\n",
      "epoch:  3580 \tcost:  1.94127\n",
      "epoch:  3600 \tcost:  1.93304\n",
      "epoch:  3620 \tcost:  1.95124\n",
      "epoch:  3640 \tcost:  1.93957\n",
      "epoch:  3660 \tcost:  1.9691\n",
      "epoch:  3680 \tcost:  1.9064\n",
      "epoch:  3700 \tcost:  1.91847\n",
      "epoch:  3720 \tcost:  1.97159\n",
      "epoch:  3740 \tcost:  1.95258\n",
      "epoch:  3760 \tcost:  1.93301\n",
      "epoch:  3780 \tcost:  1.92493\n",
      "epoch:  3800 \tcost:  1.93608\n",
      "epoch:  3820 \tcost:  1.91781\n",
      "epoch:  3840 \tcost:  1.91718\n",
      "epoch:  3860 \tcost:  1.92163\n",
      "epoch:  3880 \tcost:  1.89859\n",
      "epoch:  3900 \tcost:  1.93879\n",
      "epoch:  3920 \tcost:  1.94561\n",
      "epoch:  3940 \tcost:  1.89841\n",
      "epoch:  3960 \tcost:  1.94835\n",
      "epoch:  3980 \tcost:  1.8996\n",
      "epoch:  4000 \tcost:  1.93459\n",
      "epoch:  4020 \tcost:  1.8836\n",
      "epoch:  4040 \tcost:  1.94573\n",
      "epoch:  4060 \tcost:  1.9048\n",
      "epoch:  4080 \tcost:  1.90603\n",
      "epoch:  4100 \tcost:  1.89394\n",
      "epoch:  4120 \tcost:  1.90416\n",
      "epoch:  4140 \tcost:  1.84732\n",
      "epoch:  4160 \tcost:  1.88106\n",
      "epoch:  4180 \tcost:  1.92045\n",
      "epoch:  4200 \tcost:  1.92716\n",
      "epoch:  4220 \tcost:  1.91447\n",
      "epoch:  4240 \tcost:  1.8712\n",
      "epoch:  4260 \tcost:  1.90307\n",
      "epoch:  4280 \tcost:  1.916\n",
      "epoch:  4300 \tcost:  1.89818\n",
      "epoch:  4320 \tcost:  1.89886\n",
      "epoch:  4340 \tcost:  1.91611\n",
      "epoch:  4360 \tcost:  1.81737\n",
      "epoch:  4380 \tcost:  1.87412\n",
      "epoch:  4400 \tcost:  1.92302\n",
      "epoch:  4420 \tcost:  1.93218\n",
      "epoch:  4440 \tcost:  1.87511\n",
      "epoch:  4460 \tcost:  1.87344\n",
      "epoch:  4480 \tcost:  1.8812\n",
      "epoch:  4500 \tcost:  1.86516\n",
      "epoch:  4520 \tcost:  1.9275\n",
      "epoch:  4540 \tcost:  1.89333\n",
      "epoch:  4560 \tcost:  1.84971\n",
      "epoch:  4580 \tcost:  1.88428\n",
      "epoch:  4600 \tcost:  1.91837\n",
      "epoch:  4620 \tcost:  1.90955\n",
      "epoch:  4640 \tcost:  1.92951\n",
      "epoch:  4660 \tcost:  1.86737\n",
      "epoch:  4680 \tcost:  1.87867\n",
      "epoch:  4700 \tcost:  1.87402\n",
      "epoch:  4720 \tcost:  1.88754\n",
      "epoch:  4740 \tcost:  1.86749\n",
      "epoch:  4760 \tcost:  1.83221\n",
      "epoch:  4780 \tcost:  1.82924\n",
      "epoch:  4800 \tcost:  1.84646\n",
      "epoch:  4820 \tcost:  1.84053\n",
      "epoch:  4840 \tcost:  1.87336\n",
      "epoch:  4860 \tcost:  1.82388\n",
      "epoch:  4880 \tcost:  1.87492\n",
      "epoch:  4900 \tcost:  1.8413\n",
      "epoch:  4920 \tcost:  1.83836\n",
      "epoch:  4940 \tcost:  1.85106\n",
      "epoch:  4960 \tcost:  1.90571\n",
      "epoch:  4980 \tcost:  1.82826\n",
      "epoch:  5000 \tcost:  1.90422\n",
      "epoch:  5020 \tcost:  1.874\n",
      "epoch:  5040 \tcost:  1.84374\n",
      "epoch:  5060 \tcost:  1.86022\n",
      "epoch:  5080 \tcost:  1.87339\n",
      "epoch:  5100 \tcost:  1.87522\n",
      "epoch:  5120 \tcost:  1.79874\n",
      "epoch:  5140 \tcost:  1.86821\n",
      "epoch:  5160 \tcost:  1.84657\n",
      "epoch:  5180 \tcost:  1.86334\n",
      "epoch:  5200 \tcost:  1.83276\n",
      "epoch:  5220 \tcost:  1.79117\n",
      "epoch:  5240 \tcost:  1.82409\n",
      "epoch:  5260 \tcost:  1.82495\n",
      "epoch:  5280 \tcost:  1.82759\n",
      "epoch:  5300 \tcost:  1.90115\n",
      "epoch:  5320 \tcost:  1.82316\n",
      "epoch:  5340 \tcost:  1.86282\n",
      "epoch:  5360 \tcost:  1.84402\n",
      "epoch:  5380 \tcost:  1.8855\n",
      "epoch:  5400 \tcost:  1.85352\n",
      "epoch:  5420 \tcost:  1.90438\n",
      "epoch:  5440 \tcost:  1.83967\n",
      "epoch:  5460 \tcost:  1.85342\n",
      "epoch:  5480 \tcost:  1.84918\n",
      "epoch:  5500 \tcost:  1.80243\n",
      "epoch:  5520 \tcost:  1.80923\n",
      "epoch:  5540 \tcost:  1.80836\n",
      "epoch:  5560 \tcost:  1.81\n",
      "epoch:  5580 \tcost:  1.86277\n",
      "epoch:  5600 \tcost:  1.86576\n",
      "epoch:  5620 \tcost:  1.82464\n",
      "epoch:  5640 \tcost:  1.88053\n",
      "epoch:  5660 \tcost:  1.83629\n",
      "epoch:  5680 \tcost:  1.88211\n",
      "epoch:  5700 \tcost:  1.88304\n",
      "epoch:  5720 \tcost:  1.82152\n",
      "epoch:  5740 \tcost:  1.84179\n",
      "epoch:  5760 \tcost:  1.87378\n",
      "epoch:  5780 \tcost:  1.82861\n",
      "epoch:  5800 \tcost:  1.84798\n",
      "epoch:  5820 \tcost:  1.87435\n",
      "epoch:  5840 \tcost:  1.85131\n",
      "epoch:  5860 \tcost:  1.83035\n",
      "epoch:  5880 \tcost:  1.86867\n",
      "epoch:  5900 \tcost:  1.80165\n",
      "epoch:  5920 \tcost:  1.83502\n",
      "epoch:  5940 \tcost:  1.82133\n",
      "epoch:  5960 \tcost:  1.84605\n",
      "epoch:  5980 \tcost:  1.81907\n",
      "epoch:  6000 \tcost:  1.84216\n",
      "epoch:  6020 \tcost:  1.84554\n",
      "epoch:  6040 \tcost:  1.83132\n",
      "epoch:  6060 \tcost:  1.83472\n",
      "epoch:  6080 \tcost:  1.82146\n",
      "epoch:  6100 \tcost:  1.82799\n",
      "epoch:  6120 \tcost:  1.78584\n",
      "epoch:  6140 \tcost:  1.86023\n",
      "epoch:  6160 \tcost:  1.81055\n",
      "epoch:  6180 \tcost:  1.82122\n",
      "epoch:  6200 \tcost:  1.8168\n",
      "epoch:  6220 \tcost:  1.79762\n",
      "epoch:  6240 \tcost:  1.8203\n",
      "epoch:  6260 \tcost:  1.88331\n",
      "epoch:  6280 \tcost:  1.81978\n",
      "epoch:  6300 \tcost:  1.81062\n",
      "epoch:  6320 \tcost:  1.85846\n",
      "epoch:  6340 \tcost:  1.83998\n",
      "epoch:  6360 \tcost:  1.82304\n",
      "epoch:  6380 \tcost:  1.81574\n",
      "epoch:  6400 \tcost:  1.82904\n",
      "epoch:  6420 \tcost:  1.77544\n",
      "epoch:  6440 \tcost:  1.78313\n",
      "epoch:  6460 \tcost:  1.80464\n",
      "epoch:  6480 \tcost:  1.77889\n",
      "epoch:  6500 \tcost:  1.80804\n",
      "epoch:  6520 \tcost:  1.80011\n",
      "epoch:  6540 \tcost:  1.80117\n",
      "epoch:  6560 \tcost:  1.83678\n",
      "epoch:  6580 \tcost:  1.80695\n",
      "epoch:  6600 \tcost:  1.82456\n",
      "epoch:  6620 \tcost:  1.87432\n",
      "epoch:  6640 \tcost:  1.79675\n",
      "epoch:  6660 \tcost:  1.8598\n",
      "epoch:  6680 \tcost:  1.88529\n",
      "epoch:  6700 \tcost:  1.76191\n",
      "epoch:  6720 \tcost:  1.78814\n",
      "epoch:  6740 \tcost:  1.82089\n",
      "epoch:  6760 \tcost:  1.77458\n",
      "epoch:  6780 \tcost:  1.80586\n",
      "epoch:  6800 \tcost:  1.83329\n",
      "epoch:  6820 \tcost:  1.82882\n",
      "epoch:  6840 \tcost:  1.82458\n",
      "epoch:  6860 \tcost:  1.78615\n",
      "epoch:  6880 \tcost:  1.81286\n",
      "epoch:  6900 \tcost:  1.78541\n",
      "epoch:  6920 \tcost:  1.81928\n",
      "epoch:  6940 \tcost:  1.81628\n",
      "epoch:  6960 \tcost:  1.80345\n",
      "epoch:  6980 \tcost:  1.81404\n",
      "epoch:  7000 \tcost:  1.77842\n",
      "epoch:  7020 \tcost:  1.8068\n",
      "epoch:  7040 \tcost:  1.77517\n",
      "epoch:  7060 \tcost:  1.81418\n",
      "epoch:  7080 \tcost:  1.80561\n",
      "epoch:  7100 \tcost:  1.76074\n",
      "epoch:  7120 \tcost:  1.82509\n",
      "epoch:  7140 \tcost:  1.75261\n",
      "epoch:  7160 \tcost:  1.78792\n",
      "epoch:  7180 \tcost:  1.8007\n",
      "epoch:  7200 \tcost:  1.74724\n",
      "epoch:  7220 \tcost:  1.76516\n",
      "epoch:  7240 \tcost:  1.76185\n",
      "epoch:  7260 \tcost:  1.76913\n",
      "epoch:  7280 \tcost:  1.75326\n",
      "epoch:  7300 \tcost:  1.76237\n",
      "epoch:  7320 \tcost:  1.81477\n",
      "epoch:  7340 \tcost:  1.80485\n",
      "epoch:  7360 \tcost:  1.74646\n",
      "epoch:  7380 \tcost:  1.76633\n",
      "epoch:  7400 \tcost:  1.81746\n",
      "epoch:  7420 \tcost:  1.81237\n",
      "epoch:  7440 \tcost:  1.77089\n",
      "epoch:  7460 \tcost:  1.80515\n",
      "epoch:  7480 \tcost:  1.74349\n",
      "epoch:  7500 \tcost:  1.7821\n",
      "epoch:  7520 \tcost:  1.76189\n",
      "epoch:  7540 \tcost:  1.75876\n",
      "epoch:  7560 \tcost:  1.77643\n",
      "epoch:  7580 \tcost:  1.81163\n",
      "epoch:  7600 \tcost:  1.74302\n",
      "epoch:  7620 \tcost:  1.78742\n",
      "epoch:  7640 \tcost:  1.75769\n",
      "epoch:  7660 \tcost:  1.74245\n",
      "epoch:  7680 \tcost:  1.8294\n",
      "epoch:  7700 \tcost:  1.76112\n",
      "epoch:  7720 \tcost:  1.78791\n",
      "epoch:  7740 \tcost:  1.8312\n",
      "epoch:  7760 \tcost:  1.74781\n",
      "epoch:  7780 \tcost:  1.79044\n",
      "epoch:  7800 \tcost:  1.78012\n",
      "epoch:  7820 \tcost:  1.77585\n",
      "epoch:  7840 \tcost:  1.78187\n",
      "epoch:  7860 \tcost:  1.78362\n",
      "epoch:  7880 \tcost:  1.79766\n",
      "epoch:  7900 \tcost:  1.80149\n",
      "epoch:  7920 \tcost:  1.72354\n",
      "epoch:  7940 \tcost:  1.80303\n",
      "epoch:  7960 \tcost:  1.781\n",
      "epoch:  7980 \tcost:  1.7404\n",
      "epoch:  8000 \tcost:  1.78958\n",
      "epoch:  8020 \tcost:  1.77713\n",
      "epoch:  8040 \tcost:  1.73624\n",
      "epoch:  8060 \tcost:  1.73612\n",
      "epoch:  8080 \tcost:  1.81239\n",
      "epoch:  8100 \tcost:  1.77972\n",
      "epoch:  8120 \tcost:  1.78671\n",
      "epoch:  8140 \tcost:  1.76234\n",
      "epoch:  8160 \tcost:  1.7919\n",
      "epoch:  8180 \tcost:  1.76491\n",
      "epoch:  8200 \tcost:  1.76008\n",
      "epoch:  8220 \tcost:  1.79351\n",
      "epoch:  8240 \tcost:  1.76893\n",
      "epoch:  8260 \tcost:  1.7088\n",
      "epoch:  8280 \tcost:  1.74238\n",
      "epoch:  8300 \tcost:  1.75826\n",
      "epoch:  8320 \tcost:  1.71829\n",
      "epoch:  8340 \tcost:  1.77866\n",
      "epoch:  8360 \tcost:  1.81202\n",
      "epoch:  8380 \tcost:  1.79092\n",
      "epoch:  8400 \tcost:  1.75834\n",
      "epoch:  8420 \tcost:  1.76856\n",
      "epoch:  8440 \tcost:  1.74029\n",
      "epoch:  8460 \tcost:  1.76132\n",
      "epoch:  8480 \tcost:  1.71914\n",
      "epoch:  8500 \tcost:  1.71843\n",
      "epoch:  8520 \tcost:  1.80377\n",
      "epoch:  8540 \tcost:  1.76838\n",
      "epoch:  8560 \tcost:  1.76754\n",
      "epoch:  8580 \tcost:  1.74447\n",
      "epoch:  8600 \tcost:  1.77722\n",
      "epoch:  8620 \tcost:  1.72962\n",
      "epoch:  8640 \tcost:  1.75711\n",
      "epoch:  8660 \tcost:  1.75412\n",
      "epoch:  8680 \tcost:  1.72104\n",
      "epoch:  8700 \tcost:  1.72862\n",
      "epoch:  8720 \tcost:  1.77526\n",
      "epoch:  8740 \tcost:  1.72207\n",
      "epoch:  8760 \tcost:  1.73428\n",
      "epoch:  8780 \tcost:  1.68284\n",
      "epoch:  8800 \tcost:  1.72348\n",
      "epoch:  8820 \tcost:  1.75446\n",
      "epoch:  8840 \tcost:  1.72559\n",
      "epoch:  8860 \tcost:  1.81563\n",
      "epoch:  8880 \tcost:  1.81029\n",
      "epoch:  8900 \tcost:  1.78145\n",
      "epoch:  8920 \tcost:  1.73679\n",
      "epoch:  8940 \tcost:  1.75661\n",
      "epoch:  8960 \tcost:  1.78837\n",
      "epoch:  8980 \tcost:  1.73479\n",
      "epoch:  9000 \tcost:  1.71803\n",
      "epoch:  9020 \tcost:  1.78493\n",
      "epoch:  9040 \tcost:  1.79701\n",
      "epoch:  9060 \tcost:  1.76386\n",
      "epoch:  9080 \tcost:  1.73173\n",
      "epoch:  9100 \tcost:  1.69187\n",
      "epoch:  9120 \tcost:  1.75785\n",
      "epoch:  9140 \tcost:  1.76233\n",
      "epoch:  9160 \tcost:  1.74785\n",
      "epoch:  9180 \tcost:  1.75666\n",
      "epoch:  9200 \tcost:  1.68649\n",
      "epoch:  9220 \tcost:  1.74305\n",
      "epoch:  9240 \tcost:  1.70134\n",
      "epoch:  9260 \tcost:  1.70231\n",
      "epoch:  9280 \tcost:  1.77038\n",
      "epoch:  9300 \tcost:  1.67365\n",
      "epoch:  9320 \tcost:  1.74164\n",
      "epoch:  9340 \tcost:  1.73248\n",
      "epoch:  9360 \tcost:  1.72377\n",
      "epoch:  9380 \tcost:  1.75897\n",
      "epoch:  9400 \tcost:  1.79074\n",
      "epoch:  9420 \tcost:  1.7557\n",
      "epoch:  9440 \tcost:  1.71545\n",
      "epoch:  9460 \tcost:  1.77146\n",
      "epoch:  9480 \tcost:  1.71936\n",
      "epoch:  9500 \tcost:  1.67289\n",
      "epoch:  9520 \tcost:  1.70692\n",
      "epoch:  9540 \tcost:  1.75736\n",
      "epoch:  9560 \tcost:  1.72747\n",
      "epoch:  9580 \tcost:  1.73467\n",
      "epoch:  9600 \tcost:  1.7415\n",
      "epoch:  9620 \tcost:  1.73519\n",
      "epoch:  9640 \tcost:  1.74813\n",
      "epoch:  9660 \tcost:  1.70022\n",
      "epoch:  9680 \tcost:  1.72669\n",
      "epoch:  9700 \tcost:  1.75784\n",
      "epoch:  9720 \tcost:  1.67071\n",
      "epoch:  9740 \tcost:  1.74791\n",
      "epoch:  9760 \tcost:  1.7164\n",
      "epoch:  9780 \tcost:  1.67186\n",
      "epoch:  9800 \tcost:  1.75203\n",
      "epoch:  9820 \tcost:  1.67866\n",
      "epoch:  9840 \tcost:  1.7499\n",
      "epoch:  9860 \tcost:  1.71728\n",
      "epoch:  9880 \tcost:  1.74562\n",
      "epoch:  9900 \tcost:  1.73182\n",
      "epoch:  9920 \tcost:  1.75014\n",
      "epoch:  9940 \tcost:  1.70007\n",
      "epoch:  9960 \tcost:  1.74927\n",
      "epoch:  9980 \tcost:  1.76333\n"
     ]
    }
   ],
   "source": [
    "possible_batch_idx = range(data.shape[0] - time_steps - 1)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    batch_id = random.sample(population=possible_batch_idx, \n",
    "                             k=batch_size)\n",
    "    \n",
    "    for j in range(time_steps):\n",
    "        idx_X = [k + j for k in batch_id]\n",
    "        idx_Y = [k + j + 1 for k in batch_id]\n",
    "        \n",
    "        batch_x[:, j, :] = data[idx_X, :]\n",
    "        batch_y[:, j, :] = data[idx_Y, :]\n",
    "    \n",
    "    init_value = np.zeros((batch_x.shape[0], state_size))\n",
    "    training_cost, _ = sess.run([cost, train_op], feed_dict={X:batch_x, Y:batch_y, hidden_init:init_value})\n",
    "    \n",
    "    if i % display_interval == 0:\n",
    "        print(\"epoch: \", i, \"\\tcost: \", training_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'saved/model.ckpt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, \"saved/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4.2. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Pre-allocate 'hidden_last_state'\n",
    "hidden_last_state = np.zeros(state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def test_op(x, init_zero_state=True):\n",
    "    ## Reset the initial state of the network\n",
    "    if init_zero_state:\n",
    "        init_value = np.zeros(state_size)\n",
    "    else:\n",
    "        init_value = hidden_last_state\n",
    "    out, hidden_next_state = sess.run([outputs_activated, hidden_new_state], feed_dict={X:[x], hidden_init:[init_value]})\n",
    "    \n",
    "    return out[0][0], hidden_next_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TEST_PREFIX = 'The '\n",
    "TEST_PREFIX = TEST_PREFIX.lower()\n",
    "\n",
    "for i in range(len(TEST_PREFIX)):\n",
    "    test_data = vocab_encoding(corpus=TEST_PREFIX[i], vocab=vocab)\n",
    "    out, hidden_last_state = test_op(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE: \n",
      "the ma iniloloreara ini myofofofofenelonofashamofamofomevenedofeachanoupenofrolalinovinenedreyono i conextheanoupewithenonousisha ofounouneno henousthevithisofo counokifouno isofofofleyoflofofleneyofofunofoflenopasoupofoupunoupali o?\n",
      "grenofofoflionenousofonenounofenofrisonofevewhofrofofedispreasekitomanofofoupi evinonothi thenononino ounouthonexenofononorono.\n",
      "ealonofo, heli imyenofanofropasa\n",
      "erisi inistofexi ca i inoofo ewofofanonafofino; isprofrewexunofi isevelinofi itinofasofinacofo ofaneanofokine\n"
     ]
    }
   ],
   "source": [
    "print(\"SENTENCE: \")\n",
    "gen_str = TEST_PREFIX\n",
    "for i in range(500):\n",
    "    element = np.random.choice(range(len(vocab)), p=out)\n",
    "    gen_str += vocab[element]\n",
    "    \n",
    "    out, _ = test_op(vocab_encoding(vocab[element], vocab), init_zero_state=False)\n",
    "print(gen_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
