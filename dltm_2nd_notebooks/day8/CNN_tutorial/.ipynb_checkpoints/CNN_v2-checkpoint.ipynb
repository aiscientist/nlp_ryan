{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural network (version 2)\n",
    "- 미리 만들어진 모델을 불러와서 학습을 해봅시다.\n",
    "- nets/kthvgg_slim.py 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from nets.kthvgg_slim import vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load MNIST data\n",
    "- 이번에는 one_hot=False로 데이터를 불러와봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_idx3/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_idx3/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_idx3/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_idx3/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# one_hot=False\n",
    "mnist = input_data.read_data_sets(\"MNIST_idx3/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_trn, Y_trn = mnist.train.images, mnist.train.labels\n",
    "X_val, Y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, Y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 3 4 6 1]\n"
     ]
    }
   ],
   "source": [
    "print(Y_trn[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training points:  55000\n",
      "Number of validation points:  5000\n",
      "Number of test points:  10000\n"
     ]
    }
   ],
   "source": [
    "num_trn = Y_trn.shape[0]\n",
    "num_val = Y_val.shape[0]\n",
    "num_test = Y_test.shape[0]\n",
    "\n",
    "print(\"Number of training points: \", num_trn)\n",
    "print(\"Number of validation points: \", num_val)\n",
    "print(\"Number of test points: \", num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of X: 784 (28 x 28)\n",
      "Dimension of Y: None.. Y is a array of integers.\n"
     ]
    }
   ],
   "source": [
    "dim_X = X_trn.shape[1]\n",
    "pixel_X = int(np.sqrt(dim_X)) # np.sqrt의 출력이 float32이므로, 이를 int 자료형으로 변경\n",
    "# dim_Y = Y_trn.shape[1]\n",
    "\n",
    "print(\"Dimension of X: %d (%d x %d)\" % (dim_X, pixel_X, pixel_X))\n",
    "# print(\"Dimension of Y: \", dim_Y)\n",
    "print(\"Dimension of Y: None.. Y is a array of integers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_trn = X_trn.reshape(-1, 28, 28, 1)\n",
    "X_val = X_val.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build the graph\n",
    "Tensorflow에서는 모델을 'graph'로 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Placeholder for inputs and outputs\n",
    "- Shape of the placeholder for inputs: [batch_size, input_dimension]\n",
    "- Shape of the placeholder for outputs: [batch_size]\n",
    "- Placeholder의 batch_size를 None으로 하면, placeholder에 들어가기 전에 batch size를 조절해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"Inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = tf.placeholder(tf.int32, [None], name=\"Labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "test = X.get_shape()\n",
    "# test = X.get_shape()[1]\n",
    "print(test)\n",
    "# print(type(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Build the model\n",
    "\n",
    "- Weight와 bias, 그래프 구조를 생성하는 함수를 만들어봅시다.\n",
    "- 여기에서는 2개의 hidden layers를 생성하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def fully_connected(inputs, hidden_dim_1, hidden_dim_2, num_classes, scope='SimpleFCN'):\n",
    "#     \"\"\"\n",
    "#     [fully_connected] 2개의 hidden layer를 갖는 feed-forward network 생성\n",
    "    \n",
    "#     [Args]\n",
    "#       - inputs: 입력 데이터를 위한 placeholder\n",
    "#       - hidden_dim_1: 첫 번째 은닉층의 노드 수\n",
    "#       - hidden_dim_2: 두 번째 은닉층의 노드 수\n",
    "#       - num_classes: 예측하고자 하는 클래스의 수 (= 출력층의 노드 수))\n",
    "#       - Scope: default value (\"SimpleFCN\")\n",
    "#     \"\"\"\n",
    "#     # Inputs에서 1차원의 텐서들이 placeholder로 들어온다고 가정\n",
    "#     input_dim = inputs.get_shape()[1]\n",
    "    \n",
    "#     # tf.truncated_normal_initializer의 형태를 간소화\n",
    "#     trunc_normal = lambda stddev: tf.truncated_normal_initializer(mean=0.0, stddev=stddev)\n",
    "    \n",
    "#     # tf.constant_initializer의 형태를 간소화\n",
    "#     constant = lambda value: tf.constant_initializer(value=value)\n",
    "    \n",
    "#     # Define \"end_points\"\n",
    "#     end_points = {}\n",
    "    \n",
    "#     with tf.variable_scope(scope):\n",
    "#         with tf.variable_scope('HiddenLayer_1'):\n",
    "#             W_h1 = tf.get_variable(\"weights\", [input_dim, hidden_dim_1], initializer=trunc_normal(0.1))\n",
    "#             b_h1 = tf.get_variable(\"biases\", [hidden_dim_1], initializer=constant(0.0))\n",
    "#             h1 = tf.nn.relu(tf.matmul(inputs, W_h1)+ b_h1, name=\"Activation\")\n",
    "#             end_points['h1'] = h1\n",
    "            \n",
    "#         with tf.variable_scope('hiddenLayer_2'):\n",
    "#             W_h2 = tf.get_variable(\"weights\", [hidden_dim_1, hidden_dim_2], initializer=trunc_normal(0.09))\n",
    "#             b_h2 = tf.get_variable(\"biases\", [hidden_dim_2], initializer=constant(0.01))\n",
    "#             h2 = tf.nn.relu(tf.matmul(h1, W_h2) + b_h2, name=\"Activation\")\n",
    "#             end_points['h2'] = h2\n",
    "            \n",
    "#         with tf.variable_scope('OutputLayer'):\n",
    "#             W_o = tf.get_variable(\"weights\", [hidden_dim_2, num_classes], initializer=trunc_normal(0.1))\n",
    "#             b_o = tf.get_variable(\"biases\", [num_classes], initializer=constant(0.0))\n",
    "#             with tf.variable_scope('Logits'): logits = tf.matmul(h2, W_o) + b_o\n",
    "# #             logits = tf.matmul(h2, W_o) + b_o\n",
    "#             end_points['logits'] = logits\n",
    "    \n",
    "#     return logits, end_points\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits, end_points = vgg(inputs=X, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"KTH_VGG/Logits/BiasAdd:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('KTH_VGG/Conv1',\n",
      "              <tf.Tensor 'KTH_VGG/Conv1/Relu:0' shape=(?, 28, 28, 32) dtype=float32>),\n",
      "             ('KTH_VGG/Pool1',\n",
      "              <tf.Tensor 'KTH_VGG/Pool1/MaxPool:0' shape=(?, 14, 14, 32) dtype=float32>),\n",
      "             ('KTH_VGG/Conv2',\n",
      "              <tf.Tensor 'KTH_VGG/Conv2/Relu:0' shape=(?, 14, 14, 64) dtype=float32>),\n",
      "             ('KTH_VGG/Pool2',\n",
      "              <tf.Tensor 'KTH_VGG/Pool2/MaxPool:0' shape=(?, 7, 7, 64) dtype=float32>),\n",
      "             ('KTH_VGG/Conv3',\n",
      "              <tf.Tensor 'KTH_VGG/Conv3/Relu:0' shape=(?, 7, 7, 128) dtype=float32>),\n",
      "             ('KTH_VGG/Pool3',\n",
      "              <tf.Tensor 'KTH_VGG/Pool3/MaxPool:0' shape=(?, 3, 3, 128) dtype=float32>),\n",
      "             ('KTH_VGG/Conv4',\n",
      "              <tf.Tensor 'KTH_VGG/Conv4/Relu:0' shape=(?, 3, 3, 256) dtype=float32>),\n",
      "             ('KTH_VGG/Pool4',\n",
      "              <tf.Tensor 'KTH_VGG/Pool4/MaxPool:0' shape=(?, 1, 1, 256) dtype=float32>),\n",
      "             ('KTH_VGG/Flatten5',\n",
      "              <tf.Tensor 'KTH_VGG/Flatten5/Reshape:0' shape=(?, 256) dtype=float32>),\n",
      "             ('KTH_VGG/FC6',\n",
      "              <tf.Tensor 'KTH_VGG/FC6/Relu:0' shape=(?, 256) dtype=float32>),\n",
      "             ('KTH_VGG/Dropout6',\n",
      "              <tf.Tensor 'KTH_VGG/Dropout6/dropout/mul:0' shape=(?, 256) dtype=float32>),\n",
      "             ('KTH_VGG/Logits',\n",
      "              <tf.Tensor 'KTH_VGG/Logits/BiasAdd:0' shape=(?, 10) dtype=float32>)])\n"
     ]
    }
   ],
   "source": [
    "# Print my end points\n",
    "pprint(end_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Loss function\n",
    "- Classification 문제에서 제일 많이 사용하는 loss function은 **cross-entropy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 가지 옵션이 있음.\n",
    "- tf.nn.softmax_cross_entropy_with_logits: Y가 one-hot encoded 되어 있을 때\n",
    "- tf.nn.sparse_softmax_cross_entropy_with_logits: Y가 class에 대한 index 값일 때)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Training operator\n",
    "- First, define the oprimizer. (**optimizer**)\n",
    "- And then, define training operator. (**train_op**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자주 사용하는 optimizer로는 다음과 같은 것들이 있음.\n",
    "- tf.train.GradientDescentOptimizer\n",
    "- tf.train.AdagradOptimizer\n",
    "- tf.train.MomentumOptimizer\n",
    "- **tf.train.AdamOptimizer** (많은 연구자들이 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자세한 사항은 [TensorFlow API_guides: Training](https://www.tensorflow.org/api_guides/python/train) 참조!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Predicting operator\n",
    "- correct_prediction: boolean (True or False)\n",
    "- accuracy: 먼저 correct_prediction을 float32로 변환 후에 배치 내 평균을 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.nn.in_top_k(x, y, k)**\n",
    "- tf.nn.in_top_k(x, y, k)는 prediction x의 상위 k개의 결과가 true label y를 포함하는지를 계산\n",
    "- 이에 대한 output은 boolean 으로 나오므로, 이를 0, 1로 바꿔주기 위해서 tf.cast를 이용하여 float32로 변환한 이후에 accuracy를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.nn.in_top_k(logits, Y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"InTopK:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(correct_prediction)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Run the session\n",
    "- 앞서 만든 graph, operator 등을 돌리는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 128\n",
      "128 \t 256\n",
      "256 \t 384\n",
      "384 \t 512\n",
      "512 \t 640\n",
      "640 \t 768\n",
      "768 \t 896\n",
      "896 \t 1024\n",
      "1024 \t 1152\n",
      "1152 \t 1280\n",
      "1280 \t 1408\n",
      "1408 \t 1536\n",
      "1536 \t 1664\n",
      "1664 \t 1792\n",
      "1792 \t 1920\n",
      "1920 \t 2048\n",
      "2048 \t 2176\n",
      "2176 \t 2304\n",
      "2304 \t 2432\n",
      "2432 \t 2560\n",
      "2560 \t 2688\n",
      "2688 \t 2816\n",
      "2816 \t 2944\n",
      "2944 \t 3072\n",
      "3072 \t 3200\n",
      "3200 \t 3328\n",
      "3328 \t 3456\n",
      "3456 \t 3584\n",
      "3584 \t 3712\n",
      "3712 \t 3840\n",
      "3840 \t 3968\n",
      "3968 \t 4096\n",
      "4096 \t 4224\n",
      "4224 \t 4352\n",
      "4352 \t 4480\n",
      "4480 \t 4608\n",
      "4608 \t 4736\n",
      "4736 \t 4864\n",
      "4864 \t 4992\n",
      "4992 \t 5120\n",
      "5120 \t 5248\n",
      "5248 \t 5376\n",
      "5376 \t 5504\n",
      "5504 \t 5632\n",
      "5632 \t 5760\n",
      "5760 \t 5888\n",
      "5888 \t 6016\n",
      "6016 \t 6144\n",
      "6144 \t 6272\n",
      "6272 \t 6400\n",
      "6400 \t 6528\n",
      "6528 \t 6656\n",
      "6656 \t 6784\n",
      "6784 \t 6912\n",
      "6912 \t 7040\n",
      "7040 \t 7168\n",
      "7168 \t 7296\n",
      "7296 \t 7424\n",
      "7424 \t 7552\n",
      "7552 \t 7680\n",
      "7680 \t 7808\n",
      "7808 \t 7936\n",
      "7936 \t 8064\n",
      "8064 \t 8192\n",
      "8192 \t 8320\n",
      "8320 \t 8448\n",
      "8448 \t 8576\n",
      "8576 \t 8704\n",
      "8704 \t 8832\n",
      "8832 \t 8960\n",
      "8960 \t 9088\n",
      "9088 \t 9216\n",
      "9216 \t 9344\n",
      "9344 \t 9472\n",
      "9472 \t 9600\n",
      "9600 \t 9728\n",
      "9728 \t 9856\n",
      "9856 \t 9984\n",
      "9984 \t 10112\n",
      "10112 \t 10240\n",
      "10240 \t 10368\n",
      "10368 \t 10496\n",
      "10496 \t 10624\n",
      "10624 \t 10752\n",
      "10752 \t 10880\n",
      "10880 \t 11008\n",
      "11008 \t 11136\n",
      "11136 \t 11264\n",
      "11264 \t 11392\n",
      "11392 \t 11520\n",
      "11520 \t 11648\n",
      "11648 \t 11776\n",
      "11776 \t 11904\n",
      "11904 \t 12032\n",
      "12032 \t 12160\n",
      "12160 \t 12288\n",
      "12288 \t 12416\n",
      "12416 \t 12544\n",
      "12544 \t 12672\n",
      "12672 \t 12800\n",
      "12800 \t 12928\n",
      "12928 \t 13056\n",
      "13056 \t 13184\n",
      "13184 \t 13312\n",
      "13312 \t 13440\n",
      "13440 \t 13568\n",
      "13568 \t 13696\n",
      "13696 \t 13824\n",
      "13824 \t 13952\n",
      "13952 \t 14080\n",
      "14080 \t 14208\n",
      "14208 \t 14336\n",
      "14336 \t 14464\n",
      "14464 \t 14592\n",
      "14592 \t 14720\n",
      "14720 \t 14848\n",
      "14848 \t 14976\n",
      "14976 \t 15104\n",
      "15104 \t 15232\n",
      "15232 \t 15360\n",
      "15360 \t 15488\n",
      "15488 \t 15616\n",
      "15616 \t 15744\n",
      "15744 \t 15872\n",
      "15872 \t 16000\n",
      "16000 \t 16128\n",
      "16128 \t 16256\n",
      "16256 \t 16384\n",
      "16384 \t 16512\n",
      "16512 \t 16640\n",
      "16640 \t 16768\n",
      "16768 \t 16896\n",
      "16896 \t 17024\n",
      "17024 \t 17152\n",
      "17152 \t 17280\n",
      "17280 \t 17408\n",
      "17408 \t 17536\n",
      "17536 \t 17664\n",
      "17664 \t 17792\n",
      "17792 \t 17920\n",
      "17920 \t 18048\n",
      "18048 \t 18176\n",
      "18176 \t 18304\n",
      "18304 \t 18432\n",
      "18432 \t 18560\n",
      "18560 \t 18688\n",
      "18688 \t 18816\n",
      "18816 \t 18944\n",
      "18944 \t 19072\n",
      "19072 \t 19200\n",
      "19200 \t 19328\n",
      "19328 \t 19456\n",
      "19456 \t 19584\n",
      "19584 \t 19712\n",
      "19712 \t 19840\n",
      "19840 \t 19968\n",
      "19968 \t 20096\n",
      "20096 \t 20224\n",
      "20224 \t 20352\n",
      "20352 \t 20480\n",
      "20480 \t 20608\n",
      "20608 \t 20736\n",
      "20736 \t 20864\n",
      "20864 \t 20992\n",
      "20992 \t 21120\n",
      "21120 \t 21248\n",
      "21248 \t 21376\n",
      "21376 \t 21504\n",
      "21504 \t 21632\n",
      "21632 \t 21760\n",
      "21760 \t 21888\n",
      "21888 \t 22016\n",
      "22016 \t 22144\n",
      "22144 \t 22272\n",
      "22272 \t 22400\n",
      "22400 \t 22528\n",
      "22528 \t 22656\n",
      "22656 \t 22784\n",
      "22784 \t 22912\n",
      "22912 \t 23040\n",
      "23040 \t 23168\n",
      "23168 \t 23296\n",
      "23296 \t 23424\n",
      "23424 \t 23552\n",
      "23552 \t 23680\n",
      "23680 \t 23808\n",
      "23808 \t 23936\n",
      "23936 \t 24064\n",
      "24064 \t 24192\n",
      "24192 \t 24320\n",
      "24320 \t 24448\n",
      "24448 \t 24576\n",
      "24576 \t 24704\n",
      "24704 \t 24832\n",
      "24832 \t 24960\n",
      "24960 \t 25088\n",
      "25088 \t 25216\n",
      "25216 \t 25344\n",
      "25344 \t 25472\n",
      "25472 \t 25600\n",
      "25600 \t 25728\n",
      "25728 \t 25856\n",
      "25856 \t 25984\n",
      "25984 \t 26112\n",
      "26112 \t 26240\n",
      "26240 \t 26368\n",
      "26368 \t 26496\n",
      "26496 \t 26624\n",
      "26624 \t 26752\n",
      "26752 \t 26880\n",
      "26880 \t 27008\n",
      "27008 \t 27136\n",
      "27136 \t 27264\n",
      "27264 \t 27392\n",
      "27392 \t 27520\n",
      "27520 \t 27648\n",
      "27648 \t 27776\n",
      "27776 \t 27904\n",
      "27904 \t 28032\n",
      "28032 \t 28160\n",
      "28160 \t 28288\n",
      "28288 \t 28416\n",
      "28416 \t 28544\n",
      "28544 \t 28672\n",
      "28672 \t 28800\n",
      "28800 \t 28928\n",
      "28928 \t 29056\n",
      "29056 \t 29184\n",
      "29184 \t 29312\n",
      "29312 \t 29440\n",
      "29440 \t 29568\n",
      "29568 \t 29696\n",
      "29696 \t 29824\n",
      "29824 \t 29952\n",
      "29952 \t 30080\n",
      "30080 \t 30208\n",
      "30208 \t 30336\n",
      "30336 \t 30464\n",
      "30464 \t 30592\n",
      "30592 \t 30720\n",
      "30720 \t 30848\n",
      "30848 \t 30976\n",
      "30976 \t 31104\n",
      "31104 \t 31232\n",
      "31232 \t 31360\n",
      "31360 \t 31488\n",
      "31488 \t 31616\n",
      "31616 \t 31744\n",
      "31744 \t 31872\n",
      "31872 \t 32000\n",
      "32000 \t 32128\n",
      "32128 \t 32256\n",
      "32256 \t 32384\n",
      "32384 \t 32512\n",
      "32512 \t 32640\n",
      "32640 \t 32768\n",
      "32768 \t 32896\n",
      "32896 \t 33024\n",
      "33024 \t 33152\n",
      "33152 \t 33280\n",
      "33280 \t 33408\n",
      "33408 \t 33536\n",
      "33536 \t 33664\n",
      "33664 \t 33792\n",
      "33792 \t 33920\n",
      "33920 \t 34048\n",
      "34048 \t 34176\n",
      "34176 \t 34304\n",
      "34304 \t 34432\n",
      "34432 \t 34560\n",
      "34560 \t 34688\n",
      "34688 \t 34816\n",
      "34816 \t 34944\n",
      "34944 \t 35072\n",
      "35072 \t 35200\n",
      "35200 \t 35328\n",
      "35328 \t 35456\n",
      "35456 \t 35584\n",
      "35584 \t 35712\n",
      "35712 \t 35840\n",
      "35840 \t 35968\n",
      "35968 \t 36096\n",
      "36096 \t 36224\n",
      "36224 \t 36352\n",
      "36352 \t 36480\n",
      "36480 \t 36608\n",
      "36608 \t 36736\n",
      "36736 \t 36864\n",
      "36864 \t 36992\n",
      "36992 \t 37120\n",
      "37120 \t 37248\n",
      "37248 \t 37376\n",
      "37376 \t 37504\n",
      "37504 \t 37632\n",
      "37632 \t 37760\n",
      "37760 \t 37888\n",
      "37888 \t 38016\n",
      "38016 \t 38144\n",
      "38144 \t 38272\n",
      "38272 \t 38400\n",
      "38400 \t 38528\n",
      "38528 \t 38656\n",
      "38656 \t 38784\n",
      "38784 \t 38912\n",
      "38912 \t 39040\n",
      "39040 \t 39168\n",
      "39168 \t 39296\n",
      "39296 \t 39424\n",
      "39424 \t 39552\n",
      "39552 \t 39680\n",
      "39680 \t 39808\n",
      "39808 \t 39936\n",
      "39936 \t 40064\n",
      "40064 \t 40192\n",
      "40192 \t 40320\n",
      "40320 \t 40448\n",
      "40448 \t 40576\n",
      "40576 \t 40704\n",
      "40704 \t 40832\n",
      "40832 \t 40960\n",
      "40960 \t 41088\n",
      "41088 \t 41216\n",
      "41216 \t 41344\n",
      "41344 \t 41472\n",
      "41472 \t 41600\n",
      "41600 \t 41728\n",
      "41728 \t 41856\n",
      "41856 \t 41984\n",
      "41984 \t 42112\n",
      "42112 \t 42240\n",
      "42240 \t 42368\n",
      "42368 \t 42496\n",
      "42496 \t 42624\n",
      "42624 \t 42752\n",
      "42752 \t 42880\n",
      "42880 \t 43008\n",
      "43008 \t 43136\n",
      "43136 \t 43264\n",
      "43264 \t 43392\n",
      "43392 \t 43520\n",
      "43520 \t 43648\n",
      "43648 \t 43776\n",
      "43776 \t 43904\n",
      "43904 \t 44032\n",
      "44032 \t 44160\n",
      "44160 \t 44288\n",
      "44288 \t 44416\n",
      "44416 \t 44544\n",
      "44544 \t 44672\n",
      "44672 \t 44800\n",
      "44800 \t 44928\n",
      "44928 \t 45056\n",
      "45056 \t 45184\n",
      "45184 \t 45312\n",
      "45312 \t 45440\n",
      "45440 \t 45568\n",
      "45568 \t 45696\n",
      "45696 \t 45824\n",
      "45824 \t 45952\n",
      "45952 \t 46080\n",
      "46080 \t 46208\n",
      "46208 \t 46336\n",
      "46336 \t 46464\n",
      "46464 \t 46592\n",
      "46592 \t 46720\n",
      "46720 \t 46848\n",
      "46848 \t 46976\n",
      "46976 \t 47104\n",
      "47104 \t 47232\n",
      "47232 \t 47360\n",
      "47360 \t 47488\n",
      "47488 \t 47616\n",
      "47616 \t 47744\n",
      "47744 \t 47872\n",
      "47872 \t 48000\n",
      "48000 \t 48128\n",
      "48128 \t 48256\n",
      "48256 \t 48384\n",
      "48384 \t 48512\n",
      "48512 \t 48640\n",
      "48640 \t 48768\n",
      "48768 \t 48896\n",
      "48896 \t 49024\n",
      "49024 \t 49152\n",
      "49152 \t 49280\n",
      "49280 \t 49408\n",
      "49408 \t 49536\n",
      "49536 \t 49664\n",
      "49664 \t 49792\n",
      "49792 \t 49920\n",
      "49920 \t 50048\n",
      "50048 \t 50176\n",
      "50176 \t 50304\n",
      "50304 \t 50432\n",
      "50432 \t 50560\n",
      "50560 \t 50688\n",
      "50688 \t 50816\n",
      "50816 \t 50944\n",
      "50944 \t 51072\n",
      "51072 \t 51200\n",
      "51200 \t 51328\n",
      "51328 \t 51456\n",
      "51456 \t 51584\n",
      "51584 \t 51712\n",
      "51712 \t 51840\n",
      "51840 \t 51968\n",
      "51968 \t 52096\n",
      "52096 \t 52224\n",
      "52224 \t 52352\n",
      "52352 \t 52480\n",
      "52480 \t 52608\n",
      "52608 \t 52736\n",
      "52736 \t 52864\n",
      "52864 \t 52992\n",
      "52992 \t 53120\n",
      "53120 \t 53248\n",
      "53248 \t 53376\n",
      "53376 \t 53504\n",
      "53504 \t 53632\n",
      "53632 \t 53760\n",
      "53760 \t 53888\n",
      "53888 \t 54016\n",
      "54016 \t 54144\n",
      "54144 \t 54272\n",
      "54272 \t 54400\n",
      "54400 \t 54528\n",
      "54528 \t 54656\n",
      "54656 \t 54784\n",
      "54784 \t 54912\n"
     ]
    }
   ],
   "source": [
    "# Batch 인덱스 생성\n",
    "start_idx = range(0, num_trn, BATCH_SIZE)\n",
    "end_idx = range(BATCH_SIZE, num_trn + 1, BATCH_SIZE)\n",
    "for start, end in zip(start_idx, end_idx): print(start, '\\t', end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430\n"
     ]
    }
   ],
   "source": [
    "# Batch의 개수\n",
    "print(len(start_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_cost_list = list()\n",
    "val_cost_list = list()\n",
    "val_accuracy_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 epoch] training cost 2.2990\n",
      "[2 epoch] training cost 2.2963\n",
      "[3 epoch] training cost 2.2963\n",
      "[4 epoch] training cost 2.2963\n",
      "[5 epoch] training cost 2.2963\n",
      "[6 epoch] training cost 2.2963\n",
      "[7 epoch] training cost 2.2963\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess, tf.device(\"/cpu:0\"):\n",
    "    # Variable initialization\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Indices for constructing batches\n",
    "    start_idx = range(0, num_trn, BATCH_SIZE)\n",
    "    end_idx = range(BATCH_SIZE, num_trn + 1, BATCH_SIZE)\n",
    "    \n",
    "    NUM_BATCHES = len(start_idx)\n",
    "    \n",
    "    for epoch in range(0,NUM_EPOCHS):\n",
    "\n",
    "        # Set \"trn_cost\" as 0 before starting the epoch\n",
    "        trn_cost = 0\n",
    "        \n",
    "        # Training phase\n",
    "        for start, end in zip(start_idx, end_idx):\n",
    "\n",
    "            # Construct the input batch\n",
    "            batch_xs = X_trn[start:end]\n",
    "            batch_ys = Y_trn[start:end]\n",
    "            \n",
    "            # Calculate cost\n",
    "            tmp_cost, _ = sess.run([cost, train_op], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            trn_cost += tmp_cost\n",
    "        \n",
    "        trn_cost = trn_cost / NUM_BATCHES\n",
    "        trn_cost_list.append(trn_cost)\n",
    "        print(\"[{} epoch] training cost {:0.4f}\".format((epoch + 1), trn_cost))\n",
    "        \n",
    "        # Validation phase\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            val_cost, val_accuracy = sess.run([cost, accuracy], feed_dict={X: X_val, Y: Y_val})\n",
    "            val_cost_list.append(val_cost)\n",
    "            val_accuracy_list.append(val_accuracy)\n",
    "            print(\"\\t[{} epoch] validation accuracy {:0.4f}\".format((epoch + 1), val_accuracy))\n",
    "            \n",
    "    # Test phase\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={X: X_test, Y: Y_test})\n",
    "    print(\"\\n\")\n",
    "    print(\"Test accuracy: {:0.4f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
