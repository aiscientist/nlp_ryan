{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "하루치 뉴스를 분석할 수도 있지만, 때로는 미리 분석하려는 모든 날의 뉴스들로부터 단어사전을 만들어둘 필요도 있습니다. 이번에는 corpus_10days의 10일치 뉴스에 대해서 명사를 모두 추출한 뒤, universial dictionary를 만드는 것을 알아보고, 이를 이용하여 2016-10-20 뉴스에 대하여 뉴스의 군집화를 한 뒤, 각 군집의 키워드를 추출함으로써 그날 뉴스의 핫키워드를 추출해 보도록 하겠습니다. \n",
    "\n",
    "Day5_2에서는 doc2vec을 이용하였기 때문에 각 영화에 해당하는 corpus를 다시 loading 하여야 했습니다. 이번에는 각 뉴스 문서 별로 term frequency matrix를 만들고, 이를 이용하여 곧바로 Regularized Logistic Regression을 통하여 키워드를 선택하겠습니다. \n",
    "\n",
    "명사 셋을 추출할 것인지에 대한 configuration parameters를 아래에 적어뒀습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "EXTRACT_NOUN_SET = False\n",
    "noun_dictionary_fname = '../../../data/corpus_10days/models/noun_dictionary.txt'\n",
    "\n",
    "TOKENIZE_2016_1020 = False\n",
    "normed_corpus_fname = '../../../data/corpus_10days/news/2016-10-20_article_all_normed.txt'\n",
    "tokenized_corpus_fname = '../../../data/corpus_10days/news/2016-10-20_article_all_normed_nountokenized.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "corpus_10days의 10일치 뉴스에 대하여, 각 날의 기사마다 명사를 추출하여, 이를 nouns_dictionary에 누적하여 저장하겠습니다. \n",
    "\n",
    "import는 if 구문 아래에서 실행할 수도 있습니다. 이 때에는 EXTRACT_NOUN_SET = True 일 때에만, glob, sys, corpus 등이 import 됩니다. \n",
    "\n",
    "glob는 조건을 만족하는 파일 주소들을 찾을 수 있는 library입니다. \n",
    "\n",
    "    corpus_fnames = glob.glob('../../../data/corpus_10days/news/*_article_all_normed.txt')\n",
    "    \n",
    "위 구문은 root/data/corpus_10days/news 폴더에 있는 파일들 중에서 앞 부분이 어떤 것들이 등장하던지, 뒷부분에 아래의 글자가 있는 파일들을 찾으라는 의미입니다. \n",
    "\n",
    "    _article_all_normed.txt\n",
    "    \n",
    "그 결과는 아래와 같습니다. \n",
    "\n",
    "    ['../../../data/corpus_10days/news/2016-10-28_article_all_normed.txt',\n",
    "     '../../../data/corpus_10days/news/2016-10-26_article_all_normed.txt',\n",
    "     '../../../data/corpus_10days/news/2016-10-22_article_all_normed.txt',\n",
    "     '../../../data/corpus_10days/news/2016-10-21_article_all_normed.txt',\n",
    "     '../../../data/corpus_10days/news/2016-10-29_article_all_normed.txt',\n",
    "     '../../../data/corpus_10days/news/2016-10-23_article_all_normed.txt',\n",
    "     '../../../data/corpus_10days/news/2016-10-20_article_all_normed.txt',\n",
    "     '../../../data/corpus_10days/news/2016-10-25_article_all_normed.txt',\n",
    "     '../../../data/corpus_10days/news/2016-10-24_article_all_normed.txt',\n",
    "     '../../../data/corpus_10days/news/2016-10-27_article_all_normed.txt']\n",
    "     \n",
    "\"for corpus_fname in corpus_fnames\"의 for loop을 돌면서, 매일의 뉴스 기사마다 명사를 추출합니다. \n",
    "nouns_dictionary는 set이기 때문에 update를 이용할 수 있습니다. \n",
    "\n",
    "    print('corpus name = %s, num nouns = %d' % (corpus_name, len(nouns_dictionary)))\n",
    "    \n",
    "아래의 구문을 통하여 하루의 뉴스기사를 분석할 때마다, 명사사전의 크기가 얼마나 커져가는지도 확인할 수 있습니다. \n",
    "\n",
    "이번에는 피클링을 하지 않고 명사 사전을 텍스트 파일로 저장하겠습니다. 피클링은 데이터를 binary로 저장하기 때문에 파일로 직접 읽을 수가 없습니다. 하지만 텍스트 파일로 저장한다면 눈으로 확인할 수 있을 뿐더러, 다른 프로그래밍 언어로 읽을 수도 있습니다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if EXTRACT_NOUN_SET:\n",
    "    import glob\n",
    "    import sys\n",
    "    sys.path.append('../soy/')\n",
    "    sys.path.append('../mypy')\n",
    "    from soy.nlp.tags import LRNounExtractor\n",
    "    from corpus import Corpus\n",
    "\n",
    "    corpus_fnames = glob.glob('../../../data/corpus_10days/news/*_article_all_normed.txt')\n",
    "    print('num corpus = %d' % len(corpus_fnames))\n",
    "    \n",
    "    nouns_dictionary = set()\n",
    "    \n",
    "    for corpus_fname in corpus_fnames:\n",
    "        news_corpus = Corpus(corpus_fname, iter_sent=True)\n",
    "        noun_extractor = LRNounExtractor()\n",
    "        nouns, cohesion_probability = noun_extractor.extract(news_corpus)\n",
    "        \n",
    "        nouns_dictionary.update(set(nouns.keys()))\n",
    "        corpus_name = corpus_fname.split('/')[-1].split(')')[0]\n",
    "        print('corpus name = %s, num nouns = %d' % (corpus_name, len(nouns_dictionary)))\n",
    "        \n",
    "    with open(noun_dictionary_fname, 'w', encoding='utf-8') as f:\n",
    "        for noun in nouns_dictionary:\n",
    "            f.write('%s\\n' % noun)\n",
    "    \n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2016-10-20일 뉴스에 대하여 CountVectorizer를 이용하여 term frequency matrix를 만든 뒤, 그날의 주요 뉴스들이 어떤 것들이 있었는지 군집화를 수행합니다. scikit learn의 KMeans는 sparse matrix로도 군집화를 수행할 수 있습니다. \n",
    "\n",
    "이를 위하여, 이전 수업 때 수행했던 custom_tokenizer를 이용합니다. noun_dictionary는 텍스트 파일로 저장되어 있으므로, 파일을 open으로 열 때에는 반드시 encoding='utf-8'을 신경써 주세요.\n",
    "\n",
    "    [noun.strip() for noun in f]\n",
    "    \n",
    "위 구문에서 noun.strip()을 하면 줄바꿈 기호 '\\n'이 사라집니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['이번', '국정농단', '사태', '대하여', '뉴스', '이용', '분석', '수행']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(noun_dictionary_fname, encoding='utf-8') as f:\n",
    "    nouns_dictionary = [noun.strip() for noun in f]\n",
    "    print(len(nouns_dictionary))\n",
    "\n",
    "def custom_tokenize(doc):    \n",
    "    def parse_noun(token):\n",
    "        for e in reversed(range(1, len(token)+1)):\n",
    "            subword = token[:e]\n",
    "            if subword in nouns_dictionary:\n",
    "                return subword\n",
    "        return ''\n",
    "    \n",
    "    nouns = [parse_noun(token) for token in doc.split()]\n",
    "    nouns = [word for word in nouns if word]\n",
    "    return nouns\n",
    "\n",
    "custom_tokenize('이번 국정농단의 사태에 대하여 뉴스를 이용한 분석을 수행합니다')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "2016-10-20 뉴스에 대하여 corpus를 만듦니다. corpus의 length도 확인합니다. 당일에 30,091개의 뉴스가 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization was done\n"
     ]
    }
   ],
   "source": [
    "if TOKENIZE_2016_1020:\n",
    "    \n",
    "    import sys\n",
    "    sys.path.append('../mypy')\n",
    "    from corpus import Corpus\n",
    "    corpus_2016_1020 = Corpus(normed_corpus_fname, iter_sent=False)\n",
    "    print('corpus length of 2016-10-20 = %d' % len(corpus_2016_1020))\n",
    "\n",
    "    tokenized_corpus_2016_1020 = []\n",
    "    for num_doc, doc in enumerate(corpus_2016_1020):\n",
    "        if num_doc % 100 == 0:\n",
    "            sys.stdout.write('\\rtokenizing %d ... ' % num_doc)\n",
    "        doc = ' '.join([noun for sent in doc.split('  ') for noun in custom_tokenize(sent)]).strip()\n",
    "        tokenized_corpus_2016_1020.append(doc)    \n",
    "    print('\\rtokenization was done')\n",
    "\n",
    "    with open(tokenized_corpus_fname, 'w', encoding='utf-8') as f:\n",
    "        for doc in tokenized_corpus_2016_1020:\n",
    "            f.write('%s\\n' % doc)\n",
    "            \n",
    "else:\n",
    "    \n",
    "    with open(tokenized_corpus_fname, encoding='utf-8') as f:\n",
    "        tokenized_corpus_2016_1020 = [doc.strip() for doc in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "CountVectorizer를 이용하여 term frequency matrix를 만듦니다. 만들어 둔 term frequency matrix는 corpus_10days/models/ 아래에 저장해 두었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30091x2631 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1470416 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=0.005, max_df=0.8)\n",
    "x_2016_1020 = vectorizer.fit_transform(tokenized_corpus_2016_1020)\n",
    "print(x_2016_1020)\n",
    "\n",
    "from scipy.io import mmwrite\n",
    "mmwrite('../../../data/corpus_10days/models/2016-10-20_noun_x.mm', x_2016_1020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "x_2016_1020의 각 column에 해당하는 단어를 decoding 하기 위하여 vectorizer.vocabulary_로부터  int2vocab를 만들었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '01', '02', '03']\n"
     ]
    }
   ],
   "source": [
    "int2vocab = sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])\n",
    "int2vocab = [word for word, idx in int2vocab]\n",
    "print(int2vocab[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "만들어진 x_2016_1020을 이용하여 Spherical kmeans를 수행하겠습니다. 이를 위하여 L2로 normalization을 수행한 뒤, 클러스터의 개수를 1000개로 지정하여 k-means clustering을 수행합니다. \n",
    "\n",
    "군집화나 토픽 모델링을 수행할 때에는 예상하는 것보다 군집/토픽의 개수를 크게 잡아주세요. 중복되는 군집이 등장하면 나중에 묶으면 됩니다. 하지만, 데이터에 노이즈가 있어서 다른 군집들이 하나의 군집으로 묶인다면 나중에 해석하기가 어려워집니다. \n",
    "\n",
    "특히, 여러 군집/토픽에서 두루두루 사용될 수 있는 단어들이 이러한 노이즈 역할을 합니다. 이런 단어들을 미리 걸러낼 수 있다면 훨씬 더 정교한 모델링이 될 것입니다. (즉, 군집화나 토픽모델링에서는 불필요하다 생각되는 단어들을 과감히 쳐낼수록 결과가 깔끔합니다).  그렇지 않다면, 군집화/토픽모델링을 할 때 군집/토픽의 개수를 크게 잡아두면 좀 더 좋습니다. \n",
    "\n",
    "군집화를 수행한 뒤, 20개의 군집들에 대하여 각 군집에 할당된 뉴스의 개수가 몇 개인지 출력합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 16587.190\n",
      "Iteration  1, inertia 13143.934\n",
      "Iteration  2, inertia 12694.927\n",
      "Iteration  3, inertia 12486.385\n",
      "Iteration  4, inertia 12372.992\n",
      "Iteration  5, inertia 12308.900\n",
      "Iteration  6, inertia 12272.808\n",
      "Iteration  7, inertia 12250.201\n",
      "Iteration  8, inertia 12232.298\n",
      "Iteration  9, inertia 12220.497\n",
      "Iteration 10, inertia 12213.461\n",
      "Iteration 11, inertia 12207.667\n",
      "Iteration 12, inertia 12200.940\n",
      "Iteration 13, inertia 12194.250\n",
      "Iteration 14, inertia 12189.779\n",
      "Iteration 15, inertia 12187.236\n",
      "Iteration 16, inertia 12185.546\n",
      "Iteration 17, inertia 12184.778\n",
      "Iteration 18, inertia 12184.223\n",
      "Iteration 19, inertia 12183.920\n",
      "cluster # 0 has 1 docs\n",
      "cluster # 1 has 48 docs\n",
      "cluster # 2 has 18 docs\n",
      "cluster # 3 has 290 docs\n",
      "cluster # 4 has 34 docs\n",
      "cluster # 5 has 1 docs\n",
      "cluster # 6 has 19 docs\n",
      "cluster # 7 has 2 docs\n",
      "cluster # 8 has 2 docs\n",
      "cluster # 9 has 891 docs\n",
      "cluster # 10 has 13 docs\n",
      "cluster # 11 has 20 docs\n",
      "cluster # 12 has 19 docs\n",
      "cluster # 13 has 1 docs\n",
      "cluster # 14 has 33 docs\n",
      "cluster # 15 has 71 docs\n",
      "cluster # 16 has 6 docs\n",
      "cluster # 17 has 299 docs\n",
      "cluster # 18 has 280 docs\n",
      "cluster # 19 has 5 docs\n",
      "CPU times: user 3min 5s, sys: 1.24 s, total: 3min 6s\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import defaultdict\n",
    "\n",
    "x_2016_1020 = normalize(x_2016_1020, axis=1, norm='l2')\n",
    "kmeans = KMeans(n_clusters=1000, max_iter=20, n_init=1, verbose=1)\n",
    "clusters = kmeans.fit_predict(x_2016_1020)\n",
    "\n",
    "clusters_to_rows = defaultdict(lambda: [])\n",
    "for idx, label in enumerate(clusters):\n",
    "    clusters_to_rows[label].append(idx)\n",
    "\n",
    "for i in range(20):\n",
    "    print('cluster # %d has %d docs' % (i, len(clusters_to_rows[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "군집 15번은 71개의 뉴스가 묶여있습니다 (이건 군집화를 할 때마다 달라지므로, 다시 실행시키시면 다른 숫자일겁니다). 이 군집에 대하여 키워드를 추출하기 위해서는, 15번 군집에 해당하는 뉴스의 label을 1로, 다른 뉴스들을 -1로 둔 뒤, 해당 군집 15번을 구분할 수 있는 주요 단어들을 L1 regularized logistic regression을 이용하여 뽑을 수도 있습니다. \n",
    "\n",
    "y를 만든 뒤, 혹시 하는 마음에 x_2016_1020.shape과 같은지 확인도 해봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30091, (30091, 2631))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_aspect = 15\n",
    "y = [1 if i in clusters_to_rows[cluster_aspect] else -1 for i in range(x_2016_1020.shape[0])]\n",
    "len(y), x_2016_1020.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "LogisticRegression의 penalty를 'l1'으로 주고, Regularization cost를 1로 주었습니다. 키워드의 개수가 너무 많다면 C를 더 작게, 키워드의 개수가 너무 적다면 C를 좀 더 크게 조절할 수 있습니다. \n",
    "\n",
    "    상위 30개의 키워드: 상승 (18.446), 하락 (9.586), 미국 (9.543), 전날 (6.326), 상승세 (6.213), 예상 (6.110), 거래 (5.957), 달러 (5.282), 기록 (5.227), 증시 (5.121), 이날 (4.866), 지수 (3.715), 시장 (3.581), 마감 (3.168), 강세 (1.732), 힐러리 (1.280), 대비 (1.085), 금리 (1.021), 51 (0.270)\n",
    "    \n",
    "상위 30개의 키워드를 선택하였더니, 미국 금리에 의한 증시의 변동에 대한 이야기임을 추정할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "상승 (18.446)\n",
      "하락 (9.586)\n",
      "미국 (9.543)\n",
      "전날 (6.326)\n",
      "상승세 (6.213)\n",
      "예상 (6.110)\n",
      "거래 (5.957)\n",
      "달러 (5.282)\n",
      "기록 (5.227)\n",
      "증시 (5.121)\n",
      "이날 (4.866)\n",
      "지수 (3.715)\n",
      "시장 (3.581)\n",
      "마감 (3.168)\n",
      "강세 (1.732)\n",
      "힐러리 (1.280)\n",
      "대비 (1.085)\n",
      "금리 (1.021)\n",
      "51 (0.270)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_l1 = LogisticRegression(penalty='l1', C=1)\n",
    "logistic_l1.fit(x_2016_1020, y)\n",
    "\n",
    "keywords = sorted(enumerate(logistic_l1.coef_[0]), key=lambda x:x[1], reverse=True)[:30]\n",
    "for word, score in keywords:\n",
    "    if score == 0: break\n",
    "    print('%s (%.3f)' % (int2vocab[word], score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "이번에는 관심있는 군집들에 대하여 키워드들을 뽑아내는 함수를 만들겠습니다. interested_clusters로 입력되는 클러슽들에 대하여 위처럼 각각의 클러스터에 대한 Regularized Logistic Regression을 이용하여 키워드를 추출합니다. \n",
    "\n",
    "Regularization cost와 키워드의 개수를 선택하기 위하여 print_keywords의 함수의 arguments에 이를 넣어둡니다. L1 Logistic Regression에서 coefficient가 0이라는 것은 classification에서 유의미한 변수가 아니라는 뜻입니다. 그렇기 때문에 아래의 구문을 넣어서 coefficient가 0보다 큰 단어들만을 선택합니다. \n",
    "\n",
    "    keywords = [int2vocab[word] for word, score in keywords if score > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "아래와 같은 결과를 얻을 수 있습니다. 단어 선택과 같은 튜닝 과정없이 명사 추출 + k-means clustering + 키워드 추출만으로도 어느 정도 그날 뉴스의 토픽들을 살펴볼 수 있습니다. \n",
    "\n",
    "이제 남은 일은, 좀 더 정확한 단어를 선택하여 군집화가 잘 되게 만들고, 키워드를 추출하기에 좋은 parameter를 찾는 것입니다. \n",
    "\n",
    "하지만 이런 접근 방법에 한 가지 단점이 있습니다. 만약 18번 군집에 대한 키워드로 '디자이너'가 선택되었지만, 이 단어는 다른 군집에서의 키워드가 될 수도 있습니다. 다른 여러개의 군집에서도 키워드로 이용될 수 있는 단어라면, 18번 군집의 키워드로 선택이 되지 않을수도 있습니다. 근본적인 원인은 18번이 아닌 다른 군집에 실제 18번 군집과 비슷한 군집이 있기 때문입니다. 이는 topic modeling, doc2vec, 단어 선택을 통한 군집화의 고도화 등으로 해결해야 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "cluster # 3 keywords (from 290 news)\n",
    " > ['사진', '뉴시스', '제공', '전주', '울산', '장관', '발생']\n",
    "\n",
    "cluster # 15 keywords (from 71 news)\n",
    " > ['엑스포츠뉴스', '을지로', '오후']\n",
    "\n",
    "cluster # 17 keywords (from 299 news)\n",
    " > ['서울', '사진', '뉴시스', '여의도', '대한', '촉구', '국회', '브리핑', '이종', '스포츠재단', '강남구', '영상', '전국', '판매', '미래창조과학부', '선보', '오전', '83']\n",
    "\n",
    "cluster # 18 keywords (from 280 news)\n",
    " > ['유진', '디자이너', '진행', '브랜드', '마이데일리']\n",
    "\n",
    "cluster # 21 keywords (from 78 news)\n",
    " > ['디자이너', '조망', '콜렉션', '펼쳐진다', '헤라', '오전', '서울패션위크']\n",
    "\n",
    "cluster # 22 keywords (from 62 news)\n",
    " > ['콤비', '사랑', '코미디', '개봉', '리포트', '11월']\n",
    "\n",
    "cluster # 32 keywords (from 74 news)\n",
    " > ['버튼', '감상', '크기', '09', '눈물', '뮤직비디오', '27', '방탄소년단', '이미지']\n",
    "\n",
    "cluster # 35 keywords (from 97 news)\n",
    " > ['권리', '있습니다', '주제', '사회', '자유', '여성', '사랑', '함께', '인터넷', '합니다', '주인', '지역']\n",
    "\n",
    "cluster # 36 keywords (from 55 news)\n",
    " > ['을지로', '전자신문', '오후', '취하고', '엔터온뉴스', '컬렉션', '서울패션위크']\n",
    "\n",
    "cluster # 43 keywords (from 56 news)\n",
    "> ['재단', '검찰', '설립', '조사', '수사', '고발', '최씨', '미르']\n",
    "\n",
    "cluster # 57 keywords (from 59 news)\n",
    "> ['발사', '실패', '지난', '추가', '미사일', '무수단']\n",
    "\n",
    "cluster # 146 keywords (from 73 news)\n",
    "> ['대통령', '박근혜', '최순실', '의장', '국민', '정부', '경북', '우병우']\n",
    "\n",
    "cluster # 856 keywords (from 93 news)\n",
    "> ['학교', '학생들', '학생', '활동', '지원', '청소년', '아이들']\n",
    "\n",
    "cluster # 857 keywords (from 56 news)\n",
    "> ['한미', '북한', '양국', '압박', '대북', '외교', '국방', '협의체', '확장억제', '대한', '신설']\n",
    "\n",
    "cluster # 909 keywords (from 98 news)\n",
    "> ['의원', '국회', '지적', '2014년', '경우', '문제', '이전', '제출', '세금', '정부']\n",
    "\n",
    "cluster # 938 keywords (from 73 news)\n",
    "> ['토론', '클린턴', '대선', '여론조사', '트럼프']\n",
    "\n",
    "cluster # 959 keywords (from 141 news)\n",
    "> ['방송', '시즌', '시청자들', '지금', '프로그램', '뷰티', '정도', '오후', '여성', '지난', '후문', '제작', '8시']\n",
    "\n",
    "cluster # 971 keywords (from 101 news)\n",
    "> ['기술', '사용', '적용', '소재', '병원', '디자인', '선보', '출시', '기존', '겨울', '착용', '스포츠']\n",
    "\n",
    "cluster # 980 keywords (from 56 news)\n",
    "> ['여의도', '뉴스1', '국민의당', '국회', '토론회', '모두발언', '서울', '대표', '오전']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cluster # 3 keywords (from 290 news)\n",
      " > ['사진', '뉴시스', '제공', '전주', '울산', '장관', '발생']\n",
      "\n",
      "cluster # 15 keywords (from 71 news)\n",
      " > ['엑스포츠뉴스', '을지로', '오후']\n",
      "\n",
      "cluster # 17 keywords (from 299 news)\n",
      " > ['서울', '사진', '뉴시스', '여의도', '대한', '촉구', '국회', '브리핑', '이종', '스포츠재단', '강남구', '영상', '전국', '판매', '미래창조과학부', '선보', '오전', '83']\n",
      "\n",
      "cluster # 18 keywords (from 280 news)\n",
      " > ['유진', '디자이너', '진행', '브랜드', '마이데일리']\n",
      "\n",
      "cluster # 21 keywords (from 78 news)\n",
      " > ['디자이너', '조망', '콜렉션', '펼쳐진다', '헤라', '오전', '서울패션위크']\n",
      "\n",
      "cluster # 22 keywords (from 62 news)\n",
      " > ['콤비', '사랑', '코미디', '개봉', '리포트', '11월']\n",
      "\n",
      "cluster # 32 keywords (from 74 news)\n",
      " > ['버튼', '감상', '크기', '09', '눈물', '뮤직비디오', '27', '방탄소년단', '이미지']\n",
      "\n",
      "cluster # 35 keywords (from 97 news)\n",
      " > ['권리', '있습니다', '주제', '사회', '자유', '여성', '사랑', '함께', '인터넷', '합니다', '주인', '지역']\n",
      "\n",
      "cluster # 36 keywords (from 55 news)\n",
      " > ['을지로', '전자신문', '오후', '취하고', '엔터온뉴스', '컬렉션', '서울패션위크']\n",
      "\n",
      "cluster # 41 keywords (from 74 news)\n",
      " > ['헤라서울패션위크', '김창', '스타뉴스', '컬렉션']\n",
      "\n",
      "cluster # 43 keywords (from 56 news)\n",
      " > ['재단', '검찰', '설립', '조사', '수사', '고발', '최씨', '미르']\n",
      "\n",
      "cluster # 47 keywords (from 51 news)\n",
      " > ['강수지', '라고', '사람']\n",
      "\n",
      "cluster # 54 keywords (from 175 news)\n",
      " > ['이데일리', '종합', '공시', '현대자동차', '2016']\n",
      "\n",
      "cluster # 56 keywords (from 121 news)\n",
      " > ['21', '2016']\n",
      "\n",
      "cluster # 57 keywords (from 59 news)\n",
      " > ['발사', '실패', '지난', '추가', '미사일', '무수단']\n",
      "\n",
      "cluster # 59 keywords (from 54 news)\n",
      " > ['의혹', '이화여대', '학생들', '총장', '이대', '특혜']\n",
      "\n",
      "cluster # 70 keywords (from 70 news)\n",
      " > ['뉴시스', '서울', '오전', '계약']\n",
      "\n",
      "cluster # 72 keywords (from 124 news)\n",
      " > ['설계', '단지', '입주', '풍부', '인근', '구성', '84', '개통', '규모', '아파트', '지상', '이용', '가능', '분양', '예정', '위치', '전용', '21']\n",
      "\n",
      "cluster # 74 keywords (from 147 news)\n",
      " > ['한국', '우리나라', '세계', '영국', '설명', '아시아', '작품', '세계적', '현재', '역할', '발표', '국가', '가치', '작가', '발전', '대회', '참가', '건설', '협력']\n",
      "\n",
      "cluster # 80 keywords (from 98 news)\n",
      " > ['사진', '세상', '대구', '지난', '세계', '페이스북', '여행', '최종', '데뷔', '아이']\n",
      "\n",
      "cluster # 82 keywords (from 134 news)\n",
      " > ['라고', '엄마', '방송', '이에', '고백', '비밀', '어머니', '사진', '20일', '캡처', '출연', '마음', '많이']\n",
      "\n",
      "cluster # 86 keywords (from 56 news)\n",
      " > ['게임', '캐릭터', '출시', '엄마', '인기', '활용', '모바일', '공격', '원작']\n",
      "\n",
      "cluster # 132 keywords (from 52 news)\n",
      " > ['공시', '규모', '대표이사', '계약', '공급', '한국경제', '20일', '한경닷컴', '발행', '기자', '대한', '연합뉴스', '이라고', '대화', '취득']\n",
      "\n",
      "cluster # 134 keywords (from 73 news)\n",
      " > ['교수', '개최', '서울대', '주제', '대한', '대학']\n",
      "\n",
      "cluster # 136 keywords (from 306 news)\n",
      " > ['뉴시스', '기자', '개최', '지난', '진행', '작품', '이번', '계획', '지원', '있도록', '판매', '건강', '우수', '관계자는', '활동', '경북', '대한', '20일', '선정', '지정', '최근', '않은', '운영', '올해', '공모', '함께', '전국', '시민', '지역', '시작']\n",
      "\n",
      "cluster # 140 keywords (from 53 news)\n",
      " > ['마이데일리', '사랑', '영화', '신사동', '차태현']\n",
      "\n",
      "cluster # 146 keywords (from 73 news)\n",
      " > ['대통령', '박근혜', '최순실', '의장', '국민', '정부', '경북', '우병우']\n",
      "\n",
      "cluster # 153 keywords (from 199 news)\n",
      " > ['배우', '역시', '매력', '캐릭터', '순간', '작가', '드라마', '어떤', '번째', '연기', '시상식', '최근', '가수', '배우들', '예정', '활동', '사랑', '인물', '연인', '시간', '생각', '이야기', '모델', '작품', '소속사', '11월', '마지막', '장면', '연예뉴스', '포스터']\n",
      "\n",
      "cluster # 155 keywords (from 142 news)\n",
      " > ['중국', '광주', '정부', '경남', '이번']\n",
      "\n",
      "cluster # 156 keywords (from 112 news)\n",
      " > ['실적', '연구', '이라고', '영업이익', '증가', '전망', '개선', '주가', '내년', '목표주가', '투자증권', '기대', '매출액', '대비', '예상', '3분기']\n",
      "\n",
      "cluster # 159 keywords (from 156 news)\n",
      " > ['출시', '제품', '모델', '보관', '공간', '가능', '애플', '행사', '선보', '국내', '번째', '디자인', '가격', '특히', '시장', '최근', '일반', '직접', '건강', '방식', '것으로', '있게', '제작', '인증', '특징', '신제품']\n",
      "\n",
      "cluster # 166 keywords (from 90 news)\n",
      " > ['유발', '치료', '질환', '경우', '수술', '때문', '부작용', '건강', '중요', '가장', '심장', '환자', '필요', '하지', '풍부', '도움', '증상', '원인', '식품', '상태', '사용', '눈물', '가슴', '예방', '효과', '정신']\n",
      "\n",
      "cluster # 172 keywords (from 269 news)\n",
      " > ['지난', '시장', '것으로', '최근', '매각', '국내', '관계자는', '수익', '올해', '생산', '결정', '경우', '수주', '인수', '업체', '전국', '하지', '보유', '예상', '지적', '실제', '규모', '증가', '이상', '9월', '고객', '내년', '제도', '삼성', '이에']\n",
      "\n",
      "cluster # 174 keywords (from 92 news)\n",
      " > ['대표', '창업', '요리', '공장', '개발', '제품', '디자인', '추가', '음식', '분석', '회사', '기업', '19일', '중소기업']\n",
      "\n",
      "cluster # 176 keywords (from 100 news)\n",
      " > ['회장', '경영', '매일신문', '그룹', '대구']\n",
      "\n",
      "cluster # 180 keywords (from 148 news)\n",
      " > ['서울', '서울신문', '상담', '구입', '금융', '거래', '선정', '개최', '1천', '공모', '함께', '21일', '설치', '건물', '주민', '일자리', '취업', '역사', '행사', '관련', '올해', '소개', '주민들', '전시', '세미나', '충남', '기부', '지원', '축제', '주제']\n",
      "\n",
      "cluster # 187 keywords (from 63 news)\n",
      " > ['김선', '컬렉션', '오전', '뉴시스', '디자이너', '사진', '중구', '김홍범']\n",
      "\n",
      "cluster # 193 keywords (from 208 news)\n",
      " > ['뉴스1', '홍보', '운영', '선발', '노력', '제공', '충북', '이날', '일부', '관계자는', '정책', '실시', '없다고', '주민들', '활동', '이번', '1심', '계획', '개최', '20일', '시장', '지난', '전국', '마련', '추진', '대상', '학생들', '사장', '진행', '참가']\n",
      "\n",
      "cluster # 199 keywords (from 122 news)\n",
      " > ['서울경제']\n",
      "\n",
      "cluster # 210 keywords (from 139 news)\n",
      " > ['패션위크', '서울', '을지로', '중구']\n",
      "\n",
      "cluster # 213 keywords (from 147 news)\n",
      " > ['08', '2016']\n",
      "\n",
      "cluster # 214 keywords (from 77 news)\n",
      " > ['재배포', '매일경제', '확인', '속보', '뉴스', '무단', '디지털', '오늘', '24']\n",
      "\n",
      "cluster # 221 keywords (from 59 news)\n",
      " > ['기자회견', '더불어민주당', '뉴스1', '강진', '취재진', '받고', '정계복귀', '상임고문']\n",
      "\n",
      "cluster # 235 keywords (from 149 news)\n",
      " > ['미국', '보도', '뉴욕', '프랑스', '소장', '세계', '도시', '센터', '진출', '모델', '독일', '시작', '것이다', '사람', '정부', '발전', '연구', '보고', '자동차', '결정']\n",
      "\n",
      "cluster # 245 keywords (from 55 news)\n",
      " > ['무대', '타이틀곡', '이날', '선보', '팬들', '매력', '컴백', '24', '기자', '모습']\n",
      "\n",
      "cluster # 247 keywords (from 437 news)\n",
      " > ['금지', '해석', '있습니다', '기자', '방법', '선발', '불법', '공무원', '전국', '수정', '통일', '관심', '시대', '전쟁', '선정', '시민', '실시', '대상', '모두', '자리', '영국', '뉴스', '제공', '아직', '작가', '앵커', '했습니다', '설명', '전남', '1억']\n",
      "\n",
      "cluster # 248 keywords (from 51 news)\n",
      " > ['국정원장', '의원', '간사', '원장', '브리핑', '민주당', '새누리당', '국정원']\n",
      "\n",
      "cluster # 275 keywords (from 59 news)\n",
      " > ['화보', '생각', '특히', '여자', '진행', '앨범', '가사', '최근', '강렬', '음악', '신곡', '인터뷰', '출연']\n",
      "\n",
      "cluster # 291 keywords (from 54 news)\n",
      " > ['2015', '19']\n",
      "\n",
      "cluster # 294 keywords (from 142 news)\n",
      " > ['2016', '19', '13', '24', '21', '23']\n",
      "\n",
      "cluster # 304 keywords (from 80 news)\n",
      " > ['클린턴', '트럼프', '여성', '주장', '이에', '한국', '미국', '도널드']\n",
      "\n",
      "cluster # 338 keywords (from 112 news)\n",
      " > ['머니투데이', '공시', '20일']\n",
      "\n",
      "cluster # 342 keywords (from 60 news)\n",
      " > ['사고', '사장', '안전', '발생', '대책', '서울시', '김포공항역']\n",
      "\n",
      "cluster # 387 keywords (from 88 news)\n",
      " > ['컬렉션', '스타일', '디자인', '아이', '디자이너', '패션', '컬러', '선보', '블랙', '연출', '느낌', '2017', '소재', '모델', '이번', '의상', '브랜드']\n",
      "\n",
      "cluster # 395 keywords (from 52 news)\n",
      " > ['총격', '시민', '범인', '검거', '시민들', '경찰', '발사', '터널', '경찰관', '사람', '현장', '인근', '앵커', '총기', '서울']\n",
      "\n",
      "cluster # 401 keywords (from 73 news)\n",
      " > ['연합뉴스', '20일', '아이들', '대전', '84', '울산', '의료', '특파원', '대구', '국내', '평화', '주최', '행사']\n",
      "\n",
      "cluster # 417 keywords (from 174 news)\n",
      " > ['지역', '허용', '있도록', '노력', '관광객', '공항', '가능', '광주', '추진', '규제', '강화', '주민들', '발전', '기준', '이용', '토론회', '설치', '시설', '충북', '최근', '기대', '개선', '마리', '주민', '경우', '활성화', '결과', '차지', '계획', '지원']\n",
      "\n",
      "cluster # 429 keywords (from 130 news)\n",
      " > ['동대문', '스포츠조선', '동대문디자인플라자', '서울패션위크']\n",
      "\n",
      "cluster # 435 keywords (from 172 news)\n",
      " > ['오후', '제보', '앞에서', '애플', '뉴시스', '청와대', '19', '경남', '협상', '기록', '회의', '미세먼지', '국방부', '요구', '파업', '23일', '대기', '영상']\n",
      "\n",
      "cluster # 439 keywords (from 119 news)\n",
      " > ['방지', '오후', '스포츠동아', '동아닷컴']\n",
      "\n",
      "cluster # 445 keywords (from 412 news)\n",
      " > ['친구', '사람들', '생각', '사람', '정도', '인간', '시작', '시간', '준비', '요즘', '것이다', '마음', '자신', '이름', '하나', '사회', '어떤', '때문', '하지', '세계', '희망', '세상', '우리', '이렇게', '정상', '문제', '역사', '하루', '아닌', '너무']\n",
      "\n",
      "cluster # 463 keywords (from 65 news)\n",
      " > ['투자', '주식', '금리', '미국', '운용', '상품', '글로벌', '유럽', '부동산', '자금']\n",
      "\n",
      "cluster # 500 keywords (from 61 news)\n",
      " > ['제공', '뉴스1', '등록', '2016', '장관', '일본', '기획재정부', '문화체육관광부']\n",
      "\n",
      "cluster # 534 keywords (from 65 news)\n",
      " > ['발행', '금리', '달러', '전망', '가능성', '미국', '하락', '이날', '인상', '규모']\n",
      "\n",
      "cluster # 539 keywords (from 67 news)\n",
      " > ['혐의', '해상', '총리', '조사', '지난', '자리', '적발', '이들', '비서실장', '학교', '것으로', '집단', '검찰', '구속', '아이', '3명']\n",
      "\n",
      "cluster # 541 keywords (from 81 news)\n",
      " > ['대통령', '재단', '의혹', '미르', '엄정', '청와대', '최씨']\n",
      "\n",
      "cluster # 543 keywords (from 78 news)\n",
      " > ['마이데일리', '패션', '사진', '기사', '제공']\n",
      "\n",
      "cluster # 546 keywords (from 67 news)\n",
      " > ['정부서울청사', '종로구', '뉴시스', '사진']\n",
      "\n",
      "cluster # 560 keywords (from 143 news)\n",
      " > ['영화', '관객', '개봉', '시리즈', '감독', '이야기', '지난', '프랑스', '관객들', '원작', '그들', '사람', '주연']\n",
      "\n",
      "cluster # 562 keywords (from 66 news)\n",
      " > ['76', '여의도', '뉴시스', '복귀', '정계', '민주당', '오후', '사진', '대표', '선언', '서울']\n",
      "\n",
      "cluster # 579 keywords (from 52 news)\n",
      " > ['검찰', '롯데', '부회장', '수사', '회장', '인수', '기소']\n",
      "\n",
      "cluster # 581 keywords (from 56 news)\n",
      " > ['저녁', '사람', '식사', '예능', '모습', '방송', '웃음', '실패', '함께']\n",
      "\n",
      "cluster # 593 keywords (from 60 news)\n",
      " > ['문재인', '뉴시스', '더불어민주당', '사진', '대표', '서울']\n",
      "\n",
      "cluster # 597 keywords (from 76 news)\n",
      " > ['조망', '디자이너', '헤라', '펼쳐진다', '패션', '오후', '서울컬렉션', '서울패션위크', '콜렉션']\n",
      "\n",
      "cluster # 621 keywords (from 75 news)\n",
      " > ['여성', '대한', '건강', '여성들', '이번', '지원']\n",
      "\n",
      "cluster # 628 keywords (from 93 news)\n",
      " > ['일본', '해외', '관련', '평균', '보도', '지난', '근무', '업무', '시간', '해도', '정부']\n",
      "\n",
      "cluster # 630 keywords (from 56 news)\n",
      " > ['차량', '사고', '기능', '도로', '자동차', '발견', '것으로', '타고']\n",
      "\n",
      "cluster # 640 keywords (from 51 news)\n",
      " > ['혐의', '걸쳐', '울산', '경찰', '직장', '사용', '자신', '이상', '받고', '구속', '것으로', '경찰관', '뉴시스', '뉴스1', '김씨', '시도', '돈을', '허위', '지난']\n",
      "\n",
      "cluster # 665 keywords (from 66 news)\n",
      " > ['저작권자', '금지', '경제', '의견', '공개', '01', '자세']\n",
      "\n",
      "cluster # 674 keywords (from 127 news)\n",
      " > ['서비스', '예약', '제공', '세계', '시장', '솔루션', '정보', '지원', '기술', '플랫폼', '도입', '광고', '기업', '페이스북', '온라인', '모바일', '로봇', '공유', '적용', '목표', '기기', '통합']\n",
      "\n",
      "cluster # 678 keywords (from 103 news)\n",
      " > ['대출', '부동산', '대책', '수요', '우려', '정부', '금리', '규제', '강화', '거래', '이하', '금융', '주택', '공급', '정책']\n",
      "\n",
      "cluster # 680 keywords (from 119 news)\n",
      " > ['경찰', '공격', '20대', '위반', '사건', '카드', '남성', '불법', '신고', '수사', '경찰관', '시위', '대응', '금지', '시스템', '문제', '용의자', '대구', '관련', '영상', '광주', '사망', '주장', '재배포']\n",
      "\n",
      "cluster # 684 keywords (from 60 news)\n",
      " > ['권리', '국민', '보도', '접근', '의원']\n",
      "\n",
      "cluster # 686 keywords (from 57 news)\n",
      " > ['국정원장', '회고록', '당시', '공개', '관련', '발언', '기록', '북한', '송민순', '진실', '새누리당', '논란', '문재인', '국정원']\n",
      "\n",
      "cluster # 706 keywords (from 177 news)\n",
      " > ['출연', '방송', '이날', '파워', '이에', '웃음', '활동', '가수', '데이트', '라이브', '언급', '라디오', '좋아', '게스트', '이라고', '재배포', '배우', '20일', '질문', '방송화면', '엑스포츠뉴스', '내가', '대답', '정도', '이야기', '기자', '일간스포츠', '솔직', '멤버들', '혼자']\n",
      "\n",
      "cluster # 715 keywords (from 96 news)\n",
      " > ['헤라', '뉴시스', '디자이너', '오후', '컬렉션', '사진', '박정', '서울패션위크']\n",
      "\n",
      "cluster # 716 keywords (from 89 news)\n",
      " > ['김홍범', '걸그룹', '일간스포츠', '박세']\n",
      "\n",
      "cluster # 734 keywords (from 69 news)\n",
      " > ['선고', '징역', '기소', '범행', '혐의', '재판']\n",
      "\n",
      "cluster # 740 keywords (from 112 news)\n",
      " > ['공연', '페스티벌', '예정', '함께', '이번', '축제']\n",
      "\n",
      "cluster # 764 keywords (from 88 news)\n",
      " > ['부산', '행사', '이번', '일자리', '개최', '운영', '추진']\n",
      "\n",
      "cluster # 781 keywords (from 51 news)\n",
      " > ['총기', '인터넷', '사제총', '사용', '인터뷰', '전자발찌', '경찰관']\n",
      "\n",
      "cluster # 789 keywords (from 265 news)\n",
      " > ['체험', '행사', '프로그램', '전시', '진행', '참여', '박람회', '주제', '대회', '축제', '강연', '마련', '참가자들', '취업', '개최', '공모', '이번', '오후', '예정', '음식', '22일', '전국', '청소년', '제공', '문화', '운영', '콘텐츠', '어린', '시간', '소개']\n",
      "\n",
      "cluster # 794 keywords (from 124 news)\n",
      " > ['뉴스1', '서울', '최대', '미세먼지', '회장', '오후', '참여', '피의자', '지난', '21일', '직접', '업계', '송파구', '진행', '코스닥']\n",
      "\n",
      "cluster # 813 keywords (from 187 news)\n",
      " > ['아시아경제', '보는', '기자', '이번', '선정', '실시', '세계']\n",
      "\n",
      "cluster # 824 keywords (from 64 news)\n",
      " > ['가을', '축제', '코스', '프로그램', '여행', '특별', '위치', '운영', '함께']\n",
      "\n",
      "cluster # 830 keywords (from 58 news)\n",
      " > ['할인', '판매', '선보', '상품', '브랜드', '가격', '론칭']\n",
      "\n",
      "cluster # 836 keywords (from 107 news)\n",
      " > ['모습', '함께', '공개', '동영상', '가득', '헤럴드', '영상', '머리', '엄마', '미소', '시리즈', '부부', '공식', '사진', '당신', '모델', '재배포', '유리', '어머니', '시작', '지난', '최근', '카메라', '커플', '자랑', '포착', '개봉', '예정']\n",
      "\n",
      "cluster # 843 keywords (from 104 news)\n",
      " > ['제공', '서울', '연합뉴스', '20일', '삼성전자', '공간']\n",
      "\n",
      "cluster # 848 keywords (from 105 news)\n",
      " > ['인스타그램', '사진', '글과', '자신', '함께', '한편', '활동', '공개']\n",
      "\n",
      "cluster # 851 keywords (from 175 news)\n",
      " > ['사업', '추진', '지원', '지난', '금융', '조성', '마을', '분야', '개발', '설립', '이하', '성과']\n",
      "\n",
      "cluster # 856 keywords (from 93 news)\n",
      " > ['학교', '학생들', '학생', '활동', '지원', '청소년', '아이들']\n",
      "\n",
      "cluster # 857 keywords (from 56 news)\n",
      " > ['한미', '북한', '양국', '압박', '대북', '외교', '국방', '협의체', '확장억제', '대한', '신설']\n",
      "\n",
      "cluster # 862 keywords (from 51 news)\n",
      " > ['상승', '증시', '시장', '대비', '예상', '지수', '실적', '마감']\n",
      "\n",
      "cluster # 891 keywords (from 99 news)\n",
      " > ['기업', '11월', '기술', '지원', '기업들', '시장', '금액', '국가', '발행']\n",
      "\n",
      "cluster # 896 keywords (from 52 news)\n",
      " > ['상승', '기관', '하락', '삼성전자', '상승세', '외국인', '강세', '출발', '마감', '화학', '개인', '이날']\n",
      "\n",
      "cluster # 897 keywords (from 55 news)\n",
      " > ['전자신문', '전문기자']\n",
      "\n",
      "cluster # 898 keywords (from 77 news)\n",
      " > ['교육', '프로그램', '대학', '글로벌', '지원', '개발', '대구', '기관', '계획']\n",
      "\n",
      "cluster # 909 keywords (from 98 news)\n",
      " > ['의원', '국회', '지적', '2014년', '경우', '문제', '이전', '제출', '세금', '정부']\n",
      "\n",
      "cluster # 938 keywords (from 73 news)\n",
      " > ['토론', '클린턴', '대선', '여론조사', '트럼프']\n",
      "\n",
      "cluster # 959 keywords (from 141 news)\n",
      " > ['방송', '시즌', '시청자들', '지금', '프로그램', '뷰티', '정도', '오후', '여성', '지난', '후문', '제작', '8시']\n",
      "\n",
      "cluster # 971 keywords (from 101 news)\n",
      " > ['기술', '사용', '적용', '소재', '병원', '디자인', '선보', '출시', '기존', '겨울', '착용', '스포츠']\n",
      "\n",
      "cluster # 980 keywords (from 56 news)\n",
      " > ['여의도', '뉴스1', '국민의당', '국회', '토론회', '모두발언', '서울', '대표', '오전']\n"
     ]
    }
   ],
   "source": [
    "def print_keywords(interested_clusters, x, int2vocab, clusters_to_rows, C=1, topn=30):\n",
    "    \n",
    "    for cluster_id in interested_clusters:\n",
    "        interested_docs = clusters_to_rows[cluster_id]\n",
    "        print('\\ncluster # %d keywords (from %d news)' % (cluster_id, len(interested_docs)))\n",
    "        y = [1 if i in interested_docs else -1 for i in range(x_2016_1020.shape[0])]\n",
    "        \n",
    "        logistic_l1 = LogisticRegression(penalty='l1', C=C)\n",
    "        logistic_l1.fit(x, y)\n",
    "\n",
    "        keywords = sorted(enumerate(logistic_l1.coef_[0]), key=lambda x:x[1], reverse=True)[:topn]\n",
    "        keywords = [int2vocab[word] for word, score in keywords if score > 0]\n",
    "        print(' > %s' % (keywords))\n",
    "        \n",
    "interested_clusters = [i for i, labels in clusters_to_rows.items() if 50 < len(labels) < 500]\n",
    "print_keywords(interested_clusters, x_2016_1020, int2vocab, clusters_to_rows, C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
