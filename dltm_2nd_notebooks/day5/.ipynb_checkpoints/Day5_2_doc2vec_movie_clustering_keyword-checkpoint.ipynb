{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 좀 더 큰 영화리뷰 데이터를 이용하여 리뷰가 비슷한 영화들에 대한 군집화를 수행하고, 각 군집의 키워드들을 추출해 보겠습니다. \n",
    "\n",
    "Day5_2에서는 해당 군집에 해당하는 문서를 하나의 문서로 합치는 방법을 선택하였으며, Day5_3에서는 각 뉴스 문서별로 label을 부여하는 방식도 소개하겠습니다. 두 방식은 classification을 학습할 때의 imbalance의 규모의 차이가 생길 수 있습니다. \n",
    "\n",
    "아래에는 doc2vec, k-means를 학습할 것인지, 학습된 k-means로부터 같은 군집에 해당하는 영화들의 리뷰를 하나로 합쳐둔 corpus와 term frequency matrix를 만들 것인지 정하는 parameters를 설정해둡니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DOC2VEC = False\n",
    "TRAIN_KMEANS = False\n",
    "CREATE_MERGED_CORPUS = False\n",
    "CREATED_MERGED_CORPUS_TERMFREQUENCY_MATRIX = False\n",
    "\n",
    "tokenized_corpus_fname = '../../../data/sample_naver_movie_5000/merged_comments_tokenized.txt'\n",
    "doc2vec_fname = '../../../data/sample_naver_movie_5000/movie_review_doc2vec_model.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN_DOC2VEC=True 이면, gensim으로부터 Doc2Vec을 import하여 학습을 진행합니다. TRAIN_DOC2VEC=False이면 학습된 모델을 loading 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if TRAIN_DOC2VEC:\n",
    "    import pickle\n",
    "    from gensim.models import Doc2Vec\n",
    "    from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "    class CommentDoc2Vec:\n",
    "        def __init__(self, fname):\n",
    "            self.fname = fname\n",
    "        def __iter__(self):\n",
    "            with open(self.fname, encoding='utf-8') as f:\n",
    "                for doc in f:\n",
    "                    movie_idx, text, score = doc.split('\\t')\n",
    "                    yield TaggedDocument(words=text.split(), tags=['MOVIE_%s' % movie_idx])\n",
    "                    \n",
    "    doc2vec_corpus = CommentDoc2Vec(tokenized_corpus_fname)                    \n",
    "    doc2vec_model = Doc2Vec(doc2vec_corpus)\n",
    "    with open(doc2vec_fname, 'wb') as f:\n",
    "        pickle.dump(doc2vec_model, f)\n",
    "\n",
    "else:\n",
    "    import pickle\n",
    "    with open(doc2vec_fname, 'rb') as f:\n",
    "        doc2vec_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{영화 id: 영화 이름}의 dictionary를 pickle로부터 읽어옵니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../../../data/sample_naver_movie/navermovie_info_idx2moviename.pkl', 'rb') as f:\n",
    "    id2movie = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec 모델에 저장되어있는 docvecc의 각 document vector를 l2로 row normalization을 수행합니다. 그 뒤, KMeans.fit_predict()를 이용하여 군집화를 수행함으로써 Spherical k-means처럼 군집화를 수행합니다. \n",
    "\n",
    "    movie_vectors = normalize(doc2vec_model.docvecs.doctag_syn0, axis=1, norm='l2')\n",
    "    kmeans = KMeans(n_clusters=100, max_iter=20, n_init=1, verbose=1)\n",
    "\n",
    "그 결과는 clusters로 return 받습니다. \n",
    "\n",
    "    clusters = kmeans.fit_predict(movie_vectors)\n",
    "    \n",
    "아래는 iteration 별 inertia (training cost)입니다. Iteration의 횟수가 올라갈수록 cost는 줄어들지만, 감소폭은 거의 0에 가까워짐을 알 수 있습니다. 이는 centroids가 거의 이동하지 않음을, 즉 군집화의 결과가 거의 수렴했음을 알려줍니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Initialization complete\n",
    "    Iteration  0, inertia 4497.911\n",
    "    Iteration  1, inertia 2738.942\n",
    "    Iteration  2, inertia 2688.905\n",
    "    Iteration  3, inertia 2673.329\n",
    "    Iteration  4, inertia 2664.931\n",
    "    Iteration  5, inertia 2659.820\n",
    "    Iteration  6, inertia 2656.833\n",
    "    Iteration  7, inertia 2654.575\n",
    "    Iteration  8, inertia 2653.216\n",
    "    Iteration  9, inertia 2652.165\n",
    "    Iteration 10, inertia 2651.355\n",
    "    Iteration 11, inertia 2650.615\n",
    "    Iteration 12, inertia 2649.655\n",
    "    Iteration 13, inertia 2649.006\n",
    "    Iteration 14, inertia 2648.559\n",
    "    Iteration 15, inertia 2648.248\n",
    "    Iteration 16, inertia 2648.135\n",
    "    Iteration 17, inertia 2647.871\n",
    "    Iteration 18, inertia 2647.775\n",
    "    Iteration 19, inertia 2647.712\n",
    "    Converged at iteration 19\n",
    "    CPU times: user 3.11 s, sys: 28 ms, total: 3.14 s\n",
    "    Wall time: 609 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 4496.306\n",
      "Iteration  1, inertia 2741.853\n",
      "Iteration  2, inertia 2688.789\n",
      "Iteration  3, inertia 2670.471\n",
      "Iteration  4, inertia 2660.161\n",
      "Iteration  5, inertia 2654.474\n",
      "Iteration  6, inertia 2650.966\n",
      "Iteration  7, inertia 2648.823\n",
      "Iteration  8, inertia 2647.478\n",
      "Iteration  9, inertia 2646.863\n",
      "Iteration 10, inertia 2646.081\n",
      "Iteration 11, inertia 2645.547\n",
      "Iteration 12, inertia 2645.133\n",
      "Iteration 13, inertia 2644.814\n",
      "Iteration 14, inertia 2644.550\n",
      "Iteration 15, inertia 2644.359\n",
      "Iteration 16, inertia 2644.201\n",
      "Iteration 17, inertia 2643.939\n",
      "Iteration 18, inertia 2643.801\n",
      "Iteration 19, inertia 2643.661\n",
      "CPU times: user 2.53 s, sys: 28 ms, total: 2.56 s\n",
      "Wall time: 453 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if TRAIN_KMEANS:\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.preprocessing import normalize\n",
    "\n",
    "    movie_vectors = normalize(doc2vec_model.docvecs.doctag_syn0, axis=1, norm='l2')\n",
    "    kmeans = KMeans(n_clusters=100, max_iter=20, n_init=1, verbose=1)\n",
    "    clusters = kmeans.fit_predict(movie_vectors)\n",
    "\n",
    "    import pickle\n",
    "    with open('../../../data/sample_naver_movie_5000/kmeans_100.pkl', 'wb') as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    with open('../../../data/sample_naver_movie_5000/kmeans_100_label', 'wb') as f:\n",
    "        pickle.dump(clusters, f)\n",
    "        \n",
    "else:\n",
    "    with open('../../../data/sample_naver_movie_5000/kmeans_100.pkl', 'rb') as f:\n",
    "        kmeans = pickle.load(f)\n",
    "        \n",
    "    with open('../../../data/sample_naver_movie_5000/kmeans_100_label', 'rb') as f:\n",
    "        clusters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "군집별로 어떤 영화들이 묶이는지 확인할 수 있는 cluster_to_row를 만듦니다. 그리고 군집화의 결과가 균형적인지 (= 각 군집에 할당된 영화의 개수가 너무 많거나 너무 적지 않은지) 확인도 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster # 0 has 37 moveis\n",
      "cluster # 1 has 50 moveis\n",
      "cluster # 2 has 28 moveis\n",
      "cluster # 3 has 25 moveis\n",
      "cluster # 4 has 59 moveis\n",
      "cluster # 5 has 32 moveis\n",
      "cluster # 6 has 45 moveis\n",
      "cluster # 7 has 62 moveis\n",
      "cluster # 8 has 19 moveis\n",
      "cluster # 9 has 124 moveis\n",
      "cluster # 10 has 110 moveis\n",
      "cluster # 11 has 72 moveis\n",
      "cluster # 12 has 29 moveis\n",
      "cluster # 13 has 26 moveis\n",
      "cluster # 14 has 37 moveis\n",
      "cluster # 15 has 37 moveis\n",
      "cluster # 16 has 72 moveis\n",
      "cluster # 17 has 70 moveis\n",
      "cluster # 18 has 88 moveis\n",
      "cluster # 19 has 46 moveis\n",
      "cluster # 20 has 28 moveis\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "cluster_to_row = defaultdict(lambda: [])\n",
    "for row_id, label in enumerate(clusters):\n",
    "    cluster_to_row[label].append(row_id)\n",
    "    \n",
    "cluster_to_row = dict(cluster_to_row)\n",
    "for label, rows in cluster_to_row.items():\n",
    "    if label > 20: continue\n",
    "    print('cluster # %d has %d moveis' % (label, len(rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec.docvecs.doctags로부터 각 row id가 어떤 영화에 해당하는지를 확인하는 list를 만듦니다. 자세한 설명은 Day5_1에 적혀 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cluster # 80 (num movies = 40)\n",
      "  > 쥬만지\n",
      "  > 베이비 데이 아웃\n",
      "  > 빅\n",
      "  > 엽기적인 그녀\n",
      "  > 미이라\n",
      "  > 볼케이노\n",
      "  > 비틀쥬스\n",
      "  > 잭\n",
      "  > 단테스 피크\n",
      "  > 솔드 아웃\n",
      "\n",
      "cluster # 81 (num movies = 29)\n",
      "  > 코렐라인: 비밀의 문\n",
      "  > 점박이 : 한반도의 공룡3D\n",
      "  > 프렌즈 : 몬스터 섬의 비밀 3D\n",
      "  > 호두까기인형 3D\n",
      "  > 메리다와 마법의 숲\n",
      "  > 드래곤 헌터\n",
      "  > 파닥파닥\n",
      "  > 해피 피트 2\n",
      "  > 크루즈 패밀리\n",
      "  > 겨울왕국\n",
      "\n",
      "cluster # 82 (num movies = 56)\n",
      "  > 스트레인저 댄 픽션\n",
      "  > 당신이 그녀라면\n",
      "  > 라스트 홀리데이\n",
      "  > 로봇 앤 프랭크\n",
      "  > 파란 자전거\n",
      "  > 맨 온 더 문\n",
      "  > 로드 투 퍼디션\n",
      "  > 프라미스드 랜드\n",
      "  > 스트레이트 스토리\n",
      "  > 저지 걸\n",
      "\n",
      "cluster # 83 (num movies = 20)\n",
      "  > 토르: 천둥의 신\n",
      "  > 토르: 다크 월드\n",
      "  > 저스티스 리그\n",
      "  > 어벤져스: 인피니티 워\n",
      "  > 로보캅\n",
      "  > 킥 애스 2: 겁 없는 녀석들\n",
      "  > 스파이더맨: 홈커밍\n",
      "  > 배트맨 4 - 배트맨과 로빈\n",
      "  > 블랙 라이트닝\n",
      "  > 맨 오브 스틸\n",
      "\n",
      "cluster # 84 (num movies = 30)\n",
      "  > 커피 프린스 1호점\n",
      "  > 빠담빠담… 그와 그녀의 심장박동소리\n",
      "  > 여인의 향기\n",
      "  > 불멸의 이순신\n",
      "  > 연애시대\n",
      "  > 내 이름은 김삼순\n",
      "  > 공주의 남자\n",
      "  > 마이 걸\n",
      "  > 네 멋대로 해라\n",
      "  > 미안하다, 사랑한다\n",
      "\n",
      "cluster # 85 (num movies = 56)\n",
      "  > 아이언맨 2\n",
      "  > 타이탄의 분노\n",
      "  > 포세이돈\n",
      "  > 2012\n",
      "  > 300\n",
      "  > 토탈 리콜\n",
      "  > 스파이더맨 3\n",
      "  > 혹성탈출: 진화의 시작\n",
      "  > 스피드 레이서\n",
      "  > 킹콩\n",
      "\n",
      "cluster # 86 (num movies = 52)\n",
      "  > 터미네이터\n",
      "  > 혹성탈출\n",
      "  > 에이리언 2\n",
      "  > 레이더스\n",
      "  > 인디아나 존스\n",
      "  > 스타워즈 에피소드 5 - 제국의 역습\n",
      "  > 6번째 날\n",
      "  > 인디아나 존스 - 최후의 성전\n",
      "  > 제5원소\n",
      "  > 고질라\n",
      "\n",
      "cluster # 87 (num movies = 90)\n",
      "  > 22 블렛\n",
      "  > 식스티 세컨즈\n",
      "  > 컨뎀드\n",
      "  > 저지 드레드\n",
      "  > S.W.A.T. 특수기동대\n",
      "  > 와일드 카드\n",
      "  > 펠햄 123\n",
      "  > 코난 : 암흑의 시대\n",
      "  > 컴 아웃 파이팅\n",
      "  > 호스티지\n",
      "\n",
      "cluster # 88 (num movies = 62)\n",
      "  > 레지던트 이블: 파멸의 날\n",
      "  > 신과함께\n",
      "  > 사랑\n",
      "  > 캐리비안의 해적 5: 죽은 자는 말이 없다\n",
      "  > 우주 전쟁\n",
      "  > 뜨거운 것이 좋아\n",
      "  > 세븐 데이즈\n",
      "  > 타짜\n",
      "  > 7년의 밤\n",
      "  > 크로싱\n",
      "\n",
      "cluster # 89 (num movies = 115)\n",
      "  > 내 남자의 순이\n",
      "  > 구세주 2\n",
      "  > 위대한 유산\n",
      "  > 스카우트\n",
      "  > 김관장 대 김관장 대 김관장\n",
      "  > 조폭 마누라 3\n",
      "  > 달콤, 살벌한 연인\n",
      "  > 투사부일체\n",
      "  > 걸스카우트\n",
      "  > 유감스러운 도시\n",
      "\n",
      "cluster # 90 (num movies = 67)\n",
      "  > 미드나잇 미트 트레인\n",
      "  > 나이트메어\n",
      "  > 하이 레인\n",
      "  > 데스티네이션\n",
      "  > 블러디 발렌타인\n",
      "  > 데스워치\n",
      "  > None\n",
      "  > 피스트\n",
      "  > 메이\n",
      "  > 떼시스\n",
      "\n",
      "cluster # 91 (num movies = 47)\n",
      "  > 마돈나\n",
      "  > 돌이킬 수 없는\n",
      "  > 돈 크라이 마미\n",
      "  > 공정사회\n",
      "  > 거인\n",
      "  > 한공주\n",
      "  > 다우더\n",
      "  > 모범생\n",
      "  > 들개\n",
      "  > 소셜포비아\n",
      "\n",
      "cluster # 92 (num movies = 10)\n",
      "  > 메탈리카 스루 더 네버\n",
      "  > 서태지밴드 라이브 투어 더 뫼비우스\n",
      "  > 오페라의 유령 : 25주년 특별 공연\n",
      "  > 퀸 락 몬트리올\n",
      "  > 마비노기\n",
      "  > 마이클 잭슨의 디스 이즈 잇\n",
      "  > 빌리 엘리어트 뮤지컬 라이브\n",
      "  > 서태지 심포니\n",
      "  > 블랙 가스펠\n",
      "  > 아마존의 눈물\n",
      "\n",
      "cluster # 93 (num movies = 28)\n",
      "  > 파이스토리 : 악당상어 소탕작전\n",
      "  > 슈퍼노바 지구 탈출기\n",
      "  > 세이빙 산타\n",
      "  > 링스 어드벤처\n",
      "  > 테드: 황금도시 파이티티를 찾아서\n",
      "  > 극장판 파워레인저:캡틴포스 VS 미라클포스 199 히어로 대결전\n",
      "  > 더 자이언트\n",
      "  > 홍길동 2084\n",
      "  > 진바오의 모험\n",
      "  > 저스틴\n",
      "\n",
      "cluster # 94 (num movies = 44)\n",
      "  > 태풍\n",
      "  > 무극\n",
      "  > 성룡의 신화\n",
      "  > 영웅 :  천하의 시작\n",
      "  > 비천무\n",
      "  > 적벽대전 1부 - 거대한 전쟁의 시작\n",
      "  > 대상해\n",
      "  > 검우강호\n",
      "  > 적인걸2: 신도해왕의 비밀\n",
      "  > 서울공략\n",
      "\n",
      "cluster # 95 (num movies = 53)\n",
      "  > 마의\n",
      "  > 아이리스 2\n",
      "  > 궁\n",
      "  > 사랑비\n",
      "  > 난폭한 로맨스\n",
      "  > 최고다 이순신\n",
      "  > 대물\n",
      "  > 태왕사신기\n",
      "  > 메이퀸\n",
      "  > 하이킥! 짧은 다리의 역습\n",
      "\n",
      "cluster # 96 (num movies = 48)\n",
      "  > 캠퍼스 S 커플\n",
      "  > 돼지 같은 여자\n",
      "  > 청춘정담\n",
      "  > 사랑의 가위바위보\n",
      "  > 사랑해! 진영아\n",
      "  > 네버다이 버터플라이\n",
      "  > 연애의 온도\n",
      "  > 그댄 나의 뱀파이어\n",
      "  > 우리들의 헤어진 여자친구\n",
      "  > 마이 블랙 미니드레스\n",
      "\n",
      "cluster # 97 (num movies = 23)\n",
      "  > 취권 2\n",
      "  > 취권\n",
      "  > 폴리스 스토리\n",
      "  > 성룡의 신주쿠 살인사건\n",
      "  > CIA\n",
      "  > 대병소장\n",
      "  > 포비든 킹덤 - 전설의 마스터를 찾아서\n",
      "  > 턱시도\n",
      "  > 80일간의 세계일주\n",
      "  > 러시 아워 2\n",
      "\n",
      "cluster # 98 (num movies = 55)\n",
      "  > 갱스 오브 뉴욕\n",
      "  > 엘리트 스쿼드\n",
      "  > 에비에이터\n",
      "  > 체인질링\n",
      "  > 콜래트럴\n",
      "  > 3:10 투 유마\n",
      "  > 로드 오브 워\n",
      "  > 트레이터\n",
      "  > 세븐\n",
      "  > 에너미 오브 스테이트\n",
      "\n",
      "cluster # 99 (num movies = 51)\n",
      "  > 데스티네이션 2\n",
      "  > 스타쉽 트루퍼스 3\n",
      "  > 콜렉션\n",
      "  > 큐브 제로\n",
      "  > 레지던트 이블 5 : 최후의 심판 3D\n",
      "  > 쏘우 - 여섯번의 기회\n",
      "  > 쏘우 4\n",
      "  > 더 퍼지:거리의 반란\n",
      "  > 데드 캠프 2\n",
      "  > 파라노말 액티비티 3\n"
     ]
    }
   ],
   "source": [
    "row2movie_id = sorted(doc2vec_model.docvecs.doctags.items(), key=lambda x:x[1].offset)\n",
    "row2movie_id = [row[0].split('_')[1] for row in row2movie_id]\n",
    "row2movie_name = [id2movie.get(row, None) for row in row2movie_id]\n",
    "\n",
    "for label, rows in cluster_to_row.items():\n",
    "    if label < 80: continue\n",
    "    print('\\ncluster # %d (num movies = %d)' % (label, len(rows)))    \n",
    "    for row in rows[:10]:\n",
    "        print('  > %s' % row2movie_name[row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "군집 # 99번은 공포영화들이 섞여 있음을 살펴볼 수 있습니다. \n",
    "    \n",
    "    cluster # 99\n",
    "      > 데스티네이션 2\n",
    "      > 스타쉽 트루퍼스 3\n",
    "      > 콜렉션\n",
    "      > 큐브 제로\n",
    "      > 레지던트 이블 5 : 최후의 심판 3D\n",
    "      > 쏘우 - 여섯번의 기회\n",
    "      > 쏘우 4\n",
    "      > 더 퍼지:거리의 반란\n",
    "      > 데드 캠프 2\n",
    "      > 파라노말 액티비티 3\n",
    "      > 나비 효과 2\n",
    "      > 알.이.씨 3: 제네시스\n",
    "      ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cluster # 99\n",
      "  > 데스티네이션 2\n",
      "  > 스타쉽 트루퍼스 3\n",
      "  > 콜렉션\n",
      "  > 큐브 제로\n",
      "  > 레지던트 이블 5 : 최후의 심판 3D\n",
      "  > 쏘우 - 여섯번의 기회\n",
      "  > 쏘우 4\n",
      "  > 더 퍼지:거리의 반란\n",
      "  > 데드 캠프 2\n",
      "  > 파라노말 액티비티 3\n",
      "  > 나비 효과 2\n",
      "  > 알.이.씨 3: 제네시스\n",
      "  > 쥬라기 공원 3\n",
      "  > 데드 캠프 3\n",
      "  > 알.이.씨4: 아포칼립스\n",
      "  > 알.이.씨 2\n",
      "  > 배틀 로얄 2 - 레퀴엠\n",
      "  > 사일런트 힐: 레버레이션\n",
      "  > 행오버 2\n",
      "  > 커스 오브 처키\n",
      "  > 디센트: Part 2\n",
      "  > 힐즈 아이즈 2\n",
      "  > 나비효과: 레버레이션\n",
      "  > 쏘우 3D\n",
      "  > 에이리언 3\n",
      "  > 데드 캠프4\n",
      "  > 파이널 데스티네이션 5\n",
      "  > 사탄의 인형 5 - 씨드 오브 처키\n",
      "  > 피라냐 3DD\n",
      "  > 레지던트 이블 4: 끝나지 않은 전쟁 3D\n",
      "  > 쏘우 5\n",
      "  > 네 무덤에 침을 뱉어라 2\n",
      "  > 파이널 데스티네이션 4\n",
      "  > 지퍼스 크리퍼스 2\n",
      "  > 에이리언 VS. 프레데터 2\n",
      "  > 에이리언 4\n",
      "  > 스크림 4G\n",
      "  > 휴먼 센티피드 2\n",
      "  > 아드레날린 24 2\n",
      "  > None\n",
      "  > 원초적 본능 2\n",
      "  > 쥬라기 공원 2 - 잃어버린 세계\n",
      "  > 스타쉽 트루퍼스 2\n",
      "  > 쏘우 3\n",
      "  > 데드 캠프 5\n",
      "  > 프레데터 2\n",
      "  > 데스티네이션 3 - 파이널 데스티네이션\n",
      "  > 쏘우 2\n",
      "  > 큐브 2\n",
      "  > 아나콘다 2 - 사라지지 않는 저주\n",
      "  > 텍사스 전기톱 연쇄살인사건 - 0\n"
     ]
    }
   ],
   "source": [
    "print('\\ncluster # %d' % 99)\n",
    "for row in cluster_to_row[99]:\n",
    "    print('  > %s' % row2movie_name[row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "군집 # 84에는 한국 드라마들이 많이 있으며, 허준, 동이와 같은 사극이나 경성 스캔들과 같은 시대배경이 예전/고전인 드라마들 + 밝은 풍의 연애/성장 드라마들이 섞여 있음을 볼 수 있습니다. \n",
    "    \n",
    "    cluster # 84\n",
    "      > 커피 프린스 1호점\n",
    "      > 빠담빠담… 그와 그녀의 심장박동소리\n",
    "      > 여인의 향기\n",
    "      > 불멸의 이순신\n",
    "      > 연애시대\n",
    "      > 내 이름은 김삼순\n",
    "      > 공주의 남자\n",
    "      > 마이 걸\n",
    "      > 네 멋대로 해라\n",
    "      > 미안하다, 사랑한다\n",
    "      > 경성 스캔들\n",
    "      > 공부의 신\n",
    "      > 하얀 거탑\n",
    "      > 그들이 사는 세상\n",
    "      > 허준\n",
    "      > 동이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cluster # 84\n",
      "  > 커피 프린스 1호점\n",
      "  > 빠담빠담… 그와 그녀의 심장박동소리\n",
      "  > 여인의 향기\n",
      "  > 불멸의 이순신\n",
      "  > 연애시대\n",
      "  > 내 이름은 김삼순\n",
      "  > 공주의 남자\n",
      "  > 마이 걸\n",
      "  > 네 멋대로 해라\n",
      "  > 미안하다, 사랑한다\n",
      "  > 경성 스캔들\n",
      "  > 공부의 신\n",
      "  > 하얀 거탑\n",
      "  > 그들이 사는 세상\n",
      "  > 허준\n",
      "  > 동이\n",
      "  > 오만과 편견\n",
      "  > 49일\n",
      "  > 일지매\n",
      "  > 정도전\n",
      "  > 베토벤 바이러스\n",
      "  > 대장금\n",
      "  > 쾌걸 춘향\n",
      "  > 개와 늑대의 시간\n",
      "  > 선덕여왕\n",
      "  > 추노\n",
      "  > 최고의 사랑\n",
      "  > 파스타\n",
      "  > 부활\n",
      "  > 환상의 커플\n"
     ]
    }
   ],
   "source": [
    "print('\\ncluster # %d' % 84)\n",
    "for row in cluster_to_row[84]:\n",
    "    print('  > %s' % row2movie_name[row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 영화가 어떤 군집에 해당하는지를 볼 수 있는 dict를 만듦니다. \n",
    "\n",
    "row2movie_id는 각 row가 어떤 영화인지를 알려주고, clusters는 각 row가 어떤 군집인지를 알려주므로, 이를 아래와 같이 zip을 통하여 함께 for loop을 돌면서 dict의 key:value로 만들어줍니다. \n",
    "\n",
    "    for movie_id, cluster_label in zip(row2movie_id, clusters) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_id_to_cluster = {movie_id:cluster_label for movie_id, cluster_label \n",
    "                       in zip(row2movie_id, clusters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "../data/sample_naver_movie_5000/ 아래에 각 군집별/영화별 merged corpus를 만들어 두었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4914\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "if CREATE_MERGED_CORPUS:\n",
    "    with open(tokenized_corpus_fname, encoding='utf-8') as f:\n",
    "        docs = [doc.strip().split('\\t') for doc in f]\n",
    "        movie_idxs, texts, scores = zip(*docs)\n",
    "\n",
    "    from collections import defaultdict\n",
    "    merged_by_movie = defaultdict(lambda: [])\n",
    "    merged_by_cluster = defaultdict(lambda: [])\n",
    "\n",
    "    for movie_idx, text in zip(movie_idxs, texts):\n",
    "        merged_by_movie[movie_idx].append(text)\n",
    "        \n",
    "        cluster_idx = movie_id_to_cluster[movie_idx]\n",
    "        merged_by_cluster[cluster_idx].append(text)\n",
    "\n",
    "    print(len(merged_by_movie))\n",
    "    print(len(merged_by_cluster))\n",
    "    \n",
    "    with open('../../../data/sample_naver_movie_5000/row_as_movie_comments.txt', 'w', encoding='utf-8') as fd:\n",
    "        with open('../../../data/sample_naver_movie_5000/row_as_movie_comments_movieidx.txt', 'w', encoding='utf-8') as fi:\n",
    "            for movie_id in row2movie_id:\n",
    "                texts = merged_by_movie[movie_id]\n",
    "                texts = ' '.join(texts)\n",
    "                fd.write('%s\\n' % texts)\n",
    "                fi.write('%s\\n' % movie_id)\n",
    "        \n",
    "    with open('../../../data/sample_naver_movie_5000/row_as_cluster_comments.txt', 'w', encoding='utf-8') as fd:\n",
    "        with open('../../../data/sample_naver_movie_5000/row_as_cluster_comments_clusterid.txt', 'w', encoding='utf-8') as fi:\n",
    "            for cluster_id in range(len(merged_by_cluster)):\n",
    "                texts = merged_by_cluster[cluster_id]\n",
    "                texts = ' '.join(texts)\n",
    "                fd.write('%s\\n' % texts)\n",
    "                fi.write('%s\\n' % movie_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영화별/군집별 merged corpus는 하나의 row가 하나의 영화/군집의 리뷰들의 합입니다. 이를 CountVectorizer를 이용하여 term frequency matrix로 만들겠습니다. \n",
    "\n",
    "Doc2Vec은 각 영화를 리뷰의 기준으로 비슷한 영화들이 비슷한 임베딩 공간에서의 좌표값을 가지도록 만들어주지만, 어떤 단어가 그 영화에 더 자주 등장했는지 알려주는 정보들은 없습니다. 단어 level에서 특징을 확인하기 위해서는 다시 한 번 term frequency matrix를 만들어야 합니다. \n",
    "\n",
    "나중에 term frequency matrix의 각 항목이 어떤 단어인지 확인하기 위하여 vectorizer.vocabulary_의 {단어: 단어 index} dict를 pickling 해둡니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if CREATED_MERGED_CORPUS_TERMFREQUENCY_MATRIX:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from scipy.io import mmwrite\n",
    "    import pickle\n",
    "    \n",
    "    fname = '../../../data/sample_naver_movie_5000/row_as_movie_comments.txt'\n",
    "    mm_fname = '../../../data/sample_naver_movie_5000/row_as_movie_comments.mm'\n",
    "    vocab_fname = '../../../data/sample_naver_movie_5000/row_as_movie_comments_vocab.pkl'\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        docs = [doc.strip() for doc in f]\n",
    "        \n",
    "    vectorizer = CountVectorizer(min_df=0.01)\n",
    "    x_by_movie = vectorizer.fit_transform(docs)\n",
    "    mmwrite(mm_fname, x_by_movie)\n",
    "    with open(vocab_fname, 'wb') as f:\n",
    "        pickle.dump(vectorizer.vocabulary_, f)\n",
    "    \n",
    "    \n",
    "    fname = '../../../data/sample_naver_movie_5000/row_as_cluster_comments.txt'\n",
    "    mm_fname = '../../../data/sample_naver_movie_5000/row_as_cluster_comments.mm'\n",
    "    vocab_fname = '../../../data/sample_naver_movie_5000/row_as_cluster_comments_vocab.pkl'\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        docs = [doc.strip() for doc in f]\n",
    "        \n",
    "    vectorizer = CountVectorizer()\n",
    "    x_by_cluster = vectorizer.fit_transform(docs)\n",
    "    mmwrite(mm_fname, x_by_cluster)\n",
    "    with open(vocab_fname, 'wb') as f:\n",
    "        pickle.dump(vectorizer.vocabulary_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 군집별로 영화 리뷰를 묶은 뒤 만든 term frequency matrix를 로딩합니다. \n",
    "\n",
    "군집의 크기가 100개이고, 사용된 단어에 frequency filtering 등을 하지 않았기 때문에 x의 shape이 (100, 332582)으로 매우 큰 고차원임을 확인할 수 있습니다. \n",
    "\n",
    "미리 vocabs로부터 \"단어 index --> 단어\"로 이용할 수 있는 int2vocab list를 만들어 둡니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 332582)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_fname = '../../../data/sample_naver_movie_5000/row_as_cluster_comments.mm'\n",
    "vocab_fname = '../../../data/sample_naver_movie_5000/row_as_cluster_comments_vocab.pkl'\n",
    "\n",
    "from scipy.io import mmread\n",
    "import pickle\n",
    "\n",
    "\n",
    "x = mmread(mm_fname)\n",
    "print(x.shape)\n",
    "\n",
    "with open(vocab_fname, 'rb') as f:\n",
    "    vocabs = pickle.load(f)\n",
    "    int2vocab = sorted(vocabs.items(), key=lambda x:x[1])\n",
    "    int2vocab = [word for word, idx in int2vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day3에서 연습하였던 L1 Regularized Logistic Regression (Lasso Regression)을 이용하여 각 영화 군집의 키워드를 선택해 보겠습니다. y는 각 영화 군집의 id를 넣어주면 됩니다. \n",
    "\n",
    "이렇게 할 경우에는 한 번에 100개의 영화 군집에 대한 키워드를 모두 학습하는 L1 Softmax Regression 이 됩니다. Day5_3에서는 한번에 하나의 군집에 대해서만 키워드를 추출할 수 있는 방법을 연습해 보겠습니다. \n",
    "\n",
    "Multi class classification에서 one vs others와 one vs one은 학습의 편리함과 정확도 사이에서 trade off가 있습니다. one vs one은 여러 개의 classifier를 매번 학습해야 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 332582)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y = list(range(x.shape[0]))\n",
    "logistic_l1 = LogisticRegression(penalty='l1', C=20)\n",
    "logistic_l1.fit(x, y)\n",
    "print(logistic_l1.coef_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "99번 군집은 공포영화이기 때문에 '쏘우', '스릴', '잔인', '아이들이' (아마도 아이들이 보기에 잔인한) 등의 단어들이 등장합니다. 또한 시리즈 물들이 묶여 있는 것 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쏘우\n",
      "전편\n",
      "3편\n",
      "1편\n",
      "1편보다\n",
      "전작\n",
      "반전\n",
      "2편\n",
      "잔인\n",
      "관람객\n",
      "시리즈\n",
      "스릴\n",
      "연기\n",
      "비해\n",
      "아이들이\n",
      "아이들\n",
      "애들\n",
      "기대\n",
      "긴장\n",
      "00\n"
     ]
    }
   ],
   "source": [
    "coef_99 = list(enumerate(logistic_l1.coef_[99,:]))\n",
    "keyword_99 = sorted(coef_99, key=lambda x:x[1], reverse=True)[:100]\n",
    "for word, score in keyword_99:\n",
    "    print(int2vocab[word])\n",
    "    if score == 0: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "84번 영화 군집은 감동, 반전이 있는 드라마라키워드가 추출됩니다. 하지만 둘 모두, 키워드 스럽지 않은 단어들이 선택되는 경향이 있습니다.\n",
    "\n",
    "L1 regularized regression에서는 coefficient가 0이면 classification에 해당 feature (=단어)를 이용하지 않겠다는 의미입니다. coefficient의 크기 순으로 정렬을 하였기 때문에 score가 0이 되면 더이상 출력을 하지 않습니다. \n",
    "\n",
    "    if score == 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사극\n",
      "드라마\n",
      "반전\n",
      "최고\n",
      "관람객\n",
      "감동\n",
      "연기\n",
      "다시\n",
      "역사\n",
      "하정우\n",
      "습니다\n",
      "00\n"
     ]
    }
   ],
   "source": [
    "coef_84 = list(enumerate(logistic_l1.coef_[84,:]))\n",
    "keyword_84 = sorted(coef_84, key=lambda x:x[1], reverse=True)[:100]\n",
    "for word, score in keyword_84:\n",
    "    print(int2vocab[word])\n",
    "    if score == 0: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 모든 단어를 이용하지 않고, 모든 문서에서 등장한 빈도수가 1000 이상인 단어들만 선택하여 각 군집의 키워들을 추출하겠습니다. \n",
    "\n",
    "    x.sum(axis=0)\n",
    "    \n",
    "은 column 기준으로 row sum을 한 것입니다. 이 결과를 list로 바꾼 뒤 enumerate를 돌면서 frequency가 1000이 넘는 단어의 index i를 word_atleast_1000에 넣어둡니다. \n",
    "\n",
    "그 결과 32만개의 단어 중에서 7223개의 단어가 선택되었습니다. \n",
    "\n",
    "mmread()의 return은 Scipy의 Sparse Matrix 중 COO Matrix입니다. Sparse Matrix는 다양한 종류가 있습니다. 하지만 coo matrix는 matrix[:,j]와 같은 slice가 되지 않습니다. slice가 가능한 CSR Matrix로 그 형식을 바꾼 뒤, 빈도수가 1000이 넘는 단어 리스트만을 선택한 sub-matrix, x_atleast_1000를 만듧니다. \n",
    "\n",
    "    x.tocsr()[:, word_atleast_1000]\n",
    "    \n",
    "이미 x의 type이 csr matrix라면 아래와 같이 slice 하면 됩니다. \n",
    "\n",
    "    x[:, word_atleast_1000]\n",
    "    \n",
    "그 결과 (100, 7223)의 부분행렬이 만들어짐을 볼 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 21, 23, 101, 184, 243, 394, 428, 492] ...\n",
      "7223\n",
      "(100, 7223)\n"
     ]
    }
   ],
   "source": [
    "word_atleast_1000 = x.sum(axis=0)[0,:].tolist()[0]\n",
    "word_atleast_1000 = [i for i, freq in enumerate(word_atleast_1000) if freq >= 1000]\n",
    "print(word_atleast_1000[:10], '...')\n",
    "print(len(word_atleast_1000))\n",
    "\n",
    "x_atleast_1000 = x.tocsr()[:, word_atleast_1000]\n",
    "print(x_atleast_1000.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 이용하여 다시 한 번 99번과 84번 군집의 키워드를 추출하였습니다. 이 때 아래처럼 단어를 print하는 함수가 바뀌었습니다. \n",
    "\n",
    "    print(int2vocab[word])\n",
    "    \n",
    "    --> print(int2vocab[word_atleast_1000[word]])\n",
    "    \n",
    "이는, 앞에서는 모든 단어를 포함하는 term frequency matrix, x를 이용하였지만, x_atleast_1000는 그 중에서 빈도수가 1000이 넘는 단어들만 포함되어 있습니다. 그리고 column index는 x와 다릅니다. x_atleast_1000의 column index에 해당하는 단어들은 위에서 정의했던 word_atleast_1000에 저장되어 있는 아래의 단어들입니다. \n",
    "\n",
    "    [0, 1, 21, 23, 101, 184, ...]\n",
    "    \n",
    "그러므로 아래의 word는 정확히는 word_atleast_1000의 index이기 때문에 word_atleast_1000을 한 번 거쳐서 실제 word index를 가져온 것입니다. \n",
    "\n",
    "    for word, score in keyword_99_atleast_1000:\n",
    "\n",
    "    word: word_atleast_1000에서의 index\n",
    "    word_atleast_1000[word]: 실제 단어의 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster # 99 keywords\n",
      "쏘우\n",
      "3편\n",
      "큐브\n",
      "2편\n",
      "1편\n",
      "잔인\n",
      "반전\n",
      "1편보다\n",
      "관람객\n",
      "스릴\n",
      "긴장\n",
      "시리즈\n",
      "액션\n",
      "아이들이\n",
      "아이들\n",
      "하지만\n",
      "진짜\n",
      "무서\n",
      "00\n",
      "\n",
      "cluster # 84 keywords\n",
      "사극\n",
      "김명민\n",
      "준기\n",
      "눈물\n",
      "반전\n",
      "인생\n",
      "드라마\n",
      "스릴\n",
      "역사\n",
      "관람객\n",
      "최고\n",
      "감동\n",
      "연기\n",
      "좋았\n",
      "지루\n",
      "습니다\n",
      "배우\n",
      "잔인\n",
      "다시\n",
      "정말\n",
      "긴장\n",
      "00\n"
     ]
    }
   ],
   "source": [
    "logistic_l1_atleast_1000 = LogisticRegression(penalty='l1', C=15)\n",
    "logistic_l1_atleast_1000.fit(x_atleast_1000, y)\n",
    "\n",
    "print('cluster # %d keywords' % 99)\n",
    "coef_99_atleast_1000 = list(enumerate(logistic_l1_atleast_1000.coef_[99,:]))\n",
    "keyword_99_atleast_1000 = sorted(coef_99_atleast_1000, key=lambda x:x[1], reverse=True)[:100]\n",
    "for word, score in keyword_99_atleast_1000:\n",
    "    print(int2vocab[word_atleast_1000[word]])\n",
    "    if score == 0: break\n",
    "\n",
    "print('\\ncluster # %d keywords' % 84)\n",
    "coef_84_atleast_1000 = list(enumerate(logistic_l1_atleast_1000.coef_[84,:]))\n",
    "keyword_84_atleast_1000 = sorted(coef_84_atleast_1000, key=lambda x:x[1], reverse=True)[:100]\n",
    "for word, score in keyword_84_atleast_1000:\n",
    "    print(int2vocab[word_atleast_1000[word]])\n",
    "    if score == 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
