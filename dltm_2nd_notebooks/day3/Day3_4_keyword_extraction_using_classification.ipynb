{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "small naver news로부터 명사로 이뤄진 term frequency matrix를 만든 뒤, L1 regularization logistic regression을 이용하여 키워드를 추출합니다. \n",
    "\n",
    "명사 추출을 위해서 soy.git 의 LRNounExtractor를 이용합니다.\n",
    "\n",
    "LRNounExtractor는 extract()의 return으로 (현재는) \n",
    "    \n",
    "    nouns: {word:(명사점수, 알려진 R set의 비율)}\n",
    "    cohesions: CohesionProbability class\n",
    "    \n",
    "두 가지를 return 합니다. 아직 확정된 버전이 아니어서 return type이 바뀔 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../mypy/')\n",
    "from corpus import Corpus\n",
    "\n",
    "corpus_path = '../../../data/small_naver_news/processed/corpus.txt'\n",
    "corpus = Corpus(corpus_path, iter_sent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use default r_score_file\n",
      "2398 r features was loaded\n",
      "scanning completed################################ (99.921 %)\n",
      "(L,R) has (6174, 3295) tokens\n",
      "building lr-graph completed################################ (99.921 %)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../soy/')\n",
    "from soy.nlp.tags import LRNounExtractor\n",
    "\n",
    "noun_extractor = LRNounExtractor()\n",
    "nouns, cohesions = noun_extractor.extract(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "noun score threshold 는 0.2 이상, min count는 10이상인 명사만을 선택하여 custom_tokenizer를 만듦니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1683 --> 1616 nouns are selected\n"
     ]
    }
   ],
   "source": [
    "min_count = 10\n",
    "noun_score_threshold = 0.2\n",
    "\n",
    "noun_set = {word for word, score in nouns.items() \n",
    "             if (score[0] > noun_score_threshold) \n",
    "             and (cohesions.L[word] > min_count) }\n",
    "\n",
    "print('%d --> %d nouns are selected' % (len(nouns), len(noun_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Term frequency matrix 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "parse_noun은 주어진 token에 대하여 어절의 왼쪽에 명사가 존재할 경우 이를 잘라내주는 함수입니다. 만약 '뉴스기자가'라는 어절에 대하여 '뉴스'와 '뉴스기자' 두 명사가 모두 존재한다면 길이가 더 긴 '뉴스기자'를 return 합니다. 이를 위하여 reversed(range)를 이용하여 길이의 역순으로 명사를 선택하였습니다. \n",
    "\n",
    "문서가 주어지면 추출된 명사만을 출력하는 custom_tokenize를 만들었습니다. 이를 이용하면 tokenize, part of speech tagging, 이후 명사만 추출하는 과정을 거친 것과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def custom_tokenize(doc):    \n",
    "    def parse_noun(token):\n",
    "        for e in reversed(range(1, len(token)+1)):\n",
    "            subword = token[:e]\n",
    "            if subword in noun_set:\n",
    "                return subword\n",
    "        return ''\n",
    "    \n",
    "    nouns = [parse_noun(token) for token in doc.split()]\n",
    "    nouns = [word for word in nouns if word]\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:  (의왕 국회사진기자단=연합뉴스) 김성태 '최순실 국조특위' 위원장이 26일 오전 경기도 의왕시 서울구치소에서 열린 현장청문회에 입장하고 있다. 2016.12.26\n",
      "nouns:  ['국회', \"'최순실\", '위원장', '26일', '오전', '경기', '서울구치소', '현장청문회', '입장'] \n",
      "\n",
      "\n",
      "sentence:  scoop@yna.co.kr\n",
      "nouns:  [] \n",
      "\n",
      "\n",
      "sentence:  (서울=연합뉴스) 이재희 기자 = 소녀시대의 태연이 26일 오후 서울 강남구 코엑스에서 열린 'SBS 어워즈 페스티벌(SAF) 가요대전'에서 포즈를 취하고 있다. 2016.12.26\n",
      "nouns:  ['기자', '26일', '오후', '강남구'] \n",
      "\n",
      "\n",
      "sentence:  scape@yna.co.kr\n",
      "nouns:  [] \n",
      "\n",
      "\n",
      "sentence:  영국 팝스타 조지 마이클 별세. [EPA=연합뉴스 자료사진]\n",
      "nouns:  ['영국', '조지', '마이클', '별세', '자료'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus.iter_sent = True\n",
    "\n",
    "for num_sent, sent in enumerate(corpus):    \n",
    "    if num_sent == 5:\n",
    "        break\n",
    "    print('sentence: ', sent)\n",
    "    print('nouns: ', custom_tokenize(sent), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "sklearn의 CountVectorizer를 이용하여 term frequency matrix를 만듦니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1355x1605 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 38327 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=custom_tokenize)\n",
    "\n",
    "corpus.iter_sent = False\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Classification을 위한 positive / negative set 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "국회라는 단어가 들어간 문서와 그렇지 않은 문서로 1355개의 문서의 class를 나눈 뒤, 이를 구분할 수 있는 classifier를 학습하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "국회 id = 298\n",
      "num of doc (국회) = 210\n"
     ]
    }
   ],
   "source": [
    "aspect_word = '국회'\n",
    "aspect_id = vectorizer.vocabulary_.get(aspect_word, -1)\n",
    "\n",
    "print('%s id = %d' % (aspect_word, aspect_id))\n",
    "print('num of doc (%s) = %d' % (aspect_word, len(x[:,aspect_id].nonzero()[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "sparse matrix는 nonzero()라는 함수가 있으며, matrix에서 값이 0이 아닌 위치를 (row id list, column id list)로 나타내줍니다. aspect_id에 해당하는 컬럼을 떼어냈기 때문에 submatrix의 모양이 (1355, 1)이 되었습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1355, 1)\n",
      "(array([   0,    4,    5,   14,   23,   24,   28,   33,   44,   45,   48,\n",
      "         50,   51,   58,   59,   63,   67,   72,   81,   87,   96,  103,\n",
      "        104,  105,  114,  124,  125,  129,  135,  136,  146,  150,  151,\n",
      "        166,  171,  174,  179,  186,  191,  195,  199,  205,  206,  216,\n",
      "        226,  248,  255,  262,  266,  273,  282,  285,  292,  298,  311,\n",
      "        353,  358,  362,  381,  384,  388,  428,  438,  445,  450,  462,\n",
      "        466,  467,  472,  473,  477,  493,  495,  501,  508,  515,  517,\n",
      "        536,  545,  552,  556,  557,  560,  561,  564,  566,  574,  575,\n",
      "        596,  606,  607,  612,  613,  629,  630,  638,  640,  642,  650,\n",
      "        652,  674,  682,  684,  685,  690,  695,  698,  700,  707,  709,\n",
      "        711,  713,  723,  725,  728,  729,  737,  743,  744,  748,  752,\n",
      "        762,  772,  773,  775,  779,  781,  782,  791,  792,  793,  843,\n",
      "        845,  849,  851,  852,  856,  864,  867,  871,  876,  880,  881,\n",
      "        890,  893,  898,  903,  911,  916,  922,  945,  948,  986,  988,\n",
      "        993, 1004, 1007, 1010, 1015, 1021, 1024, 1026, 1033, 1039, 1042,\n",
      "       1044, 1048, 1057, 1061, 1068, 1076, 1083, 1091, 1093, 1094, 1097,\n",
      "       1099, 1100, 1102, 1104, 1110, 1119, 1125, 1126, 1130, 1148, 1149,\n",
      "       1159, 1165, 1167, 1175, 1187, 1191, 1199, 1234, 1239, 1243, 1264,\n",
      "       1268, 1279, 1282, 1293, 1295, 1302, 1307, 1312, 1313, 1329, 1332,\n",
      "       1350], dtype=int32),\n",
      " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "print(x[:,aspect_id].shape)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(x[:,aspect_id].nonzero())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "'국회'란 단어가 들어간 문서들의 리스트를 sparse matrix에서 가져왔습니다. 이를 이용하여 각 문서에 '국회'라는 단어가 들어있으면 1, 아니면 -1인 label list y를 만듦니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape = (1355, 1605), len(y) = 1355, num_pos = 210\n"
     ]
    }
   ],
   "source": [
    "pos_idx = set(x[:,aspect_id].nonzero()[0]) # in 함수는 list보다 set이 빠릅니다\n",
    "y = [1 if i in pos_idx else -1 for i in range(x.shape[0])]\n",
    "\n",
    "print('x shape = %s, len(y) = %d, num_pos = %d' % (str(x.shape), len(y), len(pos_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Logistic regression을 이용한 문서 분류기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### LogisticRegression의 constructor\n",
    "\n",
    "- penalty에 l2나 l1을 넣을 수 있으며, 이 때, C는 강의자료의 lambda에 해당합니다. \n",
    "- fit_intercept는 데이터의 평행이동을 해주는 중요한 페러매터이므로 True로 그대로 둡니다. \n",
    "- 학습이 될 수 있는지 debugging 용으로 확인하고 싶을 때에는 max_iter를 잠시 줄여도 좋습니다. \n",
    "- n_jobs는 동시에 여러 개의 cpu processor를 쓸 수 있도록 하는 방법입니다. 절대 수업시에는 n_jobs를 2 이상으로 설정하지 마십시요. 모든 이들이 한 번에 돌리면 CPU가 남아나질 않습니다. 코어가 여러 개인 컴퓨터에서는 n_jobs를 크게 잡으시면 빠르게 계산이 됩니다. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__init__(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)[source]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## L2 regulization을 이용한 문서 판별기 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "국회라는 단어가 들어간 문서와 들어가지 않는 문서를 분류하기 위한 예제이므로, x_train이라는 새로운 sparse term frequency matrix를 만든 뒤, 국회 단어의 빈도수를 0으로 바꿨습니다. 왜냐하면, '국회'라는 단어 자체가 y label이기 때문입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x_train = vectorizer.fit_transform(corpus)\n",
    "x_train[:,aspect_id] = 0\n",
    "\n",
    "logistic_l2 = LogisticRegression(penalty='l2')\n",
    "logistic_l2.fit(x_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "위 정확도는 training error를 보기 위함이며, test set에 의한 일반화 성능 및 (Cross validation)은 나중에 다루도록 하겠습니다. training error가 0.002이므로, 1355개의 문서를 '국회'가 들어간 문서와 들어가지 않은 문서로 거의 완벽히 나눌 수 있습니다. 국회라는 단어의 유무를 알려주는 다른 단어들이 존재한다는 의미입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1355,)\n"
     ]
    }
   ],
   "source": [
    "y_pred = logistic_l2.predict(x)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error = 0.002\n"
     ]
    }
   ],
   "source": [
    "accuracy = sum([1 if pred == answ else 0 for pred, answ in zip(y_pred, y)]) / len(y_pred)\n",
    "print('training error = %.3f' % (1 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## regression coefficients 뜯어보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "vectorizer의 vocabulary_의 크기와 logistic_l2.coef_의 크기가 같습니다. coef는 각 단어의 positive class에 대한 기여도입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression coefficient: (1, 1605)\n",
      "size of vocabulary: 1605\n"
     ]
    }
   ],
   "source": [
    "print('logistic regression coefficient:',logistic_l2.coef_.shape)\n",
    "print('size of vocabulary:', len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "matrix.reshape(-1)을 하면 matrix를 vector 형태로 바꿔줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1605,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_l2.coef_.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "vectorizer.vocabulary_ 로부터 각 차원이 어떤 단어에 해당하는지 확인할 수 있는 index2word list를 만듦니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2003년', '2007년', '2008년', '2010년', '2011년']\n"
     ]
    }
   ],
   "source": [
    "index2word = sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])\n",
    "index2word = [word for word, i in index2word]\n",
    "print(index2word[50:55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \"국 : -0.056\n",
      "       \"국민 : 0.037\n",
      "        \"그 : -0.173\n",
      "        \"내 : 0.053\n",
      "       \"내년 : -0.004\n",
      "        \"대 : -0.195\n",
      "      \"대통령 : -0.024\n",
      "       \"미국 : -0.010\n",
      "        \"반 : -0.002\n",
      "        \"새 : 0.016\n",
      "        \"시 : -0.014\n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(logistic_l2.coef_.reshape(-1)):\n",
    "    print('%10s : %.3f' % (index2word[i], v))\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "coef에 대하여 그 크기가 큰 순서대로 sorting을 하여 살펴보겠습니다. \n",
    "\n",
    "회의, 원내대표, 최고, 당대 등의 단어가 들어간 문서는 '정부'라는 단어가 들어있는 문서일 가능성이 높습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('모습', -1.0404240735708088),\n",
       " ('브리핑', -1.0388715549548639),\n",
       " ('의원들', -1.0089950006535855),\n",
       " ('비롯', -0.97448753355065421),\n",
       " ('중진회의', -0.85295739852841712),\n",
       " ('호남', -0.77340533840362102),\n",
       " ('시장', -0.69579662596732228),\n",
       " ('강남구', -0.69541565035601161),\n",
       " ('서구', -0.66699495102239281),\n",
       " ('비상대책위원장', -0.66191213893298972),\n",
       " ('대화', -0.65380745408649599),\n",
       " ('수감동', -0.61881136799606529),\n",
       " ('기자', -0.6040897668702816),\n",
       " ('오후', -0.58671000316410393),\n",
       " ('종로구', -0.56681772516425721),\n",
       " ('예방', -0.50928896296021542),\n",
       " ('선보', -0.50019149270148655),\n",
       " ('비주류', -0.49898992258268715),\n",
       " ('부산', -0.49668661257813423),\n",
       " ('관계장관회의', -0.49117958851750571)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_l2 = logistic_l2.coef_.reshape(-1)\n",
    "beta_l2 = [(index2word[i], coef) for i, coef in enumerate(coef_l2)]\n",
    "beta_l2 = sorted(beta_l2, key=lambda x:x[1], reverse=False)\n",
    "beta_l2[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## L1 (LASSO)를 이용한 keyword extraction\n",
    "\n",
    "앞의 예제에서는 L2 regularization을 하였기 때문에 많은 단어들이 모두 고려되었습니다. 이번에는 L1 regularization을 이용합니다. 이는 국회가 들어간 문서와 아닌 문서를 구분하기 위한 최소한의 features (= nouns)를 선택하는 효과가 있습니다. \n",
    "\n",
    "즉, 여기서 정의하는 키워드의 기준은, 하나의 class set과 다른 class set을 구분할 수 있는 최소한의 단어집합입니다. 키워드를 추출하기 위함이니 이번에는 x_train 대신, 국회라는 단어가 포함되어 있는 term frequency matrix인 x를 이용하여 학습합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_l1 = LogisticRegression(penalty='l1')\n",
    "logistic_l1.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "이번에는 L1 의 regularization cost에 따른 키워드 추출의 경향을 살펴보겠습니다. cost 별로 beta의 크기가 큰 top 50개의 키워드들을 top50 dict에 저장해둡니다. \n",
    "\n",
    "costs는 1/C 의 값입니다. C = 500이면 lambda = 1/500이므로, regularization이 매우 작게 됨을 의미합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with cost = 500.0\n",
      "done with cost = 200.0\n",
      "done with cost = 100.0\n",
      "done with cost = 50.0\n"
     ]
    }
   ],
   "source": [
    "costs = [500, 200, 100, 50]\n",
    "top50 = {} \n",
    "\n",
    "for cost in costs:\n",
    "    logistic_l1 = LogisticRegression(penalty='l1', C=cost)\n",
    "    logistic_l1.fit(x, y)\n",
    "    \n",
    "    coef_l1 = logistic_l1.coef_.reshape(-1)\n",
    "    beta_l1 = [(index2word[i], coef) for i, coef in enumerate(coef_l1)]\n",
    "    beta_l1 = sorted(beta_l1, key=lambda x:x[1], reverse=True)\n",
    "    top50[cost] = beta_l1[:50]\n",
    "    print('done with cost = %.1f' % cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Regularization이 강하게 되면서, 국회라는 단어가 들어간 문서집합을 설명하기 위해 선택한 단어의 개수가 줄어듦니다. 이를 국회라는 문서가 들어간 문서집합의 키워드로 추출할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        국회\t        국회\t        국회\t        국회\n",
      "        당대\t      원내대표\t      원내대표\t        의원\n",
      "      '최순실\t        의원\t        의원\t       청문회\n",
      "        회의\t        대표\t       청문회\t        대표\n",
      "      원내대표\t        최고\t        대표\t        오전\n",
      "        고발\t       청문회\t        의혹\t        실장\n",
      "        최고\t        사표\t     서울구치소\t        의혹\n",
      "     창당추진위\t        최씨\t        실장\t       최순실\n",
      "    더불어민주당\t      국정조사\t        오전\t        자택\n",
      "       구치소\t       최순실\t       최순실\t      원내대표\n",
      "        회동\t     서울구치소\t        질문\t          \n",
      "        사표\t        실장\t        자택\t          \n",
      "        의결\t        자택\t      압수수색\t          \n",
      "        현안\t     창당추진위\t       청와대\t          \n",
      "        축사\t        오전\t          \t          \n",
      "        이혼\t       문체부\t          \t          \n",
      "     서울구치소\t        회의\t          \t          \n",
      "       민간인\t        업무\t          \t          \n",
      "        의원\t      문화예술\t          \t          \n",
      "      국정조사\t        재산\t          \t          \n",
      "     기자간담회\t      국민의당\t          \t          \n",
      "        입장\t          \t          \t          \n",
      "      새누리당\t          \t          \t          \n",
      "       최순실\t          \t          \t          \n",
      "        오전\t          \t          \t          \n",
      "        확보\t          \t          \t          \n",
      "     현장청문회\t          \t          \t          \n",
      "      국민의당\t          \t          \t          \n",
      "        현장\t          \t          \t          \n",
      "        전망\t          \t          \t          \n",
      "        의혹\t          \t          \t          \n",
      "        대표\t          \t          \t          \n",
      "        공모\t          \t          \t          \n",
      "        자택\t          \t          \t          \n",
      "        발언\t          \t          \t          \n",
      "        실장\t          \t          \t          \n",
      "       대통령\t          \t          \t          \n",
      "        동참\t          \t          \t          \n",
      "        정씨\t          \t          \t          \n",
      "        총장\t          \t          \t          \n",
      "        창당\t          \t          \t          \n",
      "        헌재\t          \t          \t          \n",
      "        일부\t          \t          \t          \n",
      "        귀국\t          \t          \t          \n",
      "      국정농단\t          \t          \t          \n",
      "       살처분\t          \t          \t          \n",
      "      비서실장\t          \t          \t          \n",
      "        체포\t          \t          \t          \n",
      "     있다\"면서\t          \t          \t          \n",
      "        정치\t          \t          \t          \n"
     ]
    }
   ],
   "source": [
    "for top in range(50):\n",
    "    message = '\\t'.join(['%10s' % (top50[cost][top][0] if top50[cost][top][1] > 0.001 else '') \n",
    "                         for cost in costs])\n",
    "    print(message)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
