{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import codecs\n",
    "from random import sample\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading, preprocessing and transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 정보\n",
    "- 나루토 자막 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러온 후, 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObj = codecs.open(\"./data/naruto_send_tokenized_corpus.txt\", \"r\", \"utf-8\" )\n",
    "pairs = fileObj.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_set = []\n",
    "output_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pair in pairs:\n",
    "    pair.replace('\\r\\n','')\n",
    "    input_, output_ = pair.split('\\t')\n",
    "    input_set.append(input_)\n",
    "    output_set.append(output_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어떠냐  ====>  이걸 네가 빌린 사륜안으로 없앨 수 있겠어\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(input_set[0], ' ====> ', output_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그런 미친 소리 지랄 하지 말라 그래  ====>  알겠냐\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(input_set[12345], ' ====> ', output_set[12345])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382996"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382996"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가장 길이가 긴 문서 찾기\n",
    "- 문서를 단어 임베딩 벡터로 이루어진 이미지처럼 만들기 위해, max_document_length를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_doc_length(documents):\n",
    "    max_length = 0\n",
    "    longest_doc = 0\n",
    "    for i, document in enumerate(documents):\n",
    "        length = len(document)\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            longest_doc = i\n",
    "    return max_length, longest_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_max_length, input_longest = max_doc_length(input_set)\n",
    "output_max_length, output_longest = max_doc_length(output_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input seq 최대 길이:  107\n",
      "output seq 최대 길이:  109\n"
     ]
    }
   ],
   "source": [
    "print('input seq 최대 길이: ', input_max_length)\n",
    "print('output seq 최대 길이: ', output_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transforming\n",
    "tensorflow.contrib.learn.preprocessing 내에 **VocabularyProcessor**라는 클래스를 이용\n",
    "- 모든 문서에 등장하는 단어들에 인덱스를 할당\n",
    "- 길이가 다른 문서를 max_document_length로 맞춰주는 역할\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max(input_max_length, output_max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_set = np.array(list(vocab_processor.fit_transform(input_set)))\n",
    "output_seq_set = np.array(list(vocab_processor.fit_transform(output_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input sequence의 최대 길이는 107, output sequence의 최대 길이는 109\n",
    "- input_seq_set의 한 성분에서 뒤의 2개 요소는 zero padding이 추가적으로 들어가므로, 이를 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_seq_set = np.array([seq[0:-2] for seq in input_seq_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382996, 107)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382996, 109)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seq_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vocabulary 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/40661684/tensorflow-vocabularyprocessor\n",
    "\n",
    "# Extract word:id mapping from the object.\n",
    "vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "\n",
    "# Sort the vocabulary dictionary on the basis of values(id).\n",
    "sorted_vocab = sorted(vocab_dict.items(), key = lambda x : x[1])\n",
    "\n",
    "# Treat the id's as index into list and create a list of words in the ascending order of id's\n",
    "# word with id i goes at index i of the list.\n",
    "vocabulary = list(list(zip(*sorted_vocab))[0])\n",
    "\n",
    "# print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data를 train / validation / test set으로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(X, Y, ratio = [0.7, 0.15, 0.15] ):\n",
    "    # number of examples\n",
    "    data_len = len(X)\n",
    "    lens = [ int(data_len*item) for item in ratio ]\n",
    "\n",
    "    trainX, trainY = X[:lens[0]], Y[:lens[0]]\n",
    "    testX, testY = X[lens[0]:lens[0]+lens[1]], Y[lens[0]:lens[0]+lens[1]]\n",
    "    validX, validY = X[-lens[-1]:], Y[-lens[-1]:]\n",
    "\n",
    "    return (trainX,trainY), (testX,testY), (validX,validY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trainX, trainY), (testX, testY), (validX, validY) = split_dataset(input_seq_set, output_seq_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Seq2seq modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seq2seq_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters for data shape\n",
    "xseq_len = trainX.shape[-1]\n",
    "yseq_len = trainY.shape[-1]\n",
    "xvocab_size = len(vocabulary)  \n",
    "yvocab_size = xvocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for training model\n",
    "train_batch_size = 32\n",
    "test_batch_size = 256\n",
    "emb_dim = 128\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<log> Building Graph </log>"
     ]
    }
   ],
   "source": [
    "model = seq2seq_wrapper.Seq2Seq(xseq_len=xseq_len,\n",
    "                               yseq_len=yseq_len,\n",
    "                               xvocab_size=xvocab_size,\n",
    "                               yvocab_size=yvocab_size,\n",
    "                               ckpt_path='ckpt/naruto/',\n",
    "                               emb_dim=emb_dim,\n",
    "                               num_layers=num_layers\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rand_batch_gen(x, y, batch_size):\n",
    "    while True:\n",
    "        sample_idx = sample(list(np.arange(len(x))), batch_size)\n",
    "        yield x[sample_idx].T, y[sample_idx].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "val_batch_gen = rand_batch_gen(validX, validY, test_batch_size)\n",
    "test_batch_gen = rand_batch_gen(testX, testY, test_batch_size)\n",
    "train_batch_gen = rand_batch_gen(trainX, trainY, train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<log> Training started </log>\n",
      "\n",
      "Model saved to disk at iteration #1000\n",
      "val   loss : 0.448158\n",
      "\n",
      "Model saved to disk at iteration #2000\n",
      "val   loss : 0.427563\n",
      "\n",
      "Model saved to disk at iteration #3000\n",
      "val   loss : 0.418473\n",
      "\n",
      "Model saved to disk at iteration #4000\n",
      "val   loss : 0.417144\n",
      "\n",
      "Model saved to disk at iteration #5000\n",
      "val   loss : 0.413452\n",
      "\n",
      "Model saved to disk at iteration #6000\n",
      "val   loss : 0.404385\n",
      "\n",
      "Model saved to disk at iteration #7000\n",
      "val   loss : 0.406439\n",
      "\n",
      "Model saved to disk at iteration #8000\n",
      "val   loss : 0.405627\n",
      "\n",
      "Model saved to disk at iteration #9000\n",
      "val   loss : 0.407131\n",
      "Interrupted by user at iteration 9884\n"
     ]
    }
   ],
   "source": [
    "sess = model.train(train_batch_gen, val_batch_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = model.restore_last_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 109)\n"
     ]
    }
   ],
   "source": [
    "input_ = test_batch_gen.__next__()[0]\n",
    "output = model.predict(sess, input_)\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(sequence, lookup, separator=''): # 0 used for padding, is ignored\n",
    "    return separator.join([ lookup[element] for element in sequence if element ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q : [자 잠깐 기다 려보 라고 아무 리 그 녀석 이 못되 먹은 자식 이라 도 우리 를 배신 할리]; a : [그 의 의 을 거야]\n"
     ]
    }
   ],
   "source": [
    "replies = []\n",
    "for ii, oi in zip(input_.T, output):\n",
    "    q = decode(sequence=ii, lookup=vocabulary, separator=' ')\n",
    "    decoded = decode(sequence=oi, lookup=vocabulary, separator=' ').split(' ')\n",
    "    if decoded.count('<UNK>') == 0:\n",
    "        if decoded not in replies:\n",
    "            print('q : [{0}]; a : [{1}]'.format(q, ' '.join(decoded)))\n",
    "            replies.append(decoded)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
