{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 0. Data Loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load training set and test set\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test  = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "X_train = newsgroups_train.data\n",
    "Y_train = newsgroups_train.target\n",
    "X_test  = newsgroups_test.data\n",
    "Y_test  = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Declare two vectorizers\n",
    "count_vectorizer = CountVectorizer(min_df=40)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fitting vectorizers to the training set\n",
    "count_vectorizer = count_vectorizer.fit(X_train)\n",
    "tfidf_vectorizer = tfidf_vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Transform X_train and X_test using 2 vectorizers\n",
    "X_train_count = count_vectorizer.transform(X_train)\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "X_test_count  = count_vectorizer.transform(X_test)\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 758)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 758)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 18)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 29)\t1\n",
      "  (0, 35)\t1\n",
      "  (0, 50)\t1\n",
      "  (0, 55)\t2\n",
      "  (0, 61)\t2\n",
      "  (0, 78)\t1\n",
      "  (0, 87)\t1\n",
      "  (0, 113)\t1\n",
      "  (0, 195)\t2\n",
      "  (0, 244)\t6\n",
      "  (0, 253)\t1\n",
      "  (0, 256)\t2\n",
      "  (0, 260)\t1\n",
      "  (0, 272)\t1\n",
      "  (0, 292)\t1\n",
      "  (0, 301)\t1\n",
      "  (0, 316)\t2\n",
      "  (0, 321)\t3\n",
      "  (0, 329)\t2\n",
      "  (0, 337)\t3\n",
      "  (0, 342)\t1\n",
      "  (0, 354)\t1\n",
      "  (0, 374)\t1\n",
      "  (0, 420)\t1\n",
      "  (0, 453)\t1\n",
      "  (0, 455)\t1\n",
      "  (0, 472)\t1\n",
      "  (0, 503)\t1\n",
      "  (0, 533)\t1\n",
      "  (0, 556)\t1\n",
      "  (0, 559)\t1\n",
      "  (0, 647)\t2\n",
      "  (0, 648)\t7\n",
      "  (0, 649)\t1\n",
      "  (0, 657)\t1\n",
      "  (0, 662)\t1\n",
      "  (0, 671)\t4\n",
      "  (0, 703)\t1\n",
      "  (0, 720)\t1\n",
      "  (0, 729)\t1\n",
      "  (0, 733)\t1\n",
      "  (0, 755)\t3\n",
      "  (0, 756)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_count[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 756)\t0.0721155370883\n",
      "  (0, 755)\t0.14831743135\n",
      "  (0, 733)\t0.0552321132408\n",
      "  (0, 729)\t0.0869038419929\n",
      "  (0, 720)\t0.0779953088406\n",
      "  (0, 703)\t0.0899136950299\n",
      "  (0, 671)\t0.144021224217\n",
      "  (0, 662)\t0.0499187101099\n",
      "  (0, 657)\t0.0631328198798\n",
      "  (0, 649)\t0.0804180650654\n",
      "  (0, 648)\t0.23180983825\n",
      "  (0, 647)\t0.0836810249613\n",
      "  (0, 559)\t0.093868888692\n",
      "  (0, 556)\t0.136566528013\n",
      "  (0, 533)\t0.096000080666\n",
      "  (0, 503)\t0.12460572882\n",
      "  (0, 472)\t0.0753113616121\n",
      "  (0, 455)\t0.106203923252\n",
      "  (0, 453)\t0.0524819891906\n",
      "  (0, 420)\t0.13397450603\n",
      "  (0, 374)\t0.0705501151537\n",
      "  (0, 354)\t0.0720485235024\n",
      "  (0, 342)\t0.0423196121842\n",
      "  (0, 337)\t0.122195764007\n",
      "  (0, 329)\t0.210611295123\n",
      "  (0, 321)\t0.122719069678\n",
      "  (0, 316)\t0.109162363642\n",
      "  (0, 301)\t0.117608096826\n",
      "  (0, 292)\t0.0509033565847\n",
      "  (0, 272)\t0.105976622957\n",
      "  (0, 260)\t0.0599235329153\n",
      "  (0, 256)\t0.252864456363\n",
      "  (0, 253)\t0.0458215515453\n",
      "  (0, 244)\t0.675533177371\n",
      "  (0, 195)\t0.153717311712\n",
      "  (0, 113)\t0.0527484422433\n",
      "  (0, 87)\t0.0492024228549\n",
      "  (0, 78)\t0.106433094352\n",
      "  (0, 61)\t0.104963978381\n",
      "  (0, 55)\t0.172902163822\n",
      "  (0, 50)\t0.0387804606199\n",
      "  (0, 35)\t0.0640215378146\n",
      "  (0, 29)\t0.0931456238322\n",
      "  (0, 19)\t0.0631814559297\n",
      "  (0, 18)\t0.112020595589\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Fitting classifiers with count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Pre-define options\n",
    "num_folds = 5\n",
    "num_instances = len(X_train)\n",
    "seed = 1234\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.1. Logistic Regression\n",
    "다음과 같은 파라미터를 컨트롤하여 모델링해봅시다.\n",
    "- regulatization: L1, L2\n",
    "- C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "penalty_set = ['l1', 'l2']\n",
    "C_set = [1, 10]\n",
    "param_grid = dict(penalty=penalty_set, C=C_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:   36.4s remaining:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   39.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using count vectorizer\n",
    "clf = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=num_folds, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train_count, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  0.6053515 ,   0.57298069,  34.24835596,   0.85566149]),\n",
       " 'mean_score_time': array([ 0.00112472,  0.00116949,  0.00052738,  0.0006897 ]),\n",
       " 'mean_test_score': array([ 0.70550639,  0.70698132,  0.68682399,  0.68485742]),\n",
       " 'mean_train_score': array([ 0.92834308,  0.95341819,  0.97209889,  0.97308252]),\n",
       " 'param_C': masked_array(data = [1 1 10 10],\n",
       "              mask = [False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_penalty': masked_array(data = ['l1' 'l2' 'l1' 'l2'],\n",
       "              mask = [False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'C': 1, 'penalty': 'l1'},\n",
       "  {'C': 1, 'penalty': 'l2'},\n",
       "  {'C': 10, 'penalty': 'l1'},\n",
       "  {'C': 10, 'penalty': 'l2'}),\n",
       " 'rank_test_score': array([2, 1, 3, 4], dtype=int32),\n",
       " 'split0_test_score': array([ 0.72303922,  0.73284314,  0.70098039,  0.70343137]),\n",
       " 'split0_train_score': array([ 0.92558426,  0.95448954,  0.97109471,  0.97170972]),\n",
       " 'split1_test_score': array([ 0.70588235,  0.70098039,  0.70343137,  0.68627451]),\n",
       " 'split1_train_score': array([ 0.93050431,  0.95448954,  0.97293973,  0.97416974]),\n",
       " 'split2_test_score': array([ 0.6953317 ,  0.69287469,  0.64864865,  0.66093366]),\n",
       " 'split2_train_score': array([ 0.92870313,  0.95759066,  0.97172711,  0.97357099]),\n",
       " 'split3_test_score': array([ 0.69458128,  0.70197044,  0.67980296,  0.67980296]),\n",
       " 'split3_train_score': array([ 0.92874693,  0.9490172 ,  0.96990172,  0.97235872]),\n",
       " 'split4_test_score': array([ 0.70864198,  0.70617284,  0.70123457,  0.69382716]),\n",
       " 'split4_train_score': array([ 0.9281768 ,  0.95150399,  0.97483118,  0.97360344]),\n",
       " 'std_fit_time': array([ 0.19695237,  0.05178668,  5.74629794,  0.15591536]),\n",
       " 'std_score_time': array([  6.87940026e-05,   5.60610047e-05,   8.35214006e-05,\n",
       "          1.05851556e-04]),\n",
       " 'std_test_score': array([ 0.0104013 ,  0.01365085,  0.02093624,  0.01432654]),\n",
       " 'std_train_score': array([ 0.0015876 ,  0.00292363,  0.00168206,  0.00090559])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'C': 1, 'penalty': 'l2'}\n",
      "Best test accuracy :  0.706981317601\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf.best_params_)\n",
    "print(\"Best test\", scoring, ': ', clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_logistic_count = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. MLPClssifier\n",
    "은닉층의 사이즈를 다음과 같이 조절해봅시다.\n",
    "- 은닉층 1개 (노드 수 = 100)\n",
    "- 은닉층 2개 (노드 수 = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(learning_rate_init=0.01, max_iter=300)\n",
    "\n",
    "hidden_layer_sizes_set = [(100,), (100, 100)]\n",
    "param_grid = dict(hidden_layer_sizes=hidden_layer_sizes_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.01, max_iter=300, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'hidden_layer_sizes': [(100,), (100, 100)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using count vectorizer\n",
    "clf = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=num_folds, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train_count, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 5.21074781,  5.4507545 ]),\n",
       " 'mean_score_time': array([ 0.01862831,  0.01564145]),\n",
       " 'mean_test_score': array([ 0.71337266,  0.69419862]),\n",
       " 'mean_train_score': array([ 0.96817161,  0.95083109]),\n",
       " 'param_hidden_layer_sizes': masked_array(data = [(100,) (100, 100)],\n",
       "              mask = [False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'hidden_layer_sizes': (100,)},\n",
       "  {'hidden_layer_sizes': (100, 100)}),\n",
       " 'rank_test_score': array([1, 2], dtype=int32),\n",
       " 'split0_test_score': array([ 0.70833333,  0.7254902 ]),\n",
       " 'split0_train_score': array([ 0.97416974,  0.97539975]),\n",
       " 'split1_test_score': array([ 0.73529412,  0.67892157]),\n",
       " 'split1_train_score': array([ 0.9698647 ,  0.96494465]),\n",
       " 'split2_test_score': array([ 0.70761671,  0.63390663]),\n",
       " 'split2_train_score': array([ 0.97541487,  0.86170867]),\n",
       " 'split3_test_score': array([ 0.7044335 ,  0.70935961]),\n",
       " 'split3_train_score': array([ 0.97481572,  0.97481572]),\n",
       " 'split4_test_score': array([ 0.71111111,  0.72345679]),\n",
       " 'split4_train_score': array([ 0.946593  ,  0.97728668]),\n",
       " 'std_fit_time': array([ 2.17906174,  1.13897763]),\n",
       " 'std_score_time': array([ 0.01180109,  0.00703049]),\n",
       " 'std_test_score': array([ 0.01118417,  0.03445198]),\n",
       " 'std_train_score': array([ 0.01096439,  0.0447678 ])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'hidden_layer_sizes': (100,)}\n",
      "Best test accuracy :  0.7133726647\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf.best_params_)\n",
    "print(\"Best test\", scoring, ': ', clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_mlp_count = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. 두 모델의 비교\n",
    "Logistic regression에서 가장 성능이 좋은 모델과 MLP에서 가장 성능이 좋은 모델을 선택하여 테스트 데이터에 대한 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_models_count = []\n",
    "best_models_count.append(('LogisticRegression', best_logistic_count))\n",
    "best_models_count.append(('MLPClassifier', best_mlp_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "scores  = []\n",
    "names   = []\n",
    "for name, model in best_models_count:\n",
    "    Y_test_hat = model.predict(X_test_count)\n",
    "    results.append(metrics.confusion_matrix(Y_test, Y_test_hat))\n",
    "    scores.append(metrics.accuracy_score(Y_test, Y_test_hat))\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LogisticRegression]\n",
      "- test accuracy: 0.660754\n",
      "- confusion matrix :\n",
      " [[170  17  42  90]\n",
      " [ 20 310  46  13]\n",
      " [ 40  38 298  18]\n",
      " [ 88  19  28 116]]\n",
      "\n",
      "[MLPClassifier]\n",
      "- test accuracy: 0.645972\n",
      "- confusion matrix :\n",
      " [[174  20  32  93]\n",
      " [ 25 307  44  13]\n",
      " [ 40  41 271  42]\n",
      " [ 97  17  15 122]]\n"
     ]
    }
   ],
   "source": [
    "for name, score, cm in list(zip(names, scores, results)):\n",
    "    print('\\n[%s]' % name)\n",
    "    print('- test accuracy: %f' % score)\n",
    "    print('- confusion matrix :\\n', cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fitting classifiers with tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Pre-define options\n",
    "num_folds = 5\n",
    "num_instances = len(X_train)\n",
    "seed = 1234\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 2.1. Logistic Regression\n",
    "다음과 같은 파라미터를 컨트롤하여 모델링해봅시다.\n",
    "- regulatization: L1, L2\n",
    "- C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "penalty_set = ['l1', 'l2']\n",
    "C_set = [0.1, 1, 10, 100]\n",
    "param_grid = dict(penalty=penalty_set, C=C_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using count vectorizer\n",
    "clf = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=num_folds, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train_tfidf, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.09383607,  0.11490035,  0.11884079,  0.11128392]),\n",
       " 'mean_score_time': array([ 0.00117226,  0.00099206,  0.00084963,  0.00052657]),\n",
       " 'mean_test_score': array([ 0.69469027,  0.73107178,  0.71976401,  0.73254671]),\n",
       " 'mean_train_score': array([ 0.78822366,  0.85533489,  0.97197657,  0.94997574]),\n",
       " 'param_C': masked_array(data = [1 1 10 10],\n",
       "              mask = [False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_penalty': masked_array(data = ['l1' 'l2' 'l1' 'l2'],\n",
       "              mask = [False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'C': 1, 'penalty': 'l1'},\n",
       "  {'C': 1, 'penalty': 'l2'},\n",
       "  {'C': 10, 'penalty': 'l1'},\n",
       "  {'C': 10, 'penalty': 'l2'}),\n",
       " 'rank_test_score': array([4, 2, 3, 1], dtype=int32),\n",
       " 'split0_test_score': array([ 0.70588235,  0.75490196,  0.73284314,  0.75      ]),\n",
       " 'split0_train_score': array([ 0.78474785,  0.85670357,  0.96924969,  0.94649446]),\n",
       " 'split1_test_score': array([ 0.68382353,  0.71323529,  0.71568627,  0.73039216]),\n",
       " 'split1_train_score': array([ 0.78720787,  0.85670357,  0.97416974,  0.95141451]),\n",
       " 'split2_test_score': array([ 0.69287469,  0.72235872,  0.71253071,  0.72972973]),\n",
       " 'split2_train_score': array([ 0.7904118 ,  0.8537185 ,  0.97418562,  0.95574677]),\n",
       " 'split3_test_score': array([ 0.68472906,  0.71182266,  0.68226601,  0.72413793]),\n",
       " 'split3_train_score': array([ 0.78501229,  0.85503686,  0.97051597,  0.94717445]),\n",
       " 'split4_test_score': array([ 0.70617284,  0.75308642,  0.75555556,  0.72839506]),\n",
       " 'split4_train_score': array([ 0.79373849,  0.85451197,  0.97176182,  0.9490485 ]),\n",
       " 'std_fit_time': array([ 0.0085233 ,  0.02438574,  0.01663073,  0.01602537]),\n",
       " 'std_score_time': array([  1.78906506e-04,   7.29397213e-05,   1.67271702e-04,\n",
       "          7.90094279e-05]),\n",
       " 'std_test_score': array([ 0.00977406,  0.01906245,  0.02416317,  0.00900932]),\n",
       " 'std_train_score': array([ 0.00342425,  0.00119376,  0.00196495,  0.00335265])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'C': 10, 'penalty': 'l2'}\n",
      "Best test accuracy :  0.732546705998\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf.best_params_)\n",
    "print(\"Best test\", scoring, ': ', clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_logistic_tfidf = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. MLPClssifier\n",
    "은닉층의 사이즈를 다음과 같이 조절해봅시다.\n",
    "- 은닉층 1개 (노드 수 = 100)\n",
    "- 은닉층 2개 (노드 수 = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(learning_rate_init=0.01, max_iter=300)\n",
    "\n",
    "hidden_layer_sizes_set = [(100,), (100, 100)]\n",
    "param_grid = dict(hidden_layer_sizes=hidden_layer_sizes_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.01, max_iter=300, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'hidden_layer_sizes': [(100,), (100, 100)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using count vectorizer\n",
    "clf = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=num_folds, n_jobs=-1, verbose=1)\n",
    "clf.fit(X_train_tfidf, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 4.67469234,  3.7807632 ]),\n",
       " 'mean_score_time': array([ 0.00812488,  0.01117382]),\n",
       " 'mean_test_score': array([ 0.72173058,  0.72271386]),\n",
       " 'mean_train_score': array([ 0.97603266,  0.97603266]),\n",
       " 'param_hidden_layer_sizes': masked_array(data = [(100,) (100, 100)],\n",
       "              mask = [False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'hidden_layer_sizes': (100,)},\n",
       "  {'hidden_layer_sizes': (100, 100)}),\n",
       " 'rank_test_score': array([2, 1], dtype=int32),\n",
       " 'split0_test_score': array([ 0.73039216,  0.73284314]),\n",
       " 'split0_train_score': array([ 0.97539975,  0.97539975]),\n",
       " 'split1_test_score': array([ 0.70588235,  0.70588235]),\n",
       " 'split1_train_score': array([ 0.97785978,  0.97785978]),\n",
       " 'split2_test_score': array([ 0.71253071,  0.72481572]),\n",
       " 'split2_train_score': array([ 0.9760295,  0.9760295]),\n",
       " 'split3_test_score': array([ 0.72660099,  0.71428571]),\n",
       " 'split3_train_score': array([ 0.97420147,  0.97420147]),\n",
       " 'split4_test_score': array([ 0.73333333,  0.73580247]),\n",
       " 'split4_train_score': array([ 0.97667281,  0.97667281]),\n",
       " 'std_fit_time': array([ 0.8804391 ,  0.77621066]),\n",
       " 'std_score_time': array([ 0.00494299,  0.00624444]),\n",
       " 'std_test_score': array([ 0.0106727 ,  0.01125085]),\n",
       " 'std_train_score': array([ 0.00122491,  0.00122491])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'hidden_layer_sizes': (100, 100)}\n",
      "Best test accuracy :  0.722713864307\n"
     ]
    }
   ],
   "source": [
    "print(\"Best params: \", clf.best_params_)\n",
    "print(\"Best test\", scoring, ': ', clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_mlp_tfidf = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. 두 모델의 비교\n",
    "Logistic regression에서 가장 성능이 좋은 모델과 MLP에서 가장 성능이 좋은 모델을 선택하여 테스트 데이터에 대한 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_models = []\n",
    "best_models.append(('LogisticRegression', best_logistic_tfidf))\n",
    "best_models.append(('MLPClassifier', best_mlp_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "scores  = []\n",
    "names   = []\n",
    "for name, model in best_models:\n",
    "    Y_test_hat = model.predict(X_test_tfidf)\n",
    "    results.append(metrics.confusion_matrix(Y_test, Y_test_hat))\n",
    "    scores.append(metrics.accuracy_score(Y_test, Y_test_hat))\n",
    "    names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LogisticRegression]\n",
      "- test accuracy: 0.691796\n",
      "- confusion matrix :\n",
      " [[178  15  44  82]\n",
      " [ 12 319  47  11]\n",
      " [ 32  32 313  17]\n",
      " [ 83  13  29 126]]\n",
      "\n",
      "[MLPClassifier]\n",
      "- test accuracy: 0.667406\n",
      "- confusion matrix :\n",
      " [[158  20  26 115]\n",
      " [ 16 318  40  15]\n",
      " [ 30  35 278  51]\n",
      " [ 72  14  16 149]]\n"
     ]
    }
   ],
   "source": [
    "for name, score, cm in list(zip(names, scores, results)):\n",
    "    print('\\n[%s]' % name)\n",
    "    print('- test accuracy: %f' % score)\n",
    "    print('- confusion matrix :\\n', cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
