{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "fasttext와 gensim은 모두 pip에 등록된 패키지 입니다. 각각의 설치 방법은 아래와 같습니다. \n",
    "\n",
    "    pip install gensim\n",
    "    pip install fasttext\n",
    "    \n",
    "fasttext는 Cython에 기반하여 작동합니다. 만약 cython이 없어서 fasttext install이 안될 경우에는 먼저 아래를 설치하세요. \n",
    "\n",
    "    pip install cython\n",
    "    \n",
    "각각의 사이트는 아래와 같습니다. \n",
    "\n",
    "- [gensim][https://radimrehurek.com/gensim/]\n",
    "- [fasttext][https://pypi.python.org/pypi/fasttext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "아래는 fasttext의 parameters입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "model = fasttext.skipgram(params)\n",
    "\n",
    "https://github.com/salestock/fastText.py/blob/master/fasttext/model.py\n",
    "\n",
    "\n",
    "## Available parameters\n",
    "\n",
    "- input_file     training file path (required)\n",
    "- output         output file path (required)\n",
    "- lr             learning rate [0.05]\n",
    "- lr_update_rate change the rate of updates for the learning rate [100]\n",
    "- dim            size of word vectors [100]\n",
    "- ws             size of the context window [5]\n",
    "- epoch          number of epochs [5]\n",
    "- min_count      minimal number of word occurences [5]\n",
    "- neg            number of negatives sampled [5]\n",
    "- word_ngrams    max length of word ngram [1]\n",
    "- loss           loss function {ns, hs, softmax} [ns]\n",
    "- bucket         number of buckets [2000000]\n",
    "- minn           min length of char ngram [3]\n",
    "- maxn           max length of char ngram [6]\n",
    "- thread         number of threads [12]\n",
    "- t              sampling threshold [0.0001]\n",
    "- silent         disable the log output from the C++ extension [1]\n",
    "- encoding       specify input_file encoding [utf-8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "아래는 fasttext의 skipgram 모델을 학습하는 코드입니다. \n",
    "\n",
    "model_fname을 반드시 적어야 학습이 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "raw_corpus_fname = '' # Fill your corpus file\n",
    "model_fname = ''      # Fill your model file\n",
    "skipgram_model = fasttext.cbow(raw_corpus_fname, model_fname, loss = 'hs', ws=1, lr = 0.01, dim = 150, epoch = 5, min_count = 10, encoding = 'utf-8', thread = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "fasttext는 Word2Vec처럼 한 단어에 대하여 가장 비슷한 단어를 찾는 most_similar() 함수가 없습니다. (version '0.8.2'기준)\n",
    "\n",
    "대신, 아래와 같이 두 단어의 cosine 유사도를 계산해줍니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71771962901622532"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram_model.cosine_similarity('오빠', '오빵')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Word2VecCorpus:\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "    def __iter__(self):\n",
    "        with open(self.fname, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip().replace('\\n', '')\n",
    "                if not line:\n",
    "                    continue\n",
    "                yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word2vec_corpus = Word2VecCorpus(raw_corpus_fname)\n",
    "word2vec_model = Word2Vec(word2vec_corpus, size=150, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "fasttext는 하나의 단어에 대하여 벡터를 직접 학습하지 않습니다. 대신에 subwords의 벡터들을 바탕으로 word의 벡터를 추정합니다. 마치 doc2vec에서 word vector를 이용하여 document vector를 추정하는 것과 같습니다. \n",
    "\n",
    "좀 더 자세히 말하자면 v(어디야)는 직접 학습되지 않습니다. 하지만 v(어디야)는 [v(어디), v(디야)]를 이용하여 추정됩니다. 즉 '어디야'라는 단어는 '어디', '디야'라는 subwords를 이용하여 추정되는 것입니다. \n",
    "\n",
    "그런데, 이 경우에는 오탈자에 민감하게 됩니다. '어딛야' 같은 경우에는 [v(어딛), v(딛야)]를 이용하기 때문에 [v(어디), v(디야)]와 겹치는 subwords가 없어서 비슷한 단어로 인식되기가 어렵습니다. \n",
    "\n",
    "이는 Edit distance 수업을 할 때 언급한 것과 같습니다. 한국어의 오탈자는 초/중/종성에서 한군데 정도가 틀리기 때문에 자음/모음을 풀어서 fasttext를 학습하는게 좋습니다. 즉 어디야는 'ㅇㅓ-ㄷㅣ-ㅇㅑ-'로 표현됩니다. 종성이 비어있을 경우에는 -으로 표시하였습니다. fasttext가 word를 학습할 때 띄어쓰기를 기준으로 나누기 때문입니다. \n",
    "\n",
    "subwords는 반드시 2글자는 아닙니다. subwords의 frequency를 기준으로 fasttext 모델이 적절한 수준의 subwords를 선택하여 학습합니다. subwords의 예시를 적자면 아래와 같습니다. \n",
    "\n",
    "    'ㅇㅓ-ㄷㅣ-ㅇㅑ-' = [ㅇㅓ-, ㄷㅣ, -ㅇㅑ-]\n",
    "\n",
    "아래는 초/중/종성이 완전한 한글을 제외한 다른 글자를 제거하며 음절을 초/중/종성으로 분리하는 코드입니다. 이를 이용하여 단어를초중종성으로 나눠놓은 jamo_corpus를 만들어서 skipgram_jamo_model을 학습시키십시요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "jamo_corpus_fname = ''   # Fill your jamo corpus\n",
    "jamo_model_fname = ''    # Fill your model\n",
    "skipgram_jamo_model = fasttext.cbow(jamo_corpus_fname, jamo_model_fname, loss = 'hs', ws=1, lr = 0.01, dim = 150, epoch = 5, min_count = 10, encoding = 'utf-8', thread = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ㅇㅓ_ㅇㅣ_ㄱㅗ_ ㅋㅔㄱㅋㅔㄱ ㅇㅏ_ㅇㅣ_ㄱㅗ_ㅇㅗ_'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../soy/')\n",
    "from soy.nlp.hangle import split_jamo\n",
    "\n",
    "def preprocessing(s):\n",
    "    s_ = ' '\n",
    "    for c in s:\n",
    "        if c == ' ' and s_[-1] != ' ':\n",
    "            s_ += ' '\n",
    "            continue\n",
    "        jamo = split_jamo(c)\n",
    "        if (not jamo):\n",
    "            if s_[-1] != ' ':\n",
    "                s_ += ' '\n",
    "            continue\n",
    "        if (len(jamo) != 3) or (jamo[0] == ' ' or jamo[1] == ' '):\n",
    "            s_ += ' '\n",
    "            continue\n",
    "        if jamo[2] == ' ':\n",
    "            jamo[2] = '_'\n",
    "        s_ += ''.join(jamo)\n",
    "    return s_.strip()\n",
    "\n",
    "\n",
    "preprocessing('어이고ㅋaaf 켁켁 아이고오aaaaa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "'어디야'라는 단어에 대하여 word2vec 기준 비슷한 단어 30개를 출력했습니다. 그리고 초/중/종성을 분리하지 않은 fasttext와 분리한 fasttext에서의 similarity를 함께 출력했습니다. \n",
    "\n",
    "      어디야 - 어디양:\t (0.774, 0.866, 0.921)\n",
    "\n",
    "Word2Vec에서는 '어디야'와 '어디양'는 문맥만 고려하여 유사도가 0.751이지만, fasttext에서는 v(어디)가 공유되고, v(디야)와 v(디양)이 비슷하여 더 비슷하게 나왔습니다. 하지만 초/중/종성을 모두 풀었을 경우에는 유사도가 좀 더 올라가는 걸 볼 수 있습니다. \n",
    "\n",
    "fasttext는 word2vec과 같이 문맥적인 유사도와 subwords의 유사도까지 모두 고려하기 때문입니다. 즉 fasttext는 **형태적 유사성**과 **문맥적 유사성**을 동시에 고려합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query - words         (word2vec, fasttext, jamo_fasttext)\n",
      "  어디야 -    어댜:\t (0.825, 0.850, 0.886)\n",
      "  어디야 -   어디여:\t (0.821, 0.883, 0.909)\n",
      "  어디야 -   어디얌:\t (0.816, 0.867, 0.934)\n",
      "  어디야 -   어디고:\t (0.778, 0.854, 0.695)\n",
      "  어디야 -   어디양:\t (0.774, 0.866, 0.921)\n",
      "  어디야 -   어디니:\t (0.758, 0.854, 0.873)\n",
      "  어디야 -   어디임:\t (0.753, 0.854, 0.886)\n",
      "  어디야 -   어딘데:\t (0.751, 0.837, 0.882)\n",
      "  어디야 -   어디냐:\t (0.748, 0.844, 0.850)\n",
      "  어디야 -   오디야:\t (0.745, 0.824, 0.893)\n",
      "  어디야 -  어디야?:\t (0.744, 0.850, 1.000)\n",
      "  어디야 - 어디야??:\t (0.719, 0.836, 1.000)\n",
      "  어디야 -  어디에요:\t (0.705, 0.792, 0.768)\n",
      "  어디야 -   어딘뎅:\t (0.701, 0.770, 0.865)\n",
      "  어디야 -  어딘데?:\t (0.696, 0.801, 0.882)\n",
      "  어디야 -    어뎌:\t (0.693, 0.670, 0.770)\n",
      "  어디야 - 어딘데??:\t (0.673, 0.763, 0.882)\n",
      "  어디야 - 어디쯤이야:\t (0.672, 0.761, 0.832)\n",
      "  어디야 -  어디예요:\t (0.668, 0.772, 0.777)\n",
      "  어디야 -  어디니?:\t (0.662, 0.766, 0.873)\n",
      "  어디야 -  어디여?:\t (0.661, 0.785, 0.909)\n",
      "  어디야 -   어댜?:\t (0.658, 0.754, 0.886)\n",
      "  어디야 -  오디야?:\t (0.655, 0.784, 0.893)\n",
      "  어디야 -  어디고?:\t (0.637, 0.743, 0.695)\n",
      "  어디야 -  어디얌?:\t (0.635, 0.766, 0.934)\n",
      "  어디야 -   오디양:\t (0.634, 0.732, 0.737)\n",
      "  어디야 -  어딘뎅?:\t (0.632, 0.765, 0.865)\n",
      "  어디야 -  어디에여:\t (0.630, 0.763, 0.792)\n",
      "  어디야 -   오디얌:\t (0.628, 0.726, 0.757)\n",
      "  어디야 - 어디쯤이야?:\t (0.627, 0.734, 0.832)\n"
     ]
    }
   ],
   "source": [
    "query = '어디야'\n",
    "print('query - words         (word2vec, fasttext, jamo_fasttext)')\n",
    "for word, sim in word2vec_model.most_similar(query, topn=30):\n",
    "    fasttext_sim = skipgram_model.cosine_similarity(query, word)\n",
    "    jamo_fasttext_sim = skipgram_jamo_model.cosine_similarity(preprocessing(query), preprocessing(word))\n",
    "    print('%5s - %5s:\\t (%.3f, %.3f, %.3f)' % (query, word, sim, fasttext_sim, jamo_fasttext_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Word2Vec의 경우에는 '배거프당'과 '다씻었다'의 문맥적 유사도가 높아서 0.898의 유사도가 나왔습니다. \n",
    "\n",
    "    배거프당 - 다씻엇다:\t (0.898, 0.547, 0.441)\n",
    "\n",
    "하지만, fasttext와 jamo fasttext의 경우에는 형태적 유사성이 거의 없기 때문에 각각 0.547과 0.441로 유사도가 줄어듭니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query - words         (word2vec, fasttext, jamo_fasttext)\n",
      " 배거프당 - 배고파배고파:\t (0.921, 0.757, 0.687)\n",
      " 배거프당 -  배고프댜:\t (0.920, 0.646, 0.680)\n",
      " 배거프당 - 배부르다잉:\t (0.916, 0.666, 0.718)\n",
      " 배거프당 - 배고프다잉:\t (0.916, 0.689, 0.772)\n",
      " 배거프당 - 배부르다아:\t (0.915, 0.706, 0.730)\n",
      " 배거프당 -   배고퐝:\t (0.915, 0.657, 0.591)\n",
      " 배거프당 - 배고프다아아:\t (0.913, 0.780, 0.768)\n",
      " 배거프당 -   배거팡:\t (0.911, 0.789, 0.882)\n",
      " 배거프당 -  배부르르:\t (0.911, 0.634, 0.529)\n",
      " 배거프당 - 배고프다아:\t (0.910, 0.760, 0.781)\n",
      " 배거프당 -   구래앵:\t (0.910, 0.468, 0.299)\n",
      " 배거프당 -  배고프닷:\t (0.910, 0.677, 0.732)\n",
      " 배거프당 -   배고팜:\t (0.910, 0.670, 0.649)\n",
      " 배거프당 -   배곱하:\t (0.909, 0.716, 0.667)\n",
      " 배거프당 -  배고파앙:\t (0.908, 0.749, 0.751)\n",
      " 배거프당 -  배불러어:\t (0.908, 0.725, 0.672)\n",
      " 배거프당 -  보곺보곺:\t (0.907, 0.547, 0.204)\n",
      " 배거프당 -   배부러:\t (0.906, 0.622, 0.539)\n",
      " 배거프당 -  배곱배곱:\t (0.906, 0.712, 0.668)\n",
      " 배거프당 - 배고프다앙:\t (0.906, 0.723, 0.784)\n",
      " 배거프당 -  배고파잉:\t (0.906, 0.730, 0.737)\n",
      " 배거프당 -  배부르댱:\t (0.904, 0.605, 0.673)\n",
      " 배거프당 -  기엽겟당:\t (0.902, 0.430, 0.497)\n",
      " 배거프당 -  배부르댜:\t (0.901, 0.472, 0.656)\n",
      " 배거프당 -   뱌고파:\t (0.900, 0.495, 0.514)\n",
      " 배거프당 -  배고파유:\t (0.899, 0.713, 0.704)\n",
      " 배거프당 -  다씻엇다:\t (0.898, 0.547, 0.441)\n",
      " 배거프당 -  많이잤다:\t (0.898, 0.326, 0.208)\n",
      " 배거프당 -  배고푸닷:\t (0.898, 0.725, 0.754)\n",
      " 배거프당 -   배터졍:\t (0.896, 0.562, 0.573)\n"
     ]
    }
   ],
   "source": [
    "query = '배거프당'\n",
    "print('query - words         (word2vec, fasttext, jamo_fasttext)')\n",
    "for word, sim in word2vec_model.most_similar(query, topn=30):\n",
    "    fasttext_sim = skipgram_model.cosine_similarity(query, word)\n",
    "    jamo_fasttext_sim = skipgram_jamo_model.cosine_similarity(preprocessing(query), preprocessing(word))\n",
    "    print('%5s - %5s:\\t (%.3f, %.3f, %.3f)' % (query, word, sim, fasttext_sim, jamo_fasttext_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Word2Vec의 경우에는 '짜파게티'의 유사단어가 아래와 같이 분식들이 나왔습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('비빔면', 0.9303897023200989),\n",
       " ('불닭볶음면', 0.9284998178482056),\n",
       " ('토스트', 0.9267774820327759),\n",
       " ('베이글', 0.9165289402008057),\n",
       " ('비빔국수', 0.9150125980377197),\n",
       " ('라볶이', 0.914039134979248),\n",
       " ('갈비찜', 0.9139703512191772),\n",
       " ('삼각김밥', 0.9129242897033691),\n",
       " ('부침개', 0.9123827219009399),\n",
       " ('라묜', 0.9121387004852295)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.most_similar('짜파게티')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "하지만 '짜파게티'의 오탈자인 '짭파게티'는 코퍼스에 거의 등장하지 않기 때문에 학습이 아예 되어있지 않습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word '짭파게티' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-5579358285ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword2vec_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'짭파게티'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/lovit/anaconda2/envs/jupyter_server_py3/lib/python3.5/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab)\u001b[0m\n\u001b[0;32m   1231\u001b[0m                 \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1233\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1234\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot compute similarity with no input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word '짭파게티' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "word2vec_model.most_similar('짭파게티')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "fasttext의 가치는 여기에 있습니다. '짭파게티'와 같은 오탈자라고 하여도 초/중/종성을 분리하였다면 v(짜파게티)는 'ㅉㅏㅂㅍㅏ-ㄱㅔ-ㅌㅣ-'의 subwords들로 추정됩니다. '짜파게티'와 '짭파게티'의 초/중/종성을 분리한 subwords들이 비슷하기 때문에 v(짭파게티)는 v(짜파게티)와 비슷하게 추정될 수 있습니다. \n",
    "\n",
    "추정된 v(짭파게티)와 word2vec에서 '짜파게티'와 비슷한 10개의 단어와의 Cosine similarity를 계산하면 실제로 비슷하게 나옵니다. 하지만 '짜파게티'와의 유사도보다는 조금 감소합니다. \n",
    "\n",
    "    비빔면: 0.930 -> 0.840\n",
    "    불닭볶음면: 0.928 -> 0.860\n",
    "    \n",
    "이와 같이 fasttext는 Word2Vec에서 일어나는 out of vocabulary problems (오탈자 혹은 신조어)에 대해서도 추정할 수 있는 장점이 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jamo fasttext: 짭파게티 -   비빔면:\t (0.840)\t|\tword2vec: 짜파게티 -   비빔면: (0.930)\n",
      "jamo fasttext: 짭파게티 - 불닭볶음면:\t (0.860)\t|\tword2vec: 짜파게티 - 불닭볶음면: (0.928)\n",
      "jamo fasttext: 짭파게티 -   토스트:\t (0.788)\t|\tword2vec: 짜파게티 -   토스트: (0.927)\n",
      "jamo fasttext: 짭파게티 -   베이글:\t (0.688)\t|\tword2vec: 짜파게티 -   베이글: (0.917)\n",
      "jamo fasttext: 짭파게티 -  비빔국수:\t (0.847)\t|\tword2vec: 짜파게티 -  비빔국수: (0.915)\n",
      "jamo fasttext: 짭파게티 -   라볶이:\t (0.816)\t|\tword2vec: 짜파게티 -   라볶이: (0.914)\n",
      "jamo fasttext: 짭파게티 -   갈비찜:\t (0.740)\t|\tword2vec: 짜파게티 -   갈비찜: (0.914)\n",
      "jamo fasttext: 짭파게티 -  삼각김밥:\t (0.803)\t|\tword2vec: 짜파게티 -  삼각김밥: (0.913)\n",
      "jamo fasttext: 짭파게티 -   부침개:\t (0.759)\t|\tword2vec: 짜파게티 -   부침개: (0.912)\n",
      "jamo fasttext: 짭파게티 -    라묜:\t (0.713)\t|\tword2vec: 짜파게티 -    라묜: (0.912)\n"
     ]
    }
   ],
   "source": [
    "for word, sim in word2vec_model.most_similar('짜파게티', topn=10):\n",
    "    jamo_fasttext_sim = skipgram_jamo_model.cosine_similarity(preprocessing('짭파게티'), preprocessing(word))\n",
    "    print('jamo fasttext: 짭파게티 - %5s:\\t (%.3f)\\t|\\tword2vec: 짜파게티 - %5s: (%.3f)' % (word, jamo_fasttext_sim, word, sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
