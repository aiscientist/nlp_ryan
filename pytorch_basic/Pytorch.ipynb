{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x106d9cc70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://github.com/rguthrie3/DeepLearningForNLPInPytorch/blob/master/Deep%20Learning%20for%20Natural%20Language%20Processing%20with%20Pytorch.ipynb \n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Torch Library 소개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 1  2  3\n",
      " 4  5  6\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  1  2\n",
      "  3  4\n",
      "\n",
      "(1 ,.,.) = \n",
      "  5  6\n",
      "  7  8\n",
      "[torch.FloatTensor of size 2x2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a torch.Tensor object with the given data.  It is a 1D vector\n",
    "V_data = [1., 2., 3.]\n",
    "V = torch.Tensor(V_data)\n",
    "print(V)\n",
    "\n",
    "# Creates a matrix\n",
    "M_data = [[1., 2., 3.], [4., 5., 6]]\n",
    "M = torch.Tensor(M_data)\n",
    "print(M)\n",
    "\n",
    "# Create a 3D tensor of size 2x2x2.\n",
    "T_data = [[[1.,2.], [3.,4.]],\n",
    "          [[5.,6.], [7.,8.]]]\n",
    "T = torch.Tensor(T_data)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 1  2\n",
      " 3  4\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index into V and get a scalar\n",
    "print(V[0])\n",
    "\n",
    "# Index into M and get a vector\n",
    "print(M[0])\n",
    "\n",
    "# Index into T and get a matrix\n",
    "print(T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "  0.6614  0.2669  0.0617  0.6213 -0.4519\n",
      " -0.1661 -1.5228  0.3817 -1.0276 -0.5631\n",
      " -0.8923 -0.0583 -0.1955 -0.9656  0.4224\n",
      "  0.2673 -0.4212 -0.5107 -1.5727 -0.1232\n",
      "\n",
      "(1 ,.,.) = \n",
      "  3.5870 -1.8313  1.5987 -1.2770  0.3255\n",
      " -0.4791  1.3790  2.5286  0.4107 -0.9880\n",
      " -0.9081  0.5423  0.1103 -2.2590  0.6067\n",
      " -0.1383  0.8310 -0.2477 -0.8029  0.2366\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.2857  0.6898 -0.6331  0.8795 -0.6842\n",
      "  0.4533  0.2912 -0.8317 -0.5525  0.6355\n",
      " -0.3968 -0.6571 -1.6428  0.9803 -0.0421\n",
      " -0.8206  0.3133 -1.1352  0.3773 -0.2824\n",
      "[torch.FloatTensor of size 3x4x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Default는 Float입니다.\n",
    "x = torch.randn((3, 4, 5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-2.5667 -1.4303  0.5009  0.5438 -0.4057\n",
      " 1.1341 -1.1115  0.3501 -0.7703 -0.1473\n",
      " 0.6272  1.0935  0.0939  1.2381 -1.3459\n",
      " 0.5119 -0.6933 -0.1668 -0.9999 -1.6476\n",
      " 0.8098  0.0554  1.1340 -0.5326  0.6592\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n",
      "\n",
      "-1.5964 -0.3769 -3.1020 -0.0020 -1.0952  0.6016  0.6984 -0.8005\n",
      "-0.0995 -0.7213  1.2708  1.5381  1.4673  1.5951 -1.5279  1.0156\n",
      "[torch.FloatTensor of size 2x8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# By default, it concatenates along the first axis (concatenates rows)\n",
    "# 간단하게 말해서 그냥 아래에 쌓는 기능\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 =torch.cat([x_1, y_1])\n",
    "print(z_1)\n",
    "\n",
    "# Concatenate columns:\n",
    "# Column 형으로 쌓는 기능\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "z_2 = torch.cat([x_2, y_2], 1) # second arg specifies which axis to concat along\n",
    "print(z_2)\n",
    "\n",
    "# If your tensors are not compatible, torch will complain.  Uncomment to see the error\n",
    "# torch.cat([x_1, x_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      " -0.2020 -1.2865  0.8231 -0.6101\n",
      " -1.2960 -0.9434  0.6684  1.1628\n",
      " -0.3229  1.8782 -0.5666  0.4016\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.1153  0.3170  0.5629  0.8662\n",
      " -0.3528  0.3482  1.1371 -0.3339\n",
      " -1.4724  0.7296 -0.1312 -0.6368\n",
      "[torch.FloatTensor of size 2x3x4]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.2020 -1.2865  0.8231 -0.6101 -1.2960 -0.9434  0.6684  1.1628 -0.3229  1.8782\n",
      "-0.1153  0.3170  0.5629  0.8662 -0.3528  0.3482  1.1371 -0.3339 -1.4724  0.7296\n",
      "\n",
      "Columns 10 to 11 \n",
      "-0.5666  0.4016\n",
      "-0.1312 -0.6368\n",
      "[torch.FloatTensor of size 2x12]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.2020 -1.2865  0.8231 -0.6101 -1.2960 -0.9434  0.6684  1.1628 -0.3229  1.8782\n",
      "-0.1153  0.3170  0.5629  0.8662 -0.3528  0.3482  1.1371 -0.3339 -1.4724  0.7296\n",
      "\n",
      "Columns 10 to 11 \n",
      "-0.5666  0.4016\n",
      "-0.1312 -0.6368\n",
      "[torch.FloatTensor of size 2x12]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12)) # Reshape to 2 rows, 12 columns\n",
    "print(x.view(2, -1)) # Same as above.  If one of the dimensions is -1, its size can be inferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Computation Graphs & Auto. Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 5\n",
      " 7\n",
      " 9\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "<AddBackward1 object at 0x1064c00b8>\n"
     ]
    }
   ],
   "source": [
    "# Variables wrap tensor objects\n",
    "x = autograd.Variable( torch.Tensor([1., 2., 3]), requires_grad=True )\n",
    "# You can access the data with the .data attribute\n",
    "print(x.data)\n",
    "\n",
    "# You can also do all the same operations you did with tensors with Variables.\n",
    "y = autograd.Variable( torch.Tensor([4., 5., 6]), requires_grad=True )\n",
    "z = x + y\n",
    "print(z.data)\n",
    "\n",
    "# BUT z knows something extra.\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 21\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "<SumBackward0 object at 0x10655b780>\n"
     ]
    }
   ],
   "source": [
    "# Lets sum up all the entries in z\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s.backward() # calling .backward() on any variable will run backprop, starting from it.\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아래 관련된 정보를 이해해야함. 매우 중요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((2,2))\n",
    "y = torch.randn((2,2))\n",
    "z = x + y # These are Tensor types, and backprop would not be possible\n",
    "\n",
    "var_x = autograd.Variable( x )\n",
    "var_y = autograd.Variable( y )\n",
    "var_z = var_x + var_y # var_z contains enough information to compute gradients, as we saw above\n",
    "print(var_z.grad_fn)\n",
    "\n",
    "var_z_data = var_z.data # Get the wrapped Tensor object out of var_z...\n",
    "new_var_z = autograd.Variable( var_z_data ) # Re-wrap the tensor in a new variable\n",
    "\n",
    "# ... does new_var_z have information to backprop to x and y?\n",
    "# NO!\n",
    "print(new_var_z.grad_fn)\n",
    "# And how could it?  We yanked the tensor out of var_z (that is what var_z.data is).  This tensor\n",
    "# doesn't know anything about how it was computed.  We pass it into new_var_z, and this is all the information\n",
    "# new_var_z gets.  If var_z_data doesn't know how it was computed, theres no way new_var_z will.\n",
    "# In essence, we have broken the variable away from its past history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Learning Building Blocks: Affine maps, non-linearities and obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.4491 -0.1994  0.0853\n",
      "-0.0392  0.2063  0.0476\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Affine Maps: f(x) + Ax + b\n",
    "\n",
    "lin = nn.Linear(5,3) #Param A,b\n",
    "data = autograd.Variable( torch.randn(2, 5)) #data 2x5.\n",
    "\n",
    "print(lin(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.1277 -0.9501\n",
      " 1.4064 -0.4549\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 0.0000  0.0000\n",
      " 1.4064  0.0000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In pytorch, most non-linearities are in torch.functional (we have it imported as F)\n",
    "data = autograd.Variable( torch.randn(2, 2) )\n",
    "print(data)\n",
    "print(F.relu(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.5838\n",
      "-0.9699\n",
      "-0.9469\n",
      "-0.2330\n",
      " 0.6424\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      " 0.3413\n",
      " 0.0722\n",
      " 0.0739\n",
      " 0.1508\n",
      " 0.3619\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "-1.0750\n",
      "-2.6287\n",
      "-2.6057\n",
      "-1.8918\n",
      "-1.0164\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinseongjin/tf_nightly/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"\n",
      "/Users/sinseongjin/tf_nightly/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n",
      "/Users/sinseongjin/tf_nightly/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#Softmax and Prob\n",
    "\n",
    "data = autograd.Variable( torch.randn(5) )\n",
    "print(data)\n",
    "print(F.softmax(data))\n",
    "print(F.softmax(data).sum())\n",
    "print(F.log_softmax(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimization & Training\n",
    "# 5. Creating NW Components in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Bag-of-Words classifier\n",
    "\n",
    "data = [ (\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "         (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "         (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "         (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\") ]\n",
    "\n",
    "test_data = [ (\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "              (\"it is lost on me\".split(), \"ENGLISH\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n"
     ]
    }
   ],
   "source": [
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
    "# index into the Bag of words vector\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            #print(len(word_to_ix))\n",
    "\n",
    "print(word_to_ix)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module): # inheriting from nn.Module!\n",
    "    \n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        \n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is num_labels!\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "        \n",
    "        # NOTE! The non-linearity log softmax does not have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "        \n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.0372 -0.0723  0.0818 -0.0668 -0.1102  0.0445 -0.1418 -0.0419  0.1002  0.0733\n",
      "-0.1794 -0.1151  0.1199  0.0430 -0.0692 -0.0741  0.1253  0.1414  0.1889  0.0572\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.1670 -0.1338  0.0017 -0.0579 -0.1097 -0.1103  0.0494  0.0205 -0.1001 -0.1382\n",
      " 0.0947 -0.0158 -0.1147 -0.1921  0.1195 -0.0285  0.0806  0.0097 -0.1830 -0.0962\n",
      "\n",
      "Columns 20 to 25 \n",
      "-0.1625  0.1922 -0.0334 -0.0453 -0.0721 -0.0981\n",
      " 0.0552 -0.0397 -0.1894  0.0688  0.0075 -0.0630\n",
      "[torch.FloatTensor of size 2x26]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-02 *\n",
      "  7.9225\n",
      "  4.1404\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.6198 -0.7723\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinseongjin/tf_nightly/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# To run the model, pass in a BoW vector, but wrapped in an autograd.Variable\n",
    "sample = data[0]\n",
    "bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "log_probs = model(autograd.Variable(bow_vector))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = { \"SPANISH\": 0, \"ENGLISH\": 1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.7991 -0.5974\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-0.6153 -0.7776\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      " 0.1670\n",
      " 0.0947\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinseongjin/tf_nightly/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "#Train 시키기\n",
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "\n",
    "print(next(model.parameters())[:,word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinseongjin/tf_nightly/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Usually you want to pass over the training data several times.\n",
    "# 100 is much bigger than on a real data set, but real datasets have more than\n",
    "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # Step 1. 파이토치는 gradients를 쌓으므로 처음에 초기화 하자\n",
    "        # before each instance\n",
    "        model.zero_grad()\n",
    "    \n",
    "        # Step 2. Make our BOW vector and also we must wrap the target in a Variable\n",
    "        # as an integer.  For example, if the target is SPANISH, then we wrap the integer\n",
    "        # 0.  The loss function then knows that the 0th element of the log probabilities is\n",
    "        # the log probability corresponding to SPANISH\n",
    "        bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "        target = autograd.Variable(make_target(label, label_to_ix))\n",
    "    \n",
    "        # Step 3. Run our forward pass.\n",
    "        log_probs = model(bow_vec)\n",
    "    \n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by calling\n",
    "        # optimizer.step()\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.1232 -2.1547\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-2.7729 -0.0645\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      " 0.6655\n",
      "-0.4038\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinseongjin/tf_nightly/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)\n",
    "print(next(model.parameters())[:,word_to_ix[\"creo\"]]) # Index corresponding to Spanish goes up, English goes down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Yo', 'creo', 'que', 'si'], 'SPANISH'),\n",
       " (['it', 'is', 'lost', 'on', 'me'], 'ENGLISH')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
