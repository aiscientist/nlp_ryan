{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import csv\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "#tfe.enable_eager_execution()\n",
    "\n",
    "embed_size = 10\n",
    "window_size = 3\n",
    "dict_size = 461\n",
    "batch_size = 10\n",
    "max_length = 20\n",
    "filter_size = window_size*embed_size\n",
    "\n",
    "\n",
    "def transform(id_map):\n",
    "    raw_dir = '/home/dataset/raw_data/thugsta_db.tsv'\n",
    "    train_tfrecord_dir = './corpus_set.tfrecord'\n",
    "\n",
    "    dataset_writer = tf.python_io.TFRecordWriter(train_tfrecord_dir)\n",
    "\n",
    "    with open(raw_dir, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=u'\\t')\n",
    "        corpus = []\n",
    "        for i, row in enumerate(reader):\n",
    "            if i == 0: continue\n",
    "            for ians in range(1, 6):\n",
    "                contents = [id_map[w] if w in id_map else id_map['UNK'] for w in row[0].split(' ') if not w == '']\n",
    "                answer = [id_map[w] if w in id_map else id_map['UNK'] for w in row[ians].split(' ') if not w == '']\n",
    "                if answer:\n",
    "                    print(contents)\n",
    "                    print(answer)\n",
    "                    example = tf.train.Example()\n",
    "                    example.features.feature['contents'].int64_list.value.extend(contents)\n",
    "                    example.features.feature['answer'].int64_list.value.extend(answer)\n",
    "                    example.features.feature['contents_length'].int64_list.value.append(len(contents))\n",
    "                    example.features.feature['answer_length'].int64_list.value.append(len(answer))\n",
    "                    dataset_writer.write(example.SerializeToString())\n",
    "            print('{} written'.format(i))\n",
    "\n",
    "\n",
    "\n",
    "def create_id_map():\n",
    "    raw_dir = '/home/dataset/raw_data/thugsta_db.tsv'\n",
    "\n",
    "    dicts = {'UNK': 0}\n",
    "    with open(raw_dir, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=u'\\t')\n",
    "        for row in reader:\n",
    "            words = reduce(lambda x, y: x+' '+y, row).split(' ')\n",
    "            for w in words:\n",
    "                if w == '': continue\n",
    "                if w in dicts:\n",
    "                    dicts[w] += 1\n",
    "                else:\n",
    "                    dicts[w] = 1\n",
    "\n",
    "    word_rank = sorted(dicts, key=dicts.__getitem__, reverse=True)\n",
    "    word_ids = enumerate(word_rank)\n",
    "    id_map = dict(map(reversed, word_ids))\n",
    "    return id_map, word_rank\n",
    "\n",
    "\n",
    "\n",
    "def proto_input_fn():\n",
    "    ids = tf.constant([[1, 2, 3, 4, 3, 2, 1], [1, 2, 3, 2, 3, 2, 3]])\n",
    "    label_ids = tf.constant([[0, 0, 0, 1, 3, 1, 3, 2], [0, 0, 0, 2, 1, 3, 4, 4]])\n",
    "    return {'ids': ids,\n",
    "            'init_label': label_ids[:, :window_size],\n",
    "            'label_length': int(label_ids.shape[1])-window_size\n",
    "            }, label_ids[:, window_size:]\n",
    "\n",
    "\n",
    "def parser(serialized_example):\n",
    "    feature = {\n",
    "            'contents': tf.VarLenFeature(tf.int64),\n",
    "            'answer': tf.VarLenFeature(tf.int64)}\n",
    "\n",
    "\n",
    "    parsed_feature = tf.parse_single_example(serialized_example, feature)\n",
    "\n",
    "    contents = tf.cast(parsed_feature['contents'], tf.int32)\n",
    "    answer = tf.cast(parsed_feature['answer'], tf.int32)\n",
    "    return contents, answer\n",
    "\n",
    "\n",
    "def input_fn():\n",
    "    train_tfrecord_dir = './corpus_set.tfrecord'\n",
    "    dataset = tf.data.TFRecordDataset(train_tfrecord_dir).map(parser)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    itr = dataset.make_one_shot_iterator()\n",
    "\n",
    "    contents, answer = itr.get_next()\n",
    "\n",
    "    output_shape = tf.constant([batch_size, max_length], tf.int64)\n",
    "    contents = tf.sparse_to_dense(contents.indices, output_shape, contents.values)\n",
    "    answer = tf.sparse_to_dense(answer.indices, output_shape, answer.values)\n",
    "\n",
    "    return {'ids': contents,\n",
    "            'init_label': tf.zeros(shape=[batch_size, window_size], dtype=tf.int32),\n",
    "            'label_length': int(answer.shape[1])\n",
    "            }, answer\n",
    "\n",
    "\n",
    "def model_fn(mode, features, labels):\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "\n",
    "    embeddings = tf.Variable(tf.truncated_normal([dict_size, embed_size]), trainable=False)\n",
    "\n",
    "    # embedding\n",
    "    input_embeds = tf.nn.embedding_lookup(embeddings, features['ids'])\n",
    "\n",
    "    input_flat = tf.layers.flatten(input_embeds)\n",
    "    input_flat = tf.expand_dims(input_flat, -1)\n",
    "\n",
    "    init_label = tf.nn.embedding_lookup(embeddings, features['init_label'])\n",
    "    init_label = tf.layers.flatten(init_label)\n",
    "    init_label = tf.expand_dims(init_label, -1)\n",
    "\n",
    "    if not PREDICT:\n",
    "        label_onehot = tf.one_hot(labels, depth=dict_size, dtype=tf.float32)\n",
    "\n",
    "    # encoder\n",
    "    encoder_conv = tf.layers.conv1d(\n",
    "            inputs=input_flat,\n",
    "            filters=2*embed_size,\n",
    "            kernel_size=filter_size,\n",
    "            strides=embed_size,\n",
    "            padding='same')\n",
    "\n",
    "    encoder_glu = encoder_conv[:, :, embed_size:]*tf.nn.sigmoid(encoder_conv[:, :, :embed_size])\n",
    "\n",
    "    #decoder\n",
    "    next_ids = []\n",
    "    outs = []\n",
    "    for l in range(features['label_length']):\n",
    "        if l == 0: decoder_input = init_label\n",
    "\n",
    "        decoder_conv = tf.layers.conv1d(\n",
    "                inputs=init_label,\n",
    "                filters=2*embed_size,\n",
    "                kernel_size=filter_size,\n",
    "                strides=embed_size)\n",
    "\n",
    "\n",
    "        decoder_glu = decoder_conv[:, :, embed_size:]*tf.nn.sigmoid(decoder_conv[:, :, :embed_size])\n",
    "\n",
    "        tiled_decoder_glu = tf.tile(decoder_glu, [1, int(encoder_glu.shape[1]), 1])\n",
    "\n",
    "\n",
    "        dot_prod = tf.matmul(encoder_glu, decoder_glu, transpose_b=True)\n",
    "\n",
    "        attention = tf.nn.softmax(dot_prod, axis=1)\n",
    "\n",
    "        z_plus_e = encoder_glu + input_embeds\n",
    "\n",
    "        tiled_attention = tf.tile(attention, [1, 1, embed_size])\n",
    "\n",
    "        c = tf.reduce_sum(tiled_attention*z_plus_e, axis=1)\n",
    "        decoder_glu = tf.reshape(decoder_glu, [-1, embed_size])\n",
    "\n",
    "        logits = tf.layers.dense(c+decoder_glu, dict_size)\n",
    "\n",
    "        out = tf.nn.softmax(logits)\n",
    "\n",
    "        next_id = tf.argmax(out, axis=1)\n",
    "        next_embeds = tf.nn.embedding_lookup(embeddings, next_id)\n",
    "        next_embeds = tf.expand_dims(next_embeds, -1)\n",
    "        decoder_input = tf.concat([decoder_input[:, embed_size:], next_embeds], axis=1)\n",
    "\n",
    "        next_ids.append(next_id)\n",
    "        outs.append(logits)\n",
    "\n",
    "    outs = tf.stack(outs, axis=1)\n",
    "    next_ids = tf.stack(next_ids, axis=1)\n",
    "\n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        loss = tf.losses.softmax_cross_entropy(label_onehot, outs)\n",
    "        train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss, global_step)\n",
    "        estimator_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                train_op=train_op,\n",
    "                loss=loss)\n",
    "\n",
    "    elif EVAL:\n",
    "        loss = tf.losses.softmax_cross_entropy(label_onehot, outs)\n",
    "        eval_metric_ops = {'acc': tf.metrics.accuracy(labels, next_ids)}\n",
    "        estimator_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=loss,\n",
    "                eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "    elif PREDICT:\n",
    "        estimator_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions={'prediction': next_ids})\n",
    "\n",
    "    else:\n",
    "        raise Exception('estiamtor spec is invalid')\n",
    "\n",
    "    return estimator_spec\n",
    "\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    id_map, word_map = create_id_map()\n",
    "\n",
    "    fairseq = tf.estimator.Estimator(model_fn, model_dir='./checkpoint/model')\n",
    "#    fairseq.train(input_fn, steps=10000)\n",
    "#    fairseq.evaluate(input_fn, steps=10)\n",
    "\n",
    "    pred = fairseq.predict(input_fn)\n",
    "    for p in pred:\n",
    "        words = map(lambda i: word_map[i], p['prediction'])\n",
    "        answers = ' '.join(words)\n",
    "        print(answers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
