{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기본 값\n",
    "df_path = './dataset/chat_train_ids.in'\n",
    "buckets = [(12, 40)]\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "user_vocab_size = 9687\n",
    "bot_vocab_size = 12169"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Dataset\n",
    "\n",
    "- Datset API사용 (큐 기반)\n",
    "- Clear Structure and Simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data line 1000\n",
      "read data line 2000\n",
      "read data line 3000\n",
      "read data line 4000\n",
      "read data line 5000\n",
      "sep by train, [0, 0, 20, 12, 35, 91, 180, 236, 229, 144, 76, 3977]\n",
      "train_size: 5000.0\n"
     ]
    }
   ],
   "source": [
    "#Load Sample Dataset\n",
    "\n",
    "def read_train_set(train_path, max_size=None):\n",
    "    train_set = [[] for _ in range(buckets[0][0])]\n",
    "    \n",
    "    with open(train_path, \"r\", encoding=\"utf-8\") as ts:\n",
    "        counter = 0\n",
    "        while not max_size or counter < max_size:\n",
    "            \n",
    "            # iterator 성질을 이용하여, 유저, 봇 발화 불러오기\n",
    "            user_tokens, bot_tokens = ts.readline(), ts.readline()\n",
    "            \n",
    "            # 유저 발화에서 모르는 토큰이 있으면 언노운 아이디 부여한다\n",
    "            user_token_ids = [int(x) for x in user_tokens.split()]\n",
    "\n",
    "            # 모르는 토큰이 있으면 언노운 아이디 부여\n",
    "            for i, user_token_id in enumerate(user_token_ids):\n",
    "                if user_token_id > user_vocab_size:\n",
    "                    user_token_ids[i] = UNK_ID\n",
    "\n",
    "            # 유저 버킷 크기로 잘라낸다\n",
    "            user_token_bucket = user_token_ids[:buckets[0][0]]\n",
    "            user_token_len = len(user_token_bucket)\n",
    "\n",
    "            # 봇 발화에서 모르는 토큰이 있으면 언노운 아이디 부여한다\n",
    "            bot_token_ids = [int(x) for x in bot_tokens.split()]\n",
    "            for i, bot_token_id in enumerate(bot_token_ids):\n",
    "                if bot_token_id > bot_vocab_size:\n",
    "                    bot_token_ids[i] = UNK_ID\n",
    "\n",
    "            # 봇 버킷 크기로 잘라낸다\n",
    "            bot_token_bucket = bot_token_ids[:(buckets[0][1]-1)]\n",
    "\n",
    "            # 유저 토큰 길이에 따라 유저 발화와 봇 발화를 모은다\n",
    "            train_set[user_token_len - 1].append([user_token_bucket, bot_token_bucket])\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            # 진행도를 알아볼 수 있도록 1000번마다 프린트한다\n",
    "            if counter % 1000 == 0:\n",
    "                print(\"read data line %d\" % counter)\n",
    "\n",
    "        return train_set\n",
    "    \n",
    "train_set = read_train_set(df_path, 5000)\n",
    "train_len_hist = [len(train_set[i]) for i in range(len(train_set))]\n",
    "train_size = float(sum(train_len_hist))\n",
    "\n",
    "print(\"sep by train, {}\".format(train_len_hist))\n",
    "print(\"train_size: {}\".format(train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set[4][0][0] #고객 발화\n",
    "# train_set[4][0][1] #봇 발화\n",
    "\n",
    "df_train = [train_set[4][i] for i in range(len(train_set[4]))] #Sample로 input size 가 5인 발화를 추출한다.\n",
    "df_user = [df_train[i][0] for i in range(len(df_train))]\n",
    "df_bot = [df_train[i][1] for i in range(len(df_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6c66e3c0354a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfilter_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "#skip\n",
    "\n",
    "embed_size = 1\n",
    "window_size = 3\n",
    "dict_size = 5\n",
    "filter_size = window_size*embed_size\n",
    "\n",
    "ids = tf.constant(df_train)\n",
    "\n",
    "\n",
    "def input_fn():\n",
    "    ids = tf.constant([[5, 6, 10, 9, 2], [5, 6, 10, 60, 2]])\n",
    "    label_ids = tf.constant([[4, 22, 163, 649, 2], [4, 22, 163, 649, 2]])\n",
    "    return {'ids': ids,\n",
    "            'init_label': label_ids[:, :window_size],\n",
    "            'label_length': int(label_ids.shape[1])-window_size\n",
    "            }, label_ids[:, window_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 0 0]\n",
      " [0 0 0]], shape=(2, 3), dtype=int32)\n",
      "5\n",
      "tf.Tensor(\n",
      "[[1 3 1 3 2]\n",
      " [2 1 3 4 4]], shape=(2, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "ids = tf.constant([[1, 2, 3, 4, 3, 2, 1], [1, 2, 3, 2, 3, 2, 3]])\n",
    "label_ids = tf.constant([[0, 0, 0, 1, 3, 1, 3, 2], [0, 0, 0, 2, 1, 3, 4, 4]]) #무조건 앞에 3개를... \n",
    "init_label = label_ids[:, :window_size]\n",
    "label_length = int(label_ids.shape[1]) - window_size\n",
    "label_ids = label_ids[:, window_size:]\n",
    "\n",
    "print(init_label)\n",
    "print(label_length)\n",
    "print(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "embeddings = tfe.Variable(tf.truncated_normal([dict_size, embed_size]), trainable=False)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Embedding at 0x11af7b8d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(tf.layers.Layer):\n",
    "    \"\"\" 임베딩 층\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, **kwargs):\n",
    "        super(Embedding, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def build(self, _):\n",
    "        self.embedding = self.add_variable(\n",
    "        \"embedding_kernel\",\n",
    "        shape=[self.vocab_size, self.embedding_dim],\n",
    "        dtype=tf.float32,\n",
    "        initializer = tf.random_uniform_initializer(-0.1, 0.1),\n",
    "        trainable=True)\n",
    "    \n",
    "    def call(self, x):\n",
    "        return tf.nn.embedding_lookup(self.embedding, x)\n",
    "    \n",
    "\n",
    "Embedding(50, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0.0086463 ]\n",
      "  [-0.95740545]\n",
      "  [-1.38855588]\n",
      "  [-0.24032409]\n",
      "  [-1.38855588]\n",
      "  [-0.95740545]\n",
      "  [ 0.0086463 ]]\n",
      "\n",
      " [[ 0.0086463 ]\n",
      "  [-0.95740545]\n",
      "  [-1.38855588]\n",
      "  [-0.95740545]\n",
      "  [-1.38855588]\n",
      "  [-0.95740545]\n",
      "  [-1.38855588]]], shape=(2, 7, 1), dtype=float32)\n",
      "(2, 7, 1) (2, 3, 1)\n",
      "tf.Tensor(\n",
      "[[[ 0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  1.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  1.]]], shape=(2, 5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# embedding\n",
    "input_embeds = tf.nn.embedding_lookup(embeddings, ids)\n",
    "print(input_embeds)\n",
    "\n",
    "input_flat = tf.layers.flatten(input_embeds)\n",
    "input_flat = tf.expand_dims(input_flat, -1)\n",
    "\n",
    "init_label = tf.nn.embedding_lookup(embeddings, init_label)\n",
    "init_label = tf.layers.flatten(init_label)\n",
    "init_label = tf.expand_dims(init_label, -1)\n",
    "\n",
    "print(input_flat.shape, init_label.shape)\n",
    "\n",
    "label_onehot = tf.one_hot(label_ids, depth=dict_size, dtype=tf.float32)\n",
    "print(label_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 7, 2)\n",
      "(2, 7, 1)\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "encoder_conv = tf.layers.conv1d(\n",
    "        inputs=input_flat,\n",
    "        filters=2*embed_size,\n",
    "        kernel_size=filter_size,\n",
    "        strides=embed_size,\n",
    "        padding='same')\n",
    "\n",
    "print(encoder_conv.shape)\n",
    "#embed_size, 반으로 나눈다.\n",
    "encoder_glu = encoder_conv[:, :, embed_size:]*tf.nn.sigmoid(encoder_conv[:, :, :embed_size])\n",
    "print(encoder_glu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 2)\n",
      "(2, 1, 1)\n",
      "(2, 7, 1)\n",
      "(2, 7, 1)\n"
     ]
    }
   ],
   "source": [
    "#decoder: 그림에서 왼쪽에서 하나의 블록을 구하는 식\n",
    "\n",
    "decoder_conv = tf.layers.conv1d(\n",
    "        inputs=init_label,\n",
    "        filters=2*embed_size,\n",
    "        kernel_size=filter_size,\n",
    "        strides=embed_size)\n",
    "\n",
    "decoder_glu = decoder_conv[:, :, embed_size:]*tf.nn.sigmoid(decoder_conv[:, :, :embed_size])\n",
    "\n",
    "print(decoder_conv.shape)\n",
    "print(decoder_glu.shape)\n",
    "\n",
    "tiled_decoder_glu = tf.tile(decoder_glu, [1, int(encoder_glu.shape[1]), 1]) #decoder 단어 1개, encoder는 7개단어)\n",
    "print(tiled_decoder_glu.shape)\n",
    "\n",
    "dot_prod = tf.matmul(encoder_glu, decoder_glu, transpose_b=True)\n",
    "print(dot_prod.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 7, 1)\n",
      "(2, 7, 1)\n",
      "(2, 7, 1)\n",
      "(2, 1)\n",
      "(2, 1)\n",
      "(2, 5)\n",
      "tf.Tensor(\n",
      "[[ 0.2683596   0.21093608  0.1373333   0.22239776  0.16097324]\n",
      " [ 0.3248516   0.21198468  0.09905788  0.2328326   0.13127331]], shape=(2, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "attention = tf.nn.softmax(dot_prod, axis=1) #수정해야함\n",
    "#tf.reduce_sum(attention, axis=1)\n",
    "print(attention.shape)\n",
    "\n",
    "z_plus_e = encoder_glu + input_embeds\n",
    "print(z_plus_e.shape)\n",
    "\n",
    "tiled_attention = tf.tile(attention, [1, 1, embed_size])\n",
    "print(tiled_attention.shape)\n",
    "\n",
    "c = tf.reduce_sum(tiled_attention*z_plus_e, axis=1)\n",
    "print(c.shape)\n",
    "\n",
    "decoder_glu = tf.reshape(decoder_glu, [-1, embed_size])\n",
    "print(decoder_glu.shape)\n",
    "\n",
    "logits = tf.layers.dense(c+decoder_glu, dict_size)\n",
    "print(logits.shape)\n",
    "\n",
    "out = tf.nn.softmax(logits)\n",
    "print(out)\n",
    "\n",
    "next_id = tf.argmax(out, axis=1)\n",
    "next_embeds = tf.nn.embedding_lookup(embeddings, next_id)\n",
    "next_embeds = tf.expand_dims(next_embeds, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'next_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-9a9d903347a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minit_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_embeds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnext_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'next_ids' is not defined"
     ]
    }
   ],
   "source": [
    "decoder_input = tf.concat([init_label[:, embed_size:], next_embeds], axis=1)\n",
    "\n",
    "next_ids.append(next_id)\n",
    "outs.append(logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
