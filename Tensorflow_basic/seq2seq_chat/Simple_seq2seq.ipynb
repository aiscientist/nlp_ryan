{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib import helpers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab\n",
    "\n",
    " - Chatbot에서는 X가 질의 Y가 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xt: [[5 6 3 1]\n",
      " [7 3 0 0]\n",
      " [8 0 0 0]]\n",
      "xlen: [3, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "x = [[5, 7, 8], [6, 3], [3], [1]] #input\n",
    "xt, xlen = helpers.batch(x) #helper함수를 이용하여, 0으로 패딩 및 배치 형태로 변환\n",
    "print('xt: {}'.format(xt))\n",
    "print('xlen: {}'.format(xlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#모델 초기화 및 텐서플로우 interactive Session\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model inputs and output\n",
    "\n",
    "중요한 것은 vocab_size이며, Dynamic RNN 모델은 재학습 전에 batch사이즈와 Seq 길이가 다른 것을 받아들이지만, Vocab_size가 달라지면 재학습\n",
    "\n",
    "입력과 출력에 대해서 잘 알면 끝\n",
    "\n",
    "- `encoder_inputs` int32 tensor is shaped `[encoder_max_time, batch_size]`\n",
    "- `decoder_targets` int32 tensor is shaped `[decoder_max_time, batch_size]`\n",
    "\n",
    "We'll add one additional placeholder tensor: \n",
    "- `decoder_inputs` int32 tensor is shaped `[decoder_max_time, batch_size]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "#UNK = 3\n",
    "\n",
    "vocab_size = 100\n",
    "input_embedding_size = 32\n",
    "\n",
    "encoder_hidden_units = 32\n",
    "decoder_hidden_units = encoder_hidden_units\n",
    "\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "\n",
    "#추가로 ..\n",
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all shapes are specified with `None`s (dynamic). We can use batches of any size with any number of timesteps. This is convenient and efficient, however but there are obvious constraints: \n",
    "- Feed values for all tensors should have same `batch_size`\n",
    "- Decoder inputs and ouputs (`decoder_inputs` and `decoder_targets`) should have same `decoder_max_time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embeddings\n",
    "\n",
    "encoder_inputs and decoder_inputs 둘 다 int32 tensor이며 [max_time, batch_size]의 형태로 되어 있음. <br>\n",
    "encoder와 decoder의 RNN은 dense vector represenation of word로 되어 있음 [max_time, batch_size, input_embedding_size] <br>\n",
    "따라서, 관련 건에 대해서 word embedding 형태로 변환해야함.\n",
    "\n",
    "1. embedding matrix 초기화 (초기화는 랜덤). 여기서는 단어의 백터표현을 인코더 - 디코더 형태의 End-to-End형식으로 표현\n",
    "2. tf.nn.embedding_lookup을 활용하여 인덱싱을 한다. 4개의 단어가 있다면, 4th 열의 임베딩 메트릭스로 표현된다. 이 접근 방식은 원 핫 방식보다 연산량이 적다. Encoder와 Decoder는 임베딩을 공유한다. 실제로는 단어의 수가 몇 천개, 몇 만개로 embedding matrix가 거대해 지기 때문에 어렵다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(100, 32) dtype=float32_ref>\n",
      "Tensor(\"embedding_lookup:0\", shape=(?, ?, 32), dtype=float32)\n",
      "Tensor(\"embedding_lookup_1:0\", shape=(?, ?, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)\n",
    "print(encoder_inputs_embedded)\n",
    "print(decoder_inputs_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoder\n",
    "\n",
    " - LSTM과 dynamic rnn을 활용하여 인코더를 만든다. encoder_ouputs은 seq2seq framework에는 사용할 필요가 없어 삭제한다.\n",
    " - encoder_final_state를 활용: 마지막에 encoder roll out 때, LSTM의 hidden cell state 필요 <br>\n",
    "    -> encoder_final_state는 또한, \"thought vector\"라고 불리우는데, Decoder의 initial state로 반영 됨 <br>\n",
    "    -> 이 모델은 simple 모델이므로, attention이 반영되어 있지 않기 때문에, 유일하게 Decoder로 가는 구간임. BPTT 진행 할 때 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"encoder\") as scope:\n",
    "    #encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units) #기본모델\n",
    "    encoder_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(encoder_hidden_units) #Layer Normalization 추가\n",
    "   \n",
    "    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "        encoder_cell, encoder_inputs_embedded,\n",
    "        dtype=tf.float32, time_major=True)\n",
    "    \n",
    "    del encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMStateTuple(c=<tf.Tensor 'encoder/rnn/while/Exit_2:0' shape=(?, 32) dtype=float32>, h=<tf.Tensor 'encoder/rnn/while/Exit_3:0' shape=(?, 32) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decoder\n",
    "\n",
    " - encoder_final_state가 initial_state의 decoder로 들어가므로, compatiable해야함. 같은 Cell (LSTMCell)과 같은 hidden_units (20), 같은 layers (single layers)가 맞아야함.\n",
    " - encoder에서는 cells output에 관심이 없지만, decoder output은 실제 결과 Seq를 구하기 위하여 분포를 구하는 것이라 필요함.\n",
    " - decoder_cell의 output은 hidden_units의 사이즈이지만, 학습과 예측을 위해서 vocab_size의 logits이 필요함.<br>\n",
    "   -> projection layer라고 임시로 이름을 붙이는 이 레이어는, LSTM 출력 최상단에 위치 해 있음 (linear layer, no activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder\") as scope:\n",
    "    #decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units) #Layernorm 추가\n",
    "    decoder_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(decoder_hidden_units) #Layernorm 추가\n",
    "    \n",
    "    decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "        decoder_cell, decoder_inputs_embedded,\n",
    "        initial_state = encoder_final_state,\n",
    "        dtype=tf.float32, time_major=True, scope='plain_decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.contrib.rnn.python.ops.rnn_cell.LayerNormBasicLSTMCell object at 0x11db8ac88>\n",
      "Tensor(\"decoder/plain_decoder/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, ?, 32), dtype=float32)\n",
      "LSTMStateTuple(c=<tf.Tensor 'decoder/plain_decoder/while/Exit_2:0' shape=(?, 32) dtype=float32>, h=<tf.Tensor 'decoder/plain_decoder/while/Exit_3:0' shape=(?, 32) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_cell)\n",
    "print(decoder_outputs)\n",
    "print(decoder_final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_logits = tf.contrib.layers.linear(decoder_outputs, vocab_size)\n",
    "\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fully_connected/BiasAdd:0\", shape=(?, ?, 100), dtype=float32)\n",
      "Tensor(\"ArgMax:0\", shape=(?, ?), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_logits)\n",
    "print(decoder_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimizer\n",
    "\n",
    " - RNN ouput tensor of shape [max_time, batch_size, hidden_units] 는 protection layer의 [max_time, batch_size, vocab_size]에 맵핑 됨. vocab_size는 static이고, max_time과 batch_size는 dynamic이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy, name=\"loss\")\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualize a scalar\n",
    "with tf.name_scope('loss'):\n",
    "    tf.summary.scalar('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_2:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"loss:0\", shape=(), dtype=float32)\n",
      "name: \"Adam\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam/update_Variable/group_deps\"\n",
      "input: \"^Adam/update_encoder/rnn/layer_norm_basic_lstm_cell/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_encoder/rnn/layer_norm_basic_lstm_cell/input/gamma/ApplyAdam\"\n",
      "input: \"^Adam/update_encoder/rnn/layer_norm_basic_lstm_cell/input/beta/ApplyAdam\"\n",
      "input: \"^Adam/update_encoder/rnn/layer_norm_basic_lstm_cell/transform/gamma/ApplyAdam\"\n",
      "input: \"^Adam/update_encoder/rnn/layer_norm_basic_lstm_cell/transform/beta/ApplyAdam\"\n",
      "input: \"^Adam/update_encoder/rnn/layer_norm_basic_lstm_cell/forget/gamma/ApplyAdam\"\n",
      "input: \"^Adam/update_encoder/rnn/layer_norm_basic_lstm_cell/forget/beta/ApplyAdam\"\n",
      "input: \"^Adam/update_encoder/rnn/layer_norm_basic_lstm_cell/output/gamma/ApplyAdam\"\n",
      "input: \"^Adam/update_encoder/rnn/layer_norm_basic_lstm_cell/output/beta/ApplyAdam\"\n",
      "input: \"^Adam/update_encoder/rnn/layer_norm_basic_lstm_cell/state/gamma/ApplyAdam\"\n",
      "input: \"^Adam/update_encoder/rnn/layer_norm_basic_lstm_cell/state/beta/ApplyAdam\"\n",
      "input: \"^Adam/update_decoder/plain_decoder/layer_norm_basic_lstm_cell/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_decoder/plain_decoder/layer_norm_basic_lstm_cell/input/gamma/ApplyAdam\"\n",
      "input: \"^Adam/update_decoder/plain_decoder/layer_norm_basic_lstm_cell/input/beta/ApplyAdam\"\n",
      "input: \"^Adam/update_decoder/plain_decoder/layer_norm_basic_lstm_cell/transform/gamma/ApplyAdam\"\n",
      "input: \"^Adam/update_decoder/plain_decoder/layer_norm_basic_lstm_cell/transform/beta/ApplyAdam\"\n",
      "input: \"^Adam/update_decoder/plain_decoder/layer_norm_basic_lstm_cell/forget/gamma/ApplyAdam\"\n",
      "input: \"^Adam/update_decoder/plain_decoder/layer_norm_basic_lstm_cell/forget/beta/ApplyAdam\"\n",
      "input: \"^Adam/update_decoder/plain_decoder/layer_norm_basic_lstm_cell/output/gamma/ApplyAdam\"\n",
      "input: \"^Adam/update_decoder/plain_decoder/layer_norm_basic_lstm_cell/output/beta/ApplyAdam\"\n",
      "input: \"^Adam/update_decoder/plain_decoder/layer_norm_basic_lstm_cell/state/gamma/ApplyAdam\"\n",
      "input: \"^Adam/update_decoder/plain_decoder/layer_norm_basic_lstm_cell/state/beta/ApplyAdam\"\n",
      "input: \"^Adam/update_fully_connected/weights/ApplyAdam\"\n",
      "input: \"^Adam/update_fully_connected/biases/ApplyAdam\"\n",
      "input: \"^Adam/Assign\"\n",
      "input: \"^Adam/Assign_1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(stepwise_cross_entropy)\n",
    "print(loss)\n",
    "print(train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tensorboard\n",
    "summary_op = tf.summary.merge_all() #operation 전부 융합하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test foward pass\n",
    "\n",
    " - Shape가 중요함. Static shape가 맞지 않으면 에러로 나옴. 하지만 dynamic shapes로 지정하였기 때문에, 실행 해보면서 체크 해야한다.\n",
    "\n",
    "this is key part where everything comes together\n",
    "\n",
    "@TODO: describe\n",
    "- how encoder shape is fixed to max\n",
    "- how decoder shape is arbitraty and determined by inputs, but should probably be longer then encoder's\n",
    "- how decoder input values are also arbitraty, and how we use GO token, and what are those 0s, and what can be used instead (shifted gold sequence, beam search)\n",
    "\n",
    "@TODO: add references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_encoded:\n",
      "[[6 3 9]\n",
      " [0 4 8]\n",
      " [0 0 7]]\n",
      "decoder inputs:\n",
      "[[1 1 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "decoder predictions:\n",
      "[[99 28 27]\n",
      " [46 81 95]\n",
      " [46 81 41]\n",
      " [46 81 41]]\n"
     ]
    }
   ],
   "source": [
    "batch_ = [[6], [3, 4], [9, 8, 7]]\n",
    "\n",
    "batch_, batch_length_ = helpers.batch(batch_)\n",
    "print('batch_encoded:\\n' + str(batch_))\n",
    "\n",
    "din_, dlen_ = helpers.batch(np.ones(shape=(3, 1), dtype=np.int32),\n",
    "                            max_sequence_length=4)\n",
    "print('decoder inputs:\\n' + str(din_))\n",
    "\n",
    "pred_ = sess.run(decoder_prediction,\n",
    "    feed_dict={\n",
    "        encoder_inputs: batch_,\n",
    "        decoder_inputs: din_,\n",
    "    })\n",
    "print('decoder predictions:\\n' + str(pred_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on the toy task\n",
    "\n",
    " - 입력 순서를 기억하고 재생산하도록 학습시킬 것이다. 현재는 어떤 패턴도 가지고 있지 않기 때문에, 그냥 인코딩 및 디코딩만 진행 할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[26, 16, 44, 33, 3]\n",
      "[29, 44, 7, 2, 56, 8, 19, 2]\n",
      "[22, 35, 25]\n",
      "[52, 50, 40, 47, 11]\n",
      "[47, 43, 57, 18, 35]\n",
      "[16, 24, 7, 41, 44, 58, 21, 50]\n",
      "[6, 35, 6, 22]\n",
      "[22, 55, 23, 36, 15, 4, 3, 36]\n",
      "[10, 58, 14, 51, 36, 53, 38]\n",
      "[60, 47, 38]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=64,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_feed():\n",
    "    batch = next(batches)\n",
    "    encoder_inputs_, _ = helpers.batch(batch)\n",
    "    decoder_targets_, _ = helpers.batch(\n",
    "        [(sequence) + [EOS] for sequence in batch]\n",
    "    )\n",
    "    decoder_inputs_, _ = helpers.batch(\n",
    "        [[EOS] + (sequence) for sequence in batch]\n",
    "    )\n",
    "    return {\n",
    "        encoder_inputs: encoder_inputs_,\n",
    "        decoder_inputs: decoder_inputs_,\n",
    "        decoder_targets: decoder_targets_,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 4.671084403991699\n",
      "  sample 1:\n",
      "    input     > [48 16 19 46  0  0  0  0]\n",
      "    predicted > [70 88 88 88 88 46 84 40 40]\n",
      "  sample 2:\n",
      "    input     > [24 58  5 14  0  0  0  0]\n",
      "    predicted > [99 46 46 46 46 46 46 46 46]\n",
      "  sample 3:\n",
      "    input     > [34 15 58 12 63 32  0  0]\n",
      "    predicted > [ 0 82 54  9 54 65 21 46 41]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 1.1468178033828735\n",
      "  sample 1:\n",
      "    input     > [33  3 53 48 51  2 63 18]\n",
      "    predicted > [33  3 48 51 18 18 18 18  1]\n",
      "  sample 2:\n",
      "    input     > [ 3 42  4  5 11 33  0  0]\n",
      "    predicted > [ 3  5 44  5 11 33  1  0  0]\n",
      "  sample 3:\n",
      "    input     > [38 38  4  0  0  0  0  0]\n",
      "    predicted > [38 38  4  1  0  0  0  0  0]\n",
      "\n",
      "batch 2000\n",
      "  minibatch loss: 0.36455798149108887\n",
      "  sample 1:\n",
      "    input     > [18 17 62  9 61 39  0  0]\n",
      "    predicted > [18 17 62  9 61 39  1  0  0]\n",
      "  sample 2:\n",
      "    input     > [43 63 22  0  0  0  0  0]\n",
      "    predicted > [43 63 22  1  0  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [58 18 41  0  0  0  0  0]\n",
      "    predicted > [58 18 41  1  0  0  0  0  0]\n",
      "\n",
      "batch 3000\n",
      "  minibatch loss: 0.21348582208156586\n",
      "  sample 1:\n",
      "    input     > [30 46 35 39 63 11 46  0]\n",
      "    predicted > [30 46 35 39 63 11 46  1  0]\n",
      "  sample 2:\n",
      "    input     > [18 16 26 38  0  0  0  0]\n",
      "    predicted > [18 16 26 38  1  0  0  0  0]\n",
      "  sample 3:\n",
      "    input     > [50 18 10 15 34 54 52 28]\n",
      "    predicted > [50 18 10 15 34 52 52 28  1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        #_, l, summary = sess.run([train_op, loss, summary_op], fd)\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        \n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "        \n",
    "        #Tensorboard\n",
    "        #log_writer = tf.summary.FileWriter('tensorboard')\n",
    "        #log_writer.add_summary(summary, batch)\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layernorm 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.2658 after 300100 examples (batch_size=100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXOb/AwK/lr6jNLZnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125da9fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.summarytf.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 간단한 Simple seq2seq의 한계\n",
    "\n",
    "We have no control over transitions of `tf.nn.dynamic_rnn`, it is unrolled in a single sweep. Some of the things that are not possible without such control:\n",
    "\n",
    "- We can't feed previously generated tokens without falling back to Python loops. This means *we cannot make efficient inference with dynamic_rnn decoder*!\n",
    "\n",
    "- We can't use attention, because attention conditions decoder inputs on its previous state\n",
    "\n",
    "Solution would be to use `tf.nn.raw_rnn` instead of `tf.nn.dynamic_rnn` for decoder, as we will do in tutorial #2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
