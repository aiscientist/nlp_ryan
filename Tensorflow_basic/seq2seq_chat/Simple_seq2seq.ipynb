{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib import helpers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab\n",
    "\n",
    " - Chatbot에서는 X가 질의 Y가 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xt: [[5 6 3 1]\n",
      " [7 3 0 0]\n",
      " [8 0 0 0]]\n",
      "xlen: [3, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "x = [[5, 7, 8], [6, 3], [3], [1]] #input\n",
    "xt, xlen = helpers.batch(x) #helper함수를 이용하여, 0으로 패딩 및 배치 형태로 변환\n",
    "print('xt: {}'.format(xt))\n",
    "print('xlen: {}'.format(xlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#모델 초기화 및 텐서플로우 interactive Session\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model inputs and output\n",
    "\n",
    "중요한 것은 vocab_size이며, Dynamic RNN 모델은 재학습 전에 batch사이즈와 Seq 길이가 다른 것을 받아들이지만, Vocab_size가 달라지면 재학습\n",
    "\n",
    "입력과 출력에 대해서 잘 알면 끝\n",
    "\n",
    "- `encoder_inputs` int32 tensor is shaped `[encoder_max_time, batch_size]`\n",
    "- `decoder_targets` int32 tensor is shaped `[decoder_max_time, batch_size]`\n",
    "\n",
    "We'll add one additional placeholder tensor: \n",
    "- `decoder_inputs` int32 tensor is shaped `[decoder_max_time, batch_size]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 2000\n",
    "input_embedding_size = 64\n",
    "\n",
    "encoder_hidden_units = 64\n",
    "decoder_hidden_units = encoder_hidden_units\n",
    "\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "\n",
    "#추가로 ..\n",
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all shapes are specified with `None`s (dynamic). We can use batches of any size with any number of timesteps. This is convenient and efficient, however but there are obvious constraints: \n",
    "- Feed values for all tensors should have same `batch_size`\n",
    "- Decoder inputs and ouputs (`decoder_inputs` and `decoder_targets`) should have same `decoder_max_time`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embeddings\n",
    "\n",
    "encoder_inputs and decoder_inputs 둘 다 int32 tensor이며 [max_time, batch_size]의 형태로 되어 있음. <br>\n",
    "encoder와 decoder의 RNN은 dense vector represenation of word로 되어 있음 [max_time, batch_size, input_embedding_size] <br>\n",
    "따라서, 관련 건에 대해서 word embedding 형태로 변환해야함.\n",
    "\n",
    "1. embedding matrix 초기화 (초기화는 랜덤). 여기서는 단어의 백터표현을 인코더 - 디코더 형태의 End-to-End형식으로 표현\n",
    "2. tf.nn.embedding_lookup을 활용하여 인덱싱을 한다. 4개의 단어가 있다면, 4th 열의 임베딩 메트릭스로 표현된다. 이 접근 방식은 원 핫 방식보다 연산량이 적다. Encoder와 Decoder는 임베딩을 공유한다. 실제로는 단어의 수가 몇 천개, 몇 만개로 embedding matrix가 거대해 지기 때문에 어렵다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2000, 64) dtype=float32_ref>\n",
      "Tensor(\"embedding_lookup:0\", shape=(?, ?, 64), dtype=float32)\n",
      "Tensor(\"embedding_lookup_1:0\", shape=(?, ?, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)\n",
    "print(encoder_inputs_embedded)\n",
    "print(decoder_inputs_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoder\n",
    "\n",
    " - LSTM과 dynamic rnn을 활용하여 인코더를 만든다. encoder_ouputs은 seq2seq framework에는 사용할 필요가 없어 삭제한다.\n",
    " - encoder_final_state를 활용: 마지막에 encoder roll out 때, LSTM의 hidden cell state 필요 <br>\n",
    "    -> encoder_final_state는 또한, \"thought vector\"라고 불리우는데, Decoder의 initial state로 반영 됨 <br>\n",
    "    -> 이 모델은 simple 모델이므로, attention이 반영되어 있지 않기 때문에, 유일하게 Decoder로 가는 구간임. BPTT 진행 할 때 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"encoder\") as scope:\n",
    "    encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "    \n",
    "    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
    "        encoder_cell, encoder_inputs_embedded,\n",
    "        dtype=tf.float32, time_major=True)\n",
    "    \n",
    "    del encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMStateTuple(c=<tf.Tensor 'encoder/rnn/while/Exit_2:0' shape=(?, 64) dtype=float32>, h=<tf.Tensor 'encoder/rnn/while/Exit_3:0' shape=(?, 64) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decoder\n",
    "\n",
    " - encoder_final_state가 initial_state의 decoder로 들어가므로, compatiable해야함. 같은 Cell (LSTMCell)과 같은 hidden_units (20), 같은 layers (single layers)가 맞아야함.\n",
    " - encoder에서는 cells output에 관심이 없지만, decoder output은 실제 결과 Seq를 구하기 위하여 분포를 구하는 것이라 필요함.\n",
    " - decoder_cell의 output은 hidden_units의 사이즈이지만, 학습과 예측을 위해서 vocab_size의 logits이 필요함.<br>\n",
    "   -> projection layer라고 임시로 이름을 붙이는 이 레이어는, LSTM 출력 최상단에 위치 해 있음 (linear layer, no activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"decoder\") as scope:\n",
    "    decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "    \n",
    "    decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "        decoder_cell, decoder_inputs_embedded,\n",
    "        initial_state = encoder_final_state,\n",
    "        dtype=tf.float32, time_major=True, scope='plain_decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x11dba0d30>\n",
      "Tensor(\"decoder/plain_decoder/TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, ?, 64), dtype=float32)\n",
      "LSTMStateTuple(c=<tf.Tensor 'decoder/plain_decoder/while/Exit_2:0' shape=(?, 64) dtype=float32>, h=<tf.Tensor 'decoder/plain_decoder/while/Exit_3:0' shape=(?, 64) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_cell)\n",
    "print(decoder_outputs)\n",
    "print(decoder_final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_logits = tf.contrib.layers.linear(decoder_outputs, vocab_size)\n",
    "\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fully_connected/BiasAdd:0\", shape=(?, ?, 2000), dtype=float32)\n",
      "Tensor(\"ArgMax:0\", shape=(?, ?), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_logits)\n",
    "print(decoder_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimizer\n",
    "\n",
    " - RNN ouput tensor of shape [max_time, batch_size, hidden_units] 는 protection layer의 [max_time, batch_size, vocab_size]에 맵핑 됨. vocab_size는 static이고, max_time과 batch_size는 dynamic이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "    logits=decoder_logits,\n",
    ")\n",
    "\n",
    "loss = tf.reduce_mean(stepwise_cross_entropy, name=\"loss\")\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#visualize a scalar\n",
    "with tf.name_scope('loss'):\n",
    "    tf.summary.scalar('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_2:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"loss:0\", shape=(), dtype=float32)\n",
      "name: \"Adam\"\n",
      "op: \"NoOp\"\n",
      "input: \"^Adam/update_Variable/group_deps\"\n",
      "input: \"^Adam/update_encoder/rnn/lstm_cell/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_encoder/rnn/lstm_cell/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_decoder/plain_decoder/lstm_cell/kernel/ApplyAdam\"\n",
      "input: \"^Adam/update_decoder/plain_decoder/lstm_cell/bias/ApplyAdam\"\n",
      "input: \"^Adam/update_fully_connected/weights/ApplyAdam\"\n",
      "input: \"^Adam/update_fully_connected/biases/ApplyAdam\"\n",
      "input: \"^Adam/Assign\"\n",
      "input: \"^Adam/Assign_1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(stepwise_cross_entropy)\n",
    "print(loss)\n",
    "print(train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tensorboard\n",
    "summary_op = tf.summary.merge_all() #operation 전부 융합하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test foward pass\n",
    "\n",
    " - Shape가 중요함. Static shape가 맞지 않으면 에러로 나옴. 하지만 dynamic shapes로 지정하였기 때문에, 실행 해보면서 체크 해야한다.\n",
    "\n",
    "this is key part where everything comes together\n",
    "\n",
    "@TODO: describe\n",
    "- how encoder shape is fixed to max\n",
    "- how decoder shape is arbitraty and determined by inputs, but should probably be longer then encoder's\n",
    "- how decoder input values are also arbitraty, and how we use GO token, and what are those 0s, and what can be used instead (shifted gold sequence, beam search)\n",
    "\n",
    "@TODO: add references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_encoded:\n",
      "[[6 3 9]\n",
      " [0 4 8]\n",
      " [0 0 7]]\n",
      "decoder inputs:\n",
      "[[1 1 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "decoder predictions:\n",
      "[[1768 1162 1281]\n",
      " [ 686  757 1225]\n",
      " [ 757  757 1225]\n",
      " [1704  757 1225]]\n"
     ]
    }
   ],
   "source": [
    "batch_ = [[6], [3, 4], [9, 8, 7]]\n",
    "\n",
    "batch_, batch_length_ = helpers.batch(batch_)\n",
    "print('batch_encoded:\\n' + str(batch_))\n",
    "\n",
    "din_, dlen_ = helpers.batch(np.ones(shape=(3, 1), dtype=np.int32),\n",
    "                            max_sequence_length=4)\n",
    "print('decoder inputs:\\n' + str(din_))\n",
    "\n",
    "pred_ = sess.run(decoder_prediction,\n",
    "    feed_dict={\n",
    "        encoder_inputs: batch_,\n",
    "        decoder_inputs: din_,\n",
    "    })\n",
    "print('decoder predictions:\\n' + str(pred_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on the toy task\n",
    "\n",
    " - 입력 순서를 기억하고 재생산하도록 학습시킬 것이다. 현재는 어떤 패턴도 가지고 있지 않기 때문에, 그냥 인코딩 및 디코딩만 진행 할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of the batch:\n",
      "[41, 11, 4, 55, 40]\n",
      "[2, 2, 58, 31, 33, 46, 52]\n",
      "[35, 47, 27, 55, 58, 52, 47, 9]\n",
      "[46, 62, 2, 38]\n",
      "[11, 59, 24, 13, 46, 29]\n",
      "[50, 21, 21, 56, 52, 40]\n",
      "[15, 49, 40, 22, 28]\n",
      "[62, 55, 61, 35]\n",
      "[18, 3, 60, 28, 17, 10, 15, 38]\n",
      "[32, 26, 38, 52, 37, 48, 44, 44]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "batches = helpers.random_sequences(length_from=3, length_to=8,\n",
    "                                   vocab_lower=2, vocab_upper=64,\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "print('head of the batch:')\n",
    "for seq in next(batches)[:10]:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "  minibatch loss: 7.56494665145874\n",
      "  sample 1:\n",
      "    input     > [55 16  7 47  0  0  0  0]\n",
      "    predicted > [ 686  686  627 1460 1768  339  179  179 1704]\n",
      "  sample 2:\n",
      "    input     > [29  7 46 20 44 27 47  0]\n",
      "    predicted > [1669 1669  969 1366 1366 1669 1852 1278 1016]\n",
      "  sample 3:\n",
      "    input     > [63 22 48 44 62 59  0  0]\n",
      "    predicted > [1669  105   43  646 1946  722  402 1016 1704]\n",
      "\n",
      "batch 1000\n",
      "  minibatch loss: 2.2900331020355225\n",
      "  sample 1:\n",
      "    input     > [41 18 42  0  0  0  0  0]\n",
      "    predicted > [41 41  1  1  0  0  0  0  0]\n",
      "  sample 2:\n",
      "    input     > [25  5 38 54  5 44 40  0]\n",
      "    predicted > [25 25 25 25 25 25 60  1  0]\n",
      "  sample 3:\n",
      "    input     > [19 38 41  0  0  0  0  0]\n",
      "    predicted > [41 41 41  1  0  0  0  0  0]\n",
      "\n",
      "training interrupted\n"
     ]
    }
   ],
   "source": [
    "max_batches = 3001\n",
    "batches_in_epoch = 1000\n",
    "\n",
    "try:\n",
    "    for batch in range(max_batches):\n",
    "        fd = next_feed()\n",
    "        #_, l, summary = sess.run([train_op, loss, summary_op], fd)\n",
    "        _, l = sess.run([train_op, loss], fd)\n",
    "        loss_track.append(l)\n",
    "        \n",
    "        if batch == 0 or batch % batches_in_epoch == 0:\n",
    "            print('batch {}'.format(batch))\n",
    "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
    "            predict_ = sess.run(decoder_prediction, fd)\n",
    "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
    "                print('  sample {}:'.format(i + 1))\n",
    "                print('    input     > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 2:\n",
    "                    break\n",
    "            print()\n",
    "        \n",
    "        #Tensorboard\n",
    "        #log_writer = tf.summary.FileWriter('tensorboard')\n",
    "        #log_writer.add_summary(summary, batch)\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('training interrupted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_track)\n",
    "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.summarytf.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 간단한 Simple seq2seq의 한계\n",
    "\n",
    "We have no control over transitions of `tf.nn.dynamic_rnn`, it is unrolled in a single sweep. Some of the things that are not possible without such control:\n",
    "\n",
    "- We can't feed previously generated tokens without falling back to Python loops. This means *we cannot make efficient inference with dynamic_rnn decoder*!\n",
    "\n",
    "- We can't use attention, because attention conditions decoder inputs on its previous state\n",
    "\n",
    "Solution would be to use `tf.nn.raw_rnn` instead of `tf.nn.dynamic_rnn` for decoder, as we will do in tutorial #2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
