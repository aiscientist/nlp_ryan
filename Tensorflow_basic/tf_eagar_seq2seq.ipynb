{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기본 값\n",
    "df_path = './dataset'\n",
    "buckets = [(12, 40)]\n",
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "user_vocab_size = 9687\n",
    "bot_vocab_size = 12169"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Dataset\n",
    "\n",
    "Data from cornell dialogue: https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "lines = open(df_path + '/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conv_lines = open(df_path + '/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map each line's id with its text\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]\n",
    "        \n",
    "# Create a list of all of the conversations' lines' ids.\n",
    "convs = [ ]\n",
    "for line in conv_lines[:-1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))\n",
    "    \n",
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "You're asking me out.  That's so cute. What's your name again?\n",
      "Forget it.\n",
      "\n",
      "No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "Cameron.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if we have loaded the data correctly\n",
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(questions[i])\n",
    "    print(answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221616\n",
      "221616\n"
     ]
    }
   ],
   "source": [
    "# Compare lengths of questions and answers\n",
    "print(len(questions))\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Clean the data\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again\n",
      "well i thought we would start with pronunciation if that is okay with you\n",
      "\n",
      "well i thought we would start with pronunciation if that is okay with you\n",
      "not the hacking and gagging and spitting part  please\n",
      "\n",
      "not the hacking and gagging and spitting part  please\n",
      "okay then how about we try out some french cuisine  saturday  night\n",
      "\n",
      "you are asking me out  that is so cute that is your name again\n",
      "forget it\n",
      "\n",
      "no no it is my fault  we did not have a proper introduction \n",
      "cameron\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at some of the data to ensure that it has been cleaned well.\n",
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(clean_questions[i])\n",
    "    print(clean_answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the length of sentences\n",
    "lengths = []\n",
    "for question in clean_questions:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in clean_answers:\n",
    "    lengths.append(len(answer.split()))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Dataset\n",
    "\n",
    "- Datset API사용 (큐 기반)\n",
    "- Clear Structure and Simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, \n",
    "                                [None, None], \n",
    "                                name='input')\n",
    "    \n",
    "    targets = tf.placeholder(tf.int32, \n",
    "                             [None, None], \n",
    "                             name='targets')\n",
    "    \n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return input_data, targets, lr, keep_prob\n",
    "\n",
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, \n",
    "                              [0, 0], \n",
    "                              [batch_size, -1], \n",
    "                              [1, 1]) #마지막에 있는 단어 토큰을 제거함\n",
    "    \n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], \n",
    "                                   vocab_to_int['<GO>']), \n",
    "                           ending], 1)\n",
    "    return dec_input\n",
    "\n",
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   sequence_length, attn_length):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    \n",
    "    drop = tf.contrib.rnn.DropoutWrapper(\n",
    "               lstm, \n",
    "               input_keep_prob = keep_prob)\n",
    "    \n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    \n",
    "    _, enc_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "               cell_fw = enc_cell,\n",
    "               cell_bw = enc_cell,\n",
    "               sequence_length = sequence_length,\n",
    "               inputs = rnn_inputs, \n",
    "               dtype=tf.float32)\n",
    "    return enc_state\n",
    "\n",
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         sequence_length, decoding_scope,\n",
    "                         output_fn, keep_prob, batch_size):\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, \n",
    "                                1, \n",
    "                                dec_cell.output_size])\n",
    "    \n",
    "    att_keys, att_vals, att_score_fn, att_construct_fn = \\\n",
    "        tf.contrib.seq2seq.prepare_attention(\n",
    "             attention_states,\n",
    "             attention_option=\"bahdanau\",\n",
    "             num_units=dec_cell.output_size)\n",
    "    \n",
    "    train_decoder_fn = \\  \n",
    "       tf.contrib.seq2seq.attention_decoder_fn_train(\n",
    "             encoder_state[0],\n",
    "             att_keys,\n",
    "             att_vals,\n",
    "             att_score_fn,\n",
    "             att_construct_fn,\n",
    "             name = \"attn_dec_train\")\n",
    "    \n",
    "    train_pred, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(\n",
    "             dec_cell, \n",
    "             train_decoder_fn, \n",
    "             dec_embed_input, \n",
    "             sequence_length, \n",
    "             scope=decoding_scope)\n",
    "    \n",
    "    train_pred_drop = tf.nn.dropout(train_pred, keep_prob)\n",
    "    \n",
    "    return output_fn(train_pred_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data line 1000\n",
      "read data line 2000\n",
      "read data line 3000\n",
      "read data line 4000\n",
      "read data line 5000\n",
      "sep by train, [0, 0, 20, 12, 35, 91, 180, 236, 229, 144, 76, 3977]\n",
      "train_size: 5000.0\n"
     ]
    }
   ],
   "source": [
    "#Load Sample Dataset\n",
    "\n",
    "def read_train_set(train_path, max_size=None):\n",
    "    train_set = [[] for _ in range(buckets[0][0])]\n",
    "    \n",
    "    with open(train_path, \"r\", encoding=\"utf-8\") as ts:\n",
    "        counter = 0\n",
    "        while not max_size or counter < max_size:\n",
    "            \n",
    "            # iterator 성질을 이용하여, 유저, 봇 발화 불러오기\n",
    "            user_tokens, bot_tokens = ts.readline(), ts.readline()\n",
    "            \n",
    "            # 유저 발화에서 모르는 토큰이 있으면 언노운 아이디 부여한다\n",
    "            user_token_ids = [int(x) for x in user_tokens.split()]\n",
    "\n",
    "            # 모르는 토큰이 있으면 언노운 아이디 부여\n",
    "            for i, user_token_id in enumerate(user_token_ids):\n",
    "                if user_token_id > user_vocab_size:\n",
    "                    user_token_ids[i] = UNK_ID\n",
    "\n",
    "            # 유저 버킷 크기로 잘라낸다\n",
    "            user_token_bucket = user_token_ids[:buckets[0][0]]\n",
    "            user_token_len = len(user_token_bucket)\n",
    "\n",
    "            # 봇 발화에서 모르는 토큰이 있으면 언노운 아이디 부여한다\n",
    "            bot_token_ids = [int(x) for x in bot_tokens.split()]\n",
    "            for i, bot_token_id in enumerate(bot_token_ids):\n",
    "                if bot_token_id > bot_vocab_size:\n",
    "                    bot_token_ids[i] = UNK_ID\n",
    "\n",
    "            # 봇 버킷 크기로 잘라낸다\n",
    "            bot_token_bucket = bot_token_ids[:(buckets[0][1]-1)]\n",
    "\n",
    "            # 유저 토큰 길이에 따라 유저 발화와 봇 발화를 모은다\n",
    "            train_set[user_token_len - 1].append([user_token_bucket, bot_token_bucket])\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            # 진행도를 알아볼 수 있도록 1000번마다 프린트한다\n",
    "            if counter % 1000 == 0:\n",
    "                print(\"read data line %d\" % counter)\n",
    "\n",
    "        return train_set\n",
    "    \n",
    "train_set = read_train_set(df_path, 5000)\n",
    "train_len_hist = [len(train_set[i]) for i in range(len(train_set))]\n",
    "train_size = float(sum(train_len_hist))\n",
    "\n",
    "print(\"sep by train, {}\".format(train_len_hist))\n",
    "print(\"train_size: {}\".format(train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_set[4][0][0] #고객 발화\n",
    "# train_set[4][0][1] #봇 발화\n",
    "\n",
    "df_train = [train_set[4][i] for i in range(len(train_set[4]))] #Sample로 input size 가 5인 발화를 추출한다.\n",
    "df_user = [df_train[i][0] for i in range(len(df_train))]\n",
    "df_bot = [df_train[i][1] for i in range(len(df_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-08f0de7b823b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfilter_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "embed_size = 1\n",
    "window_size = 3\n",
    "dict_size = 5\n",
    "filter_size = window_size*embed_size\n",
    "\n",
    "ids = tf.constant(df_train)\n",
    "\n",
    "#https://tutorials.botsfloor.com/how-to-build-your-first-chatbot-c84495d4622d\n",
    "\n",
    "\n",
    "\n",
    "def input_fn():\n",
    "    ids = tf.constant([[5, 6, 10, 9, 2], [5, 6, 10, 60, 2]])\n",
    "    label_ids = tf.constant([[4, 22, 163, 649, 2], [4, 22, 163, 649, 2]])\n",
    "    return {'ids': ids,\n",
    "            'init_label': label_ids[:, :window_size],\n",
    "            'label_length': int(label_ids.shape[1])-window_size\n",
    "            }, label_ids[:, window_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 0 0]\n",
      " [0 0 0]], shape=(2, 3), dtype=int32)\n",
      "5\n",
      "tf.Tensor(\n",
      "[[1 3 1 3 2]\n",
      " [2 1 3 4 4]], shape=(2, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "ids = tf.constant([[1, 2, 3, 4, 3, 2, 1], [1, 2, 3, 2, 3, 2, 3]])\n",
    "label_ids = tf.constant([[0, 0, 0, 1, 3, 1, 3, 2], [0, 0, 0, 2, 1, 3, 4, 4]]) #무조건 앞에 3개를... \n",
    "init_label = label_ids[:, :window_size]\n",
    "label_length = int(label_ids.shape[1]) - window_size\n",
    "label_ids = label_ids[:, window_size:]\n",
    "\n",
    "print(init_label)\n",
    "print(label_length)\n",
    "print(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10)\n"
     ]
    }
   ],
   "source": [
    "embeddings = tfe.Variable(tf.truncated_normal([dict_size, embed_size]), trainable=False)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Embedding at 0x11d179198>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(tf.layers.Layer):\n",
    "    \"\"\" 임베딩 층\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, **kwargs):\n",
    "        super(Embedding, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def build(self, _):\n",
    "        self.embedding = self.add_variable(\n",
    "        \"embedding_kernel\",\n",
    "        shape=[self.vocab_size, self.embedding_dim],\n",
    "        dtype=tf.float32,\n",
    "        initializer = tf.random_uniform_initializer(-0.1, 0.1),\n",
    "        trainable=True)\n",
    "    \n",
    "    def call(self, x):\n",
    "        return tf.nn.embedding_lookup(self.embedding, x)\n",
    "    \n",
    "\n",
    "Embedding(50, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.44031993  0.27558795  1.88564384 -0.26049265  1.04315972 -0.8651666\n",
      "    0.45365837  0.21361713  0.36355501  1.71674705]\n",
      "  [ 0.00447189 -0.20288323 -0.78719151  0.80474299 -0.97947961  0.43934852\n",
      "   -0.00413364  0.87714851 -0.58071738  0.23394325]\n",
      "  [-1.54866028  0.79748154  0.61276042  0.14294747  1.46765685  1.49111998\n",
      "   -1.82410061  0.7496416   0.30667016  1.82444489]\n",
      "  [-1.24891686  1.56246781 -1.05082929 -0.79415643 -0.33950487 -1.42375171\n",
      "   -0.82772219  0.00675092 -0.51040632  1.12121177]\n",
      "  [-1.54866028  0.79748154  0.61276042  0.14294747  1.46765685  1.49111998\n",
      "   -1.82410061  0.7496416   0.30667016  1.82444489]\n",
      "  [ 0.00447189 -0.20288323 -0.78719151  0.80474299 -0.97947961  0.43934852\n",
      "   -0.00413364  0.87714851 -0.58071738  0.23394325]\n",
      "  [-0.44031993  0.27558795  1.88564384 -0.26049265  1.04315972 -0.8651666\n",
      "    0.45365837  0.21361713  0.36355501  1.71674705]]\n",
      "\n",
      " [[-0.44031993  0.27558795  1.88564384 -0.26049265  1.04315972 -0.8651666\n",
      "    0.45365837  0.21361713  0.36355501  1.71674705]\n",
      "  [ 0.00447189 -0.20288323 -0.78719151  0.80474299 -0.97947961  0.43934852\n",
      "   -0.00413364  0.87714851 -0.58071738  0.23394325]\n",
      "  [-1.54866028  0.79748154  0.61276042  0.14294747  1.46765685  1.49111998\n",
      "   -1.82410061  0.7496416   0.30667016  1.82444489]\n",
      "  [ 0.00447189 -0.20288323 -0.78719151  0.80474299 -0.97947961  0.43934852\n",
      "   -0.00413364  0.87714851 -0.58071738  0.23394325]\n",
      "  [-1.54866028  0.79748154  0.61276042  0.14294747  1.46765685  1.49111998\n",
      "   -1.82410061  0.7496416   0.30667016  1.82444489]\n",
      "  [ 0.00447189 -0.20288323 -0.78719151  0.80474299 -0.97947961  0.43934852\n",
      "   -0.00413364  0.87714851 -0.58071738  0.23394325]\n",
      "  [-1.54866028  0.79748154  0.61276042  0.14294747  1.46765685  1.49111998\n",
      "   -1.82410061  0.7496416   0.30667016  1.82444489]]], shape=(2, 7, 10), dtype=float32)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Value for attr 'Tindices' of float is not in the list of allowed values: int32, int64\n\t; NodeDef: ResourceGather = ResourceGather[Tindices=DT_FLOAT, dtype=DT_FLOAT, validate_indices=true](dummy_input, dummy_input); Op<name=ResourceGather; signature=resource:resource, indices:Tindices -> output:dtype; attr=validate_indices:bool,default=true; attr=dtype:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true> [Op:ResourceGather] name: embedding_lookup/embedding_lookup//",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-13b5d4c8ddce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minput_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0minit_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0minit_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0minit_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[0;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[1;32m    323\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m       transform_fn=None)\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[0;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mtransform_fn\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(params, ids, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \"\"\"\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36msparse_read\u001b[0;34m(self, indices, name)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m       value = gen_resource_variable_ops.resource_gather(\n\u001b[0;32m--> 690\u001b[0;31m           self._handle, indices, dtype=self._dtype, name=name)\n\u001b[0m\u001b[1;32m    691\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mresource_gather\u001b[0;34m(resource, indices, dtype, validate_indices, name)\u001b[0m\n\u001b[1;32m    242\u001b[0m               \"Tindices\", _attr_Tindices)\n\u001b[1;32m    243\u001b[0m     _result = _execute.execute(b\"ResourceGather\", 1, inputs=_inputs_flat,\n\u001b[0;32m--> 244\u001b[0;31m                                attrs=_attrs, ctx=_ctx, name=name)\n\u001b[0m\u001b[1;32m    245\u001b[0m   _execute.record_gradient(\n\u001b[1;32m    246\u001b[0m       \"ResourceGather\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Value for attr 'Tindices' of float is not in the list of allowed values: int32, int64\n\t; NodeDef: ResourceGather = ResourceGather[Tindices=DT_FLOAT, dtype=DT_FLOAT, validate_indices=true](dummy_input, dummy_input); Op<name=ResourceGather; signature=resource:resource, indices:Tindices -> output:dtype; attr=validate_indices:bool,default=true; attr=dtype:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true> [Op:ResourceGather] name: embedding_lookup/embedding_lookup//"
     ]
    }
   ],
   "source": [
    "# embedding\n",
    "input_embeds = tf.nn.embedding_lookup(embeddings, ids)\n",
    "print(input_embeds)\n",
    "\n",
    "input_flat = tf.layers.flatten(input_embeds)\n",
    "input_flat = tf.expand_dims(input_flat, -1)\n",
    "\n",
    "init_label = tf.nn.embedding_lookup(embeddings, init_label)\n",
    "init_label = tf.layers.flatten(init_label)\n",
    "init_label = tf.expand_dims(init_label, -1)\n",
    "\n",
    "print(input_flat.shape, init_label.shape)\n",
    "\n",
    "label_onehot = tf.one_hot(label_ids, depth=dict_size, dtype=tf.float32)\n",
    "print(label_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 7, 20)\n",
      "(2, 7, 10)\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "encoder_conv = tf.layers.conv1d(\n",
    "        inputs=input_flat,\n",
    "        filters=2*embed_size,\n",
    "        kernel_size=filter_size,\n",
    "        strides=embed_size,\n",
    "        padding='same')\n",
    "\n",
    "print(encoder_conv.shape)\n",
    "#embed_size, 반으로 나눈다.\n",
    "encoder_glu = encoder_conv[:, :, embed_size:]*tf.nn.sigmoid(encoder_conv[:, :, :embed_size])\n",
    "print(encoder_glu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 20)\n",
      "(2, 1, 10)\n",
      "(2, 7, 10)\n",
      "(2, 7, 1)\n",
      "tf.Tensor(\n",
      "[[[ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]]\n",
      "\n",
      " [[ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]]], shape=(2, 7, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#decoder: 그림에서 왼쪽에서 하나의 블록을 구하는 식\n",
    "\n",
    "decoder_conv = tf.layers.conv1d(\n",
    "        inputs=init_label,\n",
    "        filters=2*embed_size,\n",
    "        kernel_size=filter_size,\n",
    "        strides=embed_size)\n",
    "\n",
    "decoder_glu = decoder_conv[:, :, embed_size:]*tf.nn.sigmoid(decoder_conv[:, :, :embed_size])\n",
    "\n",
    "print(decoder_conv.shape)\n",
    "print(decoder_glu.shape)\n",
    "\n",
    "tiled_decoder_glu = tf.tile(decoder_glu, [1, int(encoder_glu.shape[1]), 1]) #decoder 단어 1개, encoder는 7개단어)\n",
    "print(tiled_decoder_glu.shape)\n",
    "\n",
    "dot_prod = tf.matmul(encoder_glu, decoder_glu, transpose_b=True)\n",
    "print(dot_prod.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 7, 1)\n",
      "(2, 7, 10)\n",
      "(2, 7, 10)\n",
      "(2, 10)\n",
      "(2, 10)\n",
      "(2, 5)\n",
      "tf.Tensor(\n",
      "[[ 0.27388293  0.11168921  0.34511361  0.04551498  0.22379932]\n",
      " [ 0.37078524  0.07891139  0.3009578   0.02622434  0.2231212 ]], shape=(2, 5), dtype=float32)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'decoder_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-b0b85f812dec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mnext_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mnext_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_embeds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mnext_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder_input' is not defined"
     ]
    }
   ],
   "source": [
    "attention = tf.nn.softmax(dot_prod, axis=1) #수정해야함\n",
    "#tf.reduce_sum(attention, axis=1)\n",
    "print(attention.shape)\n",
    "\n",
    "z_plus_e = encoder_glu + input_embeds #why???\n",
    "print(z_plus_e.shape)\n",
    "\n",
    "tiled_attention = tf.tile(attention, [1, 1, embed_size])\n",
    "print(tiled_attention.shape)\n",
    "\n",
    "c = tf.reduce_sum(tiled_attention*z_plus_e, axis=1)\n",
    "print(c.shape)\n",
    "\n",
    "decoder_glu = tf.reshape(decoder_glu, [-1, embed_size])\n",
    "print(decoder_glu.shape)\n",
    "\n",
    "logits = tf.layers.dense(c+decoder_glu, dict_size)\n",
    "print(logits.shape)\n",
    "\n",
    "out = tf.nn.softmax(logits)\n",
    "print(out)\n",
    "\n",
    "next_id = tf.argmax(out, axis=1)\n",
    "next_embeds = tf.nn.embedding_lookup(embeddings, next_id)\n",
    "next_embeds = tf.expand_dims(next_embeds, -1)\n",
    "decoder_input = tf.concat([decoder_input[:, embed_size:], next_embeds], axis=1)\n",
    "\n",
    "next_ids.append(next_id)\n",
    "outs.append(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]]\n",
      "\n",
      " [[ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]]], shape=(2, 7, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c = tf.reduce_sum(tiled_attention*z_plus_e, axis=1)\n",
    "decoder_glu = tf.reshape(decoder_glu, [-1, embed_size])\n",
    "\n",
    "logits = tf.layers.dense(c+decoder_glu, dict_size)\n",
    "\n",
    "out = tf.nn.softmax(logits) #out으로 묶기\n",
    "\n",
    "next_id = tf.argmax(out, axis=1)\n",
    "next_embeds = tf.nn.embedding_lookup(embeddings, next_id)\n",
    "next_embeds = tf.expand_dims(next_embeds, -1)\n",
    "decoder_input = tf.concat([decoder_input[:, embed_size:], next_embeds], axis=1)\n",
    "\n",
    "next_ids.append(next_id)\n",
    "outs.append(logits)\n",
    "\n",
    "outs = tf.stack(outs, axis=1)\n",
    "next_ids = tf.stack(next_ids, axis=1)\n",
    "\n",
    "if TRAIN:\n",
    "    global_step = tf.train.get_global_step()\n",
    "    loss = tf.losses.softmax_cross_entropy(label_onehot, outs)\n",
    "    train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss, global_step)\n",
    "    estimator_spec = tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            train_op=train_op,\n",
    "            loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 1.60669613  0.58048368  4.32952166 -0.75439    -6.57604551]\n",
      "  [ 2.47091866 -1.72941756 -4.18986034  2.36932921  2.30357647]\n",
      "  [ 2.63007236 -6.39063406 -1.39494836 -2.52273297  6.33367109]\n",
      "  [ 6.15402555 -5.65982628 -2.03500366  2.70822239  1.98549938]\n",
      "  [ 0.19027537  5.78388309  2.20064783 -3.58040476  1.57070911]]\n",
      "\n",
      " [[ 0.87436223 -1.44449735  6.2491827   2.25679016 -9.77695084]\n",
      "  [-0.18215799 -1.39346814  0.43136549  5.30196714  1.92673659]\n",
      "  [ 4.68214655 -7.55460072 -5.41391659 -4.77869511  9.60904503]\n",
      "  [ 7.43560982 -3.8282907  -1.15743423  0.38468164 -0.97345304]\n",
      "  [ 1.20879579  9.34952164  1.27624547 -1.68866849 -2.31034088]]], shape=(2, 5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2 0 4 0 1]\n",
      " [2 3 4 0 1]], shape=(2, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#decoder\n",
    "next_ids = []\n",
    "outs = []\n",
    "for l in range(label_length):\n",
    "    if l == 0: decoder_input = init_label\n",
    "\n",
    "    decoder_conv = tf.layers.conv1d(\n",
    "            inputs=init_label,\n",
    "            filters=2*embed_size,\n",
    "            kernel_size=filter_size,\n",
    "            strides=embed_size)\n",
    "\n",
    "\n",
    "    decoder_glu = decoder_conv[:, :, embed_size:]*tf.nn.sigmoid(decoder_conv[:, :, :embed_size])\n",
    "\n",
    "    tiled_decoder_glu = tf.tile(decoder_glu, [1, int(encoder_glu.shape[1]), 1])\n",
    "\n",
    "\n",
    "    dot_prod = tf.matmul(encoder_glu, decoder_glu, transpose_b=True)\n",
    "\n",
    "    attention = tf.nn.softmax(dot_prod)\n",
    "\n",
    "    z_plus_e = encoder_glu + input_embeds\n",
    "\n",
    "    tiled_attention = tf.tile(attention, [1, 1, embed_size])\n",
    "\n",
    "    c = tf.reduce_sum(tiled_attention*z_plus_e, axis=1)\n",
    "    decoder_glu = tf.reshape(decoder_glu, [-1, embed_size])\n",
    "\n",
    "    logits = tf.layers.dense(c+decoder_glu, dict_size)\n",
    "\n",
    "    out = tf.nn.softmax(logits)\n",
    "\n",
    "    next_id = tf.argmax(out, axis=1)\n",
    "    next_embeds = tf.nn.embedding_lookup(embeddings, next_id)\n",
    "    next_embeds = tf.expand_dims(next_embeds, -1)\n",
    "    decoder_input = tf.concat([decoder_input[:, embed_size:], next_embeds], axis=1) #옆에 붙이자, 기존꺼 버리고..\n",
    "\n",
    "    next_ids.append(next_id)\n",
    "    outs.append(logits)\n",
    "\n",
    "outs = tf.stack(outs, axis=1)\n",
    "next_ids = tf.stack(next_ids, axis=1)\n",
    "\n",
    "print(outs)\n",
    "print(next_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`loss` passed to Optimizer.compute_gradients should be a function when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-e143c1799c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    419\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         raise RuntimeError(\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0;34m\"`loss` passed to Optimizer.compute_gradients should \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m             \"be a function when eager execution is enabled.\")\n\u001b[1;32m    423\u001b[0m       \u001b[0;31m# TODO(agarwal): consider passing parameters to the `loss` function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `loss` passed to Optimizer.compute_gradients should be a function when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "loss = tf.losses.softmax_cross_entropy(label_onehot, outs)\n",
    "train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_glu = encoder_conv[:, :, embed_size:]*tf.nn.sigmoid(encoder_conv[:, :, :embed_size])\n",
    "\n",
    "#decoder\n",
    "next_ids = []\n",
    "outs = []\n",
    "for l in range(features['label_length']):\n",
    "    if l == 0: decoder_input = init_label\n",
    "\n",
    "    decoder_conv = tf.layers.conv1d(\n",
    "            inputs=init_label,\n",
    "            filters=2*embed_size,\n",
    "            kernel_size=filter_size,\n",
    "            strides=embed_size)\n",
    "\n",
    "\n",
    "    decoder_glu = decoder_conv[:, :, embed_size:]*tf.nn.sigmoid(decoder_conv[:, :, :embed_size])\n",
    "\n",
    "    tiled_decoder_glu = tf.tile(decoder_glu, [1, int(encoder_glu.shape[1]), 1])\n",
    "\n",
    "\n",
    "    dot_prod = tf.matmul(encoder_glu, decoder_glu, transpose_b=True)\n",
    "\n",
    "    attention = tf.nn.softmax(dot_prod)\n",
    "\n",
    "    z_plus_e = encoder_glu + input_embeds\n",
    "\n",
    "    tiled_attention = tf.tile(attention, [1, 1, embed_size])\n",
    "\n",
    "    c = tf.reduce_sum(tiled_attention*z_plus_e, axis=1)\n",
    "    decoder_glu = tf.reshape(decoder_glu, [-1, embed_size])\n",
    "\n",
    "    logits = tf.layers.dense(c+decoder_glu, dict_size)\n",
    "\n",
    "    out = tf.nn.softmax(logits)\n",
    "\n",
    "    next_id = tf.argmax(out, axis=1)\n",
    "    next_embeds = tf.nn.embedding_lookup(embeddings, next_id)\n",
    "    next_embeds = tf.expand_dims(next_embeds, -1)\n",
    "    decoder_input = tf.concat([decoder_input[:, embed_size:], next_embeds], axis=1) #옆에 붙이자, 기존꺼 버리고..\n",
    "\n",
    "    next_ids.append(next_id)\n",
    "    outs.append(logits)\n",
    "\n",
    "outs = tf.stack(outs, axis=1)\n",
    "next_ids = tf.stack(next_ids, axis=1)\n",
    "\n",
    "if TRAIN:\n",
    "    global_step = tf.train.get_global_step()\n",
    "    loss = tf.losses.softmax_cross_entropy(label_onehot, outs)\n",
    "    train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss, global_step)\n",
    "    estimator_spec = tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            train_op=train_op,\n",
    "            loss=loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FATAL Flags parsing error: Unknown command line flag 'f'\n",
      "Pass --helpshort or --helpfull to see help on flags.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naver/tf_nightly/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def model_fn(mode, features, labels):\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "\n",
    "    embeddings = tf.Variable(tf.truncated_normal([dict_size, embed_size]), trainable=False)\n",
    "\n",
    "    # embedding\n",
    "    input_embeds = tf.nn.embedding_lookup(embeddings, features['ids'])\n",
    "\n",
    "    input_flat = tf.layers.flatten(input_embeds)\n",
    "    input_flat = tf.expand_dims(input_flat, -1)\n",
    "\n",
    "    init_label = tf.nn.embedding_lookup(embeddings, features['init_label'])\n",
    "    init_label = tf.layers.flatten(init_label)\n",
    "    init_label = tf.expand_dims(init_label, -1)\n",
    "\n",
    "    if not PREDICT:\n",
    "        label_onehot = tf.one_hot(labels, depth=dict_size, dtype=tf.float32)\n",
    "\n",
    "    # encoder\n",
    "    encoder_conv = tf.layers.conv1d(\n",
    "            inputs=input_flat,\n",
    "            filters=2*embed_size,\n",
    "            kernel_size=filter_size,\n",
    "            strides=embed_size,\n",
    "            padding='same')\n",
    "\n",
    "    encoder_glu = encoder_conv[:, :, embed_size:]*tf.nn.sigmoid(encoder_conv[:, :, :embed_size])\n",
    "\n",
    "\n",
    "\n",
    "        decoder_glu = decoder_conv[:, :, embed_size:]*tf.nn.sigmoid(decoder_conv[:, :, :embed_size])\n",
    "\n",
    "        tiled_decoder_glu = tf.tile(decoder_glu, [1, int(encoder_glu.shape[1]), 1])\n",
    "\n",
    "\n",
    "        dot_prod = tf.matmul(encoder_glu, decoder_glu, transpose_b=True)\n",
    "\n",
    "        attention = tf.nn.softmax(dot_prod)\n",
    "\n",
    "        z_plus_e = encoder_glu + input_embeds\n",
    "\n",
    "        tiled_attention = tf.tile(attention, [1, 1, embed_size])\n",
    "\n",
    "        c = tf.reduce_sum(tiled_attention*z_plus_e, axis=1)\n",
    "        decoder_glu = tf.reshape(decoder_glu, [-1, embed_size])\n",
    "\n",
    "        logits = tf.layers.dense(c+decoder_glu, dict_size)\n",
    "\n",
    "        out = tf.nn.softmax(logits)\n",
    "\n",
    "        next_id = tf.argmax(out, axis=1)\n",
    "        next_embeds = tf.nn.embedding_lookup(embeddings, next_id)\n",
    "        next_embeds = tf.expand_dims(next_embeds, -1)\n",
    "        decoder_input = tf.concat([decoder_input[:, embed_size:], next_embeds], axis=1)\n",
    "\n",
    "        next_ids.append(next_id)\n",
    "        outs.append(logits)\n",
    "\n",
    "    outs = tf.stack(outs, axis=1)\n",
    "    next_ids = tf.stack(next_ids, axis=1)\n",
    "\n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        loss = tf.losses.softmax_cross_entropy(label_onehot, outs)\n",
    "        train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss, global_step)\n",
    "        estimator_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                train_op=train_op,\n",
    "                loss=loss)\n",
    "\n",
    "    elif EVAL:\n",
    "        loss = tf.losses.softmax_cross_entropy(label_onehot, outs)\n",
    "        eval_metric_ops = {'acc': tf.metrics.accuracy(labels, next_ids)}\n",
    "        estimator_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=loss,\n",
    "                eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "    elif PREDICT:\n",
    "        estimator_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions={'prediction': next_ids})\n",
    "\n",
    "    else:\n",
    "        raise Exception('estiamtor spec is invalid')\n",
    "\n",
    "    return estimator_spec\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    fairseq = tf.estimator.Estimator(model_fn)\n",
    "    fairseq.train(input_fn, steps=10000)\n",
    "    fairseq.evaluate(input_fn, steps=10)\n",
    "\n",
    "    pred = fairseq.predict(input_fn)\n",
    "    for p in pred:\n",
    "        print('prediction: {}'.format(p['prediction']))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
