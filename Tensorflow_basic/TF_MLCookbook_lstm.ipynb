{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set RNN Parameters\n",
    "min_word_freq = 5 # Trim the less frequent words off\n",
    "rnn_size = 128 # RNN Model size\n",
    "embedding_size = 100 # Word embedding size\n",
    "epochs = 10 # Number of epochs to cycle through data\n",
    "batch_size = 100 # Train on this many examples at once\n",
    "learning_rate = 0.001 # Learning rate\n",
    "training_seq_len = 50 # how long of a word group to consider \n",
    "embedding_size = rnn_size\n",
    "save_every = 500 # How often to save model checkpoints\n",
    "eval_every = 50 # How often to evaluate the test sentences\n",
    "prime_texts = ['thou art more', 'to be or not to', 'wherefore art thou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare Data\n",
      "Not found, downloading Shakespeare texts from www.gutenberg.org\n",
      "Cleaning Text\n",
      "Done loading/cleaning.\n"
     ]
    }
   ],
   "source": [
    "# Download/store Shakespeare data\n",
    "data_dir = 'temp'\n",
    "data_file = 'shakespeare.txt'\n",
    "model_path = 'shakespeare_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)\n",
    "\n",
    "# Declare punctuation to remove, everything except hyphens and apostrophes\n",
    "punctuation = string.punctuation\n",
    "punctuation = ''.join([x for x in punctuation if x not in ['-', \"'\"]])\n",
    "\n",
    "# Make Model Directory\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)\n",
    "\n",
    "# Make data directory\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "\n",
    "print('Loading Shakespeare Data')\n",
    "# Check if file is downloaded.\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Not found, downloading Shakespeare texts from www.gutenberg.org')\n",
    "    shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "    # Get Shakespeare text\n",
    "    response = requests.get(shakespeare_url)\n",
    "    shakespeare_file = response.content\n",
    "    # Decode binary into string\n",
    "    s_text = shakespeare_file.decode('utf-8')\n",
    "    # Drop first few descriptive paragraphs.\n",
    "    s_text = s_text[7675:]\n",
    "    # Remove newlines\n",
    "    s_text = s_text.replace('\\r\\n', '')\n",
    "    s_text = s_text.replace('\\n', '')\n",
    "    \n",
    "    # Write to file\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        out_conn.write(s_text)\n",
    "else:\n",
    "    # If file has been saved, load from that file\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        s_text = file_conn.read().replace('\\n', '')\n",
    "\n",
    "# Clean text\n",
    "print('Cleaning Text')\n",
    "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
    "s_text = re.sub('\\s+', ' ', s_text ).strip().lower()\n",
    "print('Done loading/cleaning.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build word vocabulary function\n",
    "def build_vocab(text, min_word_freq):\n",
    "    word_counts = collections.Counter(text.split(' '))\n",
    "    # limit word counts to those more frequent than cutoff\n",
    "    word_counts = {key:val for key, val in word_counts.items() if val>min_word_freq}\n",
    "    # Create vocab --> index mapping\n",
    "    words = word_counts.keys()\n",
    "    vocab_to_ix_dict = {key:(ix+1) for ix, key in enumerate(words)}\n",
    "    # Add unknown key --> 0 index\n",
    "    vocab_to_ix_dict['unknown']=0\n",
    "    # Create index --> vocab mapping\n",
    "    ix_to_vocab_dict = {val:key for key,val in vocab_to_ix_dict.items()}\n",
    "    \n",
    "    return(ix_to_vocab_dict, vocab_to_ix_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Shakespeare Vocab\n",
      "Vocabulary Length = 8009\n"
     ]
    }
   ],
   "source": [
    "# Build Shakespeare vocabulary\n",
    "print('Building Shakespeare Vocab')\n",
    "ix2vocab, vocab2ix = build_vocab(s_text, min_word_freq)\n",
    "vocab_size = len(ix2vocab) + 1\n",
    "print('Vocabulary Length = {}'.format(vocab_size))\n",
    "# Sanity Check\n",
    "assert(len(ix2vocab) == len(vocab2ix))\n",
    "\n",
    "# Convert text to word vectors\n",
    "s_text_words = s_text.split(' ')\n",
    "s_text_ix = []\n",
    "for ix, x in enumerate(s_text_words):\n",
    "    try:\n",
    "        s_text_ix.append(vocab2ix[x])\n",
    "    except:\n",
    "        s_text_ix.append(0)\n",
    "s_text_ix = np.array(s_text_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM RNN Model\n",
    "class LSTM_Model():\n",
    "    def __init__(self, embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                 training_seq_len, vocab_size, infer_sample=False):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.training_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.training_seq_len = training_seq_len\n",
    "        \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.rnn_size)\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            # Softmax Output Weights\n",
    "            W = tf.get_variable('W', [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "        \n",
    "            # Define Embedding\n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.embedding_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "                                            \n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "        \n",
    "        # If we are inferring (generating text), we add a 'loop' function\n",
    "        # Define how to get the i+1 th input from the i th output\n",
    "        def inferred_loop(prev, count):\n",
    "            # Apply hidden layer\n",
    "            prev_transformed = tf.matmul(prev, W) + b\n",
    "            # Get the index of the output (also don't run the gradient)\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "            # Get embedded vector\n",
    "            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "            return(output)\n",
    "        \n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(rnn_inputs_trimmed,\n",
    "                                      self.initial_state,\n",
    "                                      self.lstm_cell,\n",
    "                                      loop_function=inferred_loop if infer_sample else None)\n",
    "        # Non inferred outputs\n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n",
    "        # Logits and output\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_fun([self.logit_output],[tf.reshape(self.y_output, [-1])],\n",
    "                [tf.ones([self.batch_size * self.training_seq_len])],\n",
    "                self.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "        \n",
    "    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=10, prime_text='thou art'):\n",
    "        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n",
    "        word_list = prime_text.split()\n",
    "        for word in word_list[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n",
    "\n",
    "        out_sentence = prime_text\n",
    "        word = word_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            if sample == 0:\n",
    "                break\n",
    "            word = words[sample]\n",
    "            out_sentence = out_sentence + ' ' + word\n",
    "        return(out_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define LStm model\n",
    "lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                       training_seq_len, vocab_size)\n",
    "\n",
    "#Tensorflow에게 scope를 테스트용으로 사용할 것이라고 전달\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse = True):\n",
    "    test_lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                                training_seq_len, vocab_size, infer_sample=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model Save\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "# Create batches for each epoch\n",
    "num_batches = int(len(s_text_ix)/(batch_size * training_seq_len)) + 1\n",
    "# Split up text indices into subarrays, of equal size\n",
    "batches = np.array_split(s_text_ix, num_batches)\n",
    "# Reshape each split into [batch_size, training_seq_len]\n",
    "batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize all variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Eopch #1 of 10.\n",
      "Iteration: 10, Epoch: 1, Batch: 10 out of 182, Loss: 9.89\n",
      "Iteration: 20, Epoch: 1, Batch: 20 out of 182, Loss: 8.91\n",
      "Iteration: 30, Epoch: 1, Batch: 30 out of 182, Loss: 8.51\n",
      "Iteration: 40, Epoch: 1, Batch: 40 out of 182, Loss: 8.21\n",
      "Iteration: 50, Epoch: 1, Batch: 50 out of 182, Loss: 7.88\n",
      "thou art more feign of the\n",
      "to be or not to the\n",
      "wherefore art thou art henceforward of a\n",
      "Iteration: 60, Epoch: 1, Batch: 60 out of 182, Loss: 7.76\n",
      "Iteration: 70, Epoch: 1, Batch: 70 out of 182, Loss: 7.52\n",
      "Iteration: 80, Epoch: 1, Batch: 80 out of 182, Loss: 7.04\n",
      "Iteration: 90, Epoch: 1, Batch: 90 out of 182, Loss: 6.96\n",
      "Iteration: 100, Epoch: 1, Batch: 100 out of 182, Loss: 6.93\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art\n",
      "Iteration: 110, Epoch: 1, Batch: 110 out of 182, Loss: 6.99\n",
      "Iteration: 120, Epoch: 1, Batch: 120 out of 182, Loss: 6.79\n",
      "Iteration: 130, Epoch: 1, Batch: 130 out of 182, Loss: 6.85\n",
      "Iteration: 140, Epoch: 1, Batch: 140 out of 182, Loss: 6.61\n",
      "Iteration: 150, Epoch: 1, Batch: 150 out of 182, Loss: 6.55\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art a\n",
      "Iteration: 160, Epoch: 1, Batch: 160 out of 182, Loss: 6.74\n",
      "Iteration: 170, Epoch: 1, Batch: 170 out of 182, Loss: 6.69\n",
      "Iteration: 180, Epoch: 1, Batch: 180 out of 182, Loss: 6.44\n",
      "Starting Eopch #2 of 10.\n",
      "Iteration: 190, Epoch: 2, Batch: 9 out of 182, Loss: 6.49\n",
      "Iteration: 200, Epoch: 2, Batch: 19 out of 182, Loss: 6.52\n",
      "thou art more than\n",
      "to be or not to the\n",
      "wherefore art thou art\n",
      "Iteration: 210, Epoch: 2, Batch: 29 out of 182, Loss: 6.35\n",
      "Iteration: 220, Epoch: 2, Batch: 39 out of 182, Loss: 6.37\n",
      "Iteration: 230, Epoch: 2, Batch: 49 out of 182, Loss: 6.08\n",
      "Iteration: 240, Epoch: 2, Batch: 59 out of 182, Loss: 6.40\n",
      "Iteration: 250, Epoch: 2, Batch: 69 out of 182, Loss: 6.52\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art\n",
      "Iteration: 260, Epoch: 2, Batch: 79 out of 182, Loss: 6.71\n",
      "Iteration: 270, Epoch: 2, Batch: 89 out of 182, Loss: 6.70\n",
      "Iteration: 280, Epoch: 2, Batch: 99 out of 182, Loss: 6.59\n",
      "Iteration: 290, Epoch: 2, Batch: 109 out of 182, Loss: 6.49\n",
      "Iteration: 300, Epoch: 2, Batch: 119 out of 182, Loss: 6.44\n",
      "thou art more than a\n",
      "to be or not to\n",
      "wherefore art thou hast thou hast thou art thou art thou art thou\n",
      "Iteration: 310, Epoch: 2, Batch: 129 out of 182, Loss: 6.51\n",
      "Iteration: 320, Epoch: 2, Batch: 139 out of 182, Loss: 6.24\n",
      "Iteration: 330, Epoch: 2, Batch: 149 out of 182, Loss: 6.30\n",
      "Iteration: 340, Epoch: 2, Batch: 159 out of 182, Loss: 6.43\n",
      "Iteration: 350, Epoch: 2, Batch: 169 out of 182, Loss: 6.27\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou art thou art a\n",
      "Iteration: 360, Epoch: 2, Batch: 179 out of 182, Loss: 6.10\n",
      "Starting Eopch #3 of 10.\n",
      "Iteration: 370, Epoch: 3, Batch: 8 out of 182, Loss: 6.19\n",
      "Iteration: 380, Epoch: 3, Batch: 18 out of 182, Loss: 6.20\n",
      "Iteration: 390, Epoch: 3, Batch: 28 out of 182, Loss: 5.98\n",
      "Iteration: 400, Epoch: 3, Batch: 38 out of 182, Loss: 6.38\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou art thou art a\n",
      "Iteration: 410, Epoch: 3, Batch: 48 out of 182, Loss: 6.30\n",
      "Iteration: 420, Epoch: 3, Batch: 58 out of 182, Loss: 6.27\n",
      "Iteration: 430, Epoch: 3, Batch: 68 out of 182, Loss: 6.12\n",
      "Iteration: 440, Epoch: 3, Batch: 78 out of 182, Loss: 6.17\n",
      "Iteration: 450, Epoch: 3, Batch: 88 out of 182, Loss: 6.29\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou\n",
      "Iteration: 460, Epoch: 3, Batch: 98 out of 182, Loss: 6.20\n",
      "Iteration: 470, Epoch: 3, Batch: 108 out of 182, Loss: 6.25\n",
      "Iteration: 480, Epoch: 3, Batch: 118 out of 182, Loss: 6.17\n",
      "Iteration: 490, Epoch: 3, Batch: 128 out of 182, Loss: 6.09\n",
      "Iteration: 500, Epoch: 3, Batch: 138 out of 182, Loss: 6.12\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than a\n",
      "to be or not to be\n",
      "wherefore art thou hast thou hast thou\n",
      "Iteration: 510, Epoch: 3, Batch: 148 out of 182, Loss: 6.22\n",
      "Iteration: 520, Epoch: 3, Batch: 158 out of 182, Loss: 6.28\n",
      "Iteration: 530, Epoch: 3, Batch: 168 out of 182, Loss: 6.05\n",
      "Iteration: 540, Epoch: 3, Batch: 178 out of 182, Loss: 5.94\n",
      "Starting Eopch #4 of 10.\n",
      "Iteration: 550, Epoch: 4, Batch: 7 out of 182, Loss: 5.82\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou art a\n",
      "Iteration: 560, Epoch: 4, Batch: 17 out of 182, Loss: 6.28\n",
      "Iteration: 570, Epoch: 4, Batch: 27 out of 182, Loss: 6.31\n",
      "Iteration: 580, Epoch: 4, Batch: 37 out of 182, Loss: 6.33\n",
      "Iteration: 590, Epoch: 4, Batch: 47 out of 182, Loss: 6.17\n",
      "Iteration: 600, Epoch: 4, Batch: 57 out of 182, Loss: 6.12\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast thou art a\n",
      "Iteration: 610, Epoch: 4, Batch: 67 out of 182, Loss: 6.01\n",
      "Iteration: 620, Epoch: 4, Batch: 77 out of 182, Loss: 6.21\n",
      "Iteration: 630, Epoch: 4, Batch: 87 out of 182, Loss: 5.95\n",
      "Iteration: 640, Epoch: 4, Batch: 97 out of 182, Loss: 6.08\n",
      "Iteration: 650, Epoch: 4, Batch: 107 out of 182, Loss: 6.10\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou hast thou\n",
      "Iteration: 660, Epoch: 4, Batch: 117 out of 182, Loss: 6.06\n",
      "Iteration: 670, Epoch: 4, Batch: 127 out of 182, Loss: 6.02\n",
      "Iteration: 680, Epoch: 4, Batch: 137 out of 182, Loss: 6.09\n",
      "Iteration: 690, Epoch: 4, Batch: 147 out of 182, Loss: 6.32\n",
      "Iteration: 700, Epoch: 4, Batch: 157 out of 182, Loss: 6.15\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou hast thou hast thou\n",
      "Iteration: 710, Epoch: 4, Batch: 167 out of 182, Loss: 6.04\n",
      "Iteration: 720, Epoch: 4, Batch: 177 out of 182, Loss: 6.11\n",
      "Starting Eopch #5 of 10.\n",
      "Iteration: 730, Epoch: 5, Batch: 6 out of 182, Loss: 5.99\n",
      "Iteration: 740, Epoch: 5, Batch: 16 out of 182, Loss: 6.14\n",
      "Iteration: 750, Epoch: 5, Batch: 26 out of 182, Loss: 6.05\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast thou art a\n",
      "Iteration: 760, Epoch: 5, Batch: 36 out of 182, Loss: 5.86\n",
      "Iteration: 770, Epoch: 5, Batch: 46 out of 182, Loss: 5.95\n",
      "Iteration: 780, Epoch: 5, Batch: 56 out of 182, Loss: 5.87\n",
      "Iteration: 790, Epoch: 5, Batch: 66 out of 182, Loss: 6.05\n",
      "Iteration: 800, Epoch: 5, Batch: 76 out of 182, Loss: 5.82\n",
      "thou art more than than the\n",
      "to be or not to your\n",
      "wherefore art thou hast thou art a\n",
      "Iteration: 810, Epoch: 5, Batch: 86 out of 182, Loss: 5.87\n",
      "Iteration: 820, Epoch: 5, Batch: 96 out of 182, Loss: 6.01\n",
      "Iteration: 830, Epoch: 5, Batch: 106 out of 182, Loss: 5.96\n",
      "Iteration: 840, Epoch: 5, Batch: 116 out of 182, Loss: 5.98\n",
      "Iteration: 850, Epoch: 5, Batch: 126 out of 182, Loss: 6.01\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast thou art a\n",
      "Iteration: 860, Epoch: 5, Batch: 136 out of 182, Loss: 5.98\n",
      "Iteration: 870, Epoch: 5, Batch: 146 out of 182, Loss: 5.98\n",
      "Iteration: 880, Epoch: 5, Batch: 156 out of 182, Loss: 6.21\n",
      "Iteration: 890, Epoch: 5, Batch: 166 out of 182, Loss: 6.00\n",
      "Iteration: 900, Epoch: 5, Batch: 176 out of 182, Loss: 5.87\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast thou art a\n",
      "Starting Eopch #6 of 10.\n",
      "Iteration: 910, Epoch: 6, Batch: 5 out of 182, Loss: 6.04\n",
      "Iteration: 920, Epoch: 6, Batch: 15 out of 182, Loss: 5.97\n",
      "Iteration: 930, Epoch: 6, Batch: 25 out of 182, Loss: 5.71\n",
      "Iteration: 940, Epoch: 6, Batch: 35 out of 182, Loss: 6.11\n",
      "Iteration: 950, Epoch: 6, Batch: 45 out of 182, Loss: 6.12\n",
      "thou art more than than the\n",
      "to be or not to\n",
      "wherefore art thou hast thou art a\n",
      "Iteration: 960, Epoch: 6, Batch: 55 out of 182, Loss: 5.95\n",
      "Iteration: 970, Epoch: 6, Batch: 65 out of 182, Loss: 5.91\n",
      "Iteration: 980, Epoch: 6, Batch: 75 out of 182, Loss: 6.17\n",
      "Iteration: 990, Epoch: 6, Batch: 85 out of 182, Loss: 5.89\n",
      "Iteration: 1000, Epoch: 6, Batch: 95 out of 182, Loss: 5.77\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast thou hast thou hast thou\n",
      "Iteration: 1010, Epoch: 6, Batch: 105 out of 182, Loss: 6.21\n",
      "Iteration: 1020, Epoch: 6, Batch: 115 out of 182, Loss: 5.86\n",
      "Iteration: 1030, Epoch: 6, Batch: 125 out of 182, Loss: 6.15\n",
      "Iteration: 1040, Epoch: 6, Batch: 135 out of 182, Loss: 6.09\n",
      "Iteration: 1050, Epoch: 6, Batch: 145 out of 182, Loss: 5.96\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast strong\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1060, Epoch: 6, Batch: 155 out of 182, Loss: 5.90\n",
      "Iteration: 1070, Epoch: 6, Batch: 165 out of 182, Loss: 5.72\n",
      "Iteration: 1080, Epoch: 6, Batch: 175 out of 182, Loss: 6.10\n",
      "Starting Eopch #7 of 10.\n",
      "Iteration: 1090, Epoch: 7, Batch: 4 out of 182, Loss: 5.97\n",
      "Iteration: 1100, Epoch: 7, Batch: 14 out of 182, Loss: 5.90\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast a\n",
      "Iteration: 1110, Epoch: 7, Batch: 24 out of 182, Loss: 6.10\n",
      "Iteration: 1120, Epoch: 7, Batch: 34 out of 182, Loss: 5.89\n",
      "Iteration: 1130, Epoch: 7, Batch: 44 out of 182, Loss: 6.06\n",
      "Iteration: 1140, Epoch: 7, Batch: 54 out of 182, Loss: 5.82\n",
      "Iteration: 1150, Epoch: 7, Batch: 64 out of 182, Loss: 5.57\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast a\n",
      "Iteration: 1160, Epoch: 7, Batch: 74 out of 182, Loss: 5.93\n",
      "Iteration: 1170, Epoch: 7, Batch: 84 out of 182, Loss: 6.19\n",
      "Iteration: 1180, Epoch: 7, Batch: 94 out of 182, Loss: 5.90\n",
      "Iteration: 1190, Epoch: 7, Batch: 104 out of 182, Loss: 5.87\n",
      "Iteration: 1200, Epoch: 7, Batch: 114 out of 182, Loss: 5.62\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast a\n",
      "Iteration: 1210, Epoch: 7, Batch: 124 out of 182, Loss: 5.84\n",
      "Iteration: 1220, Epoch: 7, Batch: 134 out of 182, Loss: 6.05\n",
      "Iteration: 1230, Epoch: 7, Batch: 144 out of 182, Loss: 5.86\n",
      "Iteration: 1240, Epoch: 7, Batch: 154 out of 182, Loss: 5.67\n",
      "Iteration: 1250, Epoch: 7, Batch: 164 out of 182, Loss: 6.09\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou hast a\n",
      "Iteration: 1260, Epoch: 7, Batch: 174 out of 182, Loss: 6.18\n",
      "Starting Eopch #8 of 10.\n",
      "Iteration: 1270, Epoch: 8, Batch: 3 out of 182, Loss: 6.02\n",
      "Iteration: 1280, Epoch: 8, Batch: 13 out of 182, Loss: 5.85\n",
      "Iteration: 1290, Epoch: 8, Batch: 23 out of 182, Loss: 5.51\n",
      "Iteration: 1300, Epoch: 8, Batch: 33 out of 182, Loss: 5.89\n",
      "thou art more than the\n",
      "to be or not to the\n",
      "wherefore art thou hast a\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train_loss = []\n",
    "iteration_count = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #shuffle word\n",
    "    random.shuffle(batches)\n",
    "    #create targets from shuffled batch\n",
    "    targets = [np.roll(x, -1, axis=1) for x in batches]\n",
    "    #Run a through one epoch\n",
    "    print('Starting Eopch #{} of {}.'.format(epoch+1, epochs))\n",
    "    #Reset init LSTM\n",
    "    state = sess.run(lstm_model.initial_state)\n",
    "    for ix, batch in enumerate(batches):\n",
    "        training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n",
    "        c, h = lstm_model.initial_state\n",
    "        training_dict[c] = state.c\n",
    "        training_dict[h] = state.h\n",
    "        \n",
    "        temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n",
    "                                       feed_dict=training_dict)\n",
    "        train_loss.append(temp_loss)\n",
    "        \n",
    "        # Print status every 10 gens\n",
    "        if iteration_count % 10 == 0:\n",
    "            summary_nums = (iteration_count, epoch+1, ix+1, num_batches+1, temp_loss)\n",
    "            print('Iteration: {}, Epoch: {}, Batch: {} out of {}, Loss: {:.2f}'.format(*summary_nums))\n",
    "        \n",
    "        # Save the model and the vocab\n",
    "        if iteration_count % save_every == 0:\n",
    "            # Save model\n",
    "            model_file_name = os.path.join(full_model_dir, 'model')\n",
    "            saver.save(sess, model_file_name, global_step = iteration_count)\n",
    "            print('Model Saved To: {}'.format(model_file_name))\n",
    "            # Save vocabulary\n",
    "            dictionary_file = os.path.join(full_model_dir, 'vocab.pkl')\n",
    "            with open(dictionary_file, 'wb') as dict_file_conn:\n",
    "                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)\n",
    "        \n",
    "        if iteration_count % eval_every == 0:\n",
    "            for sample in prime_texts:\n",
    "                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n",
    "                \n",
    "        iteration_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot loss over time\n",
    "plt.plot(train_loss, 'k-')\n",
    "plt.title('Sequence to Sequence Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. tf.squeeze(input, squeeze_dims=None, name=None): 텐서에서 1을 제거\n",
    "\n",
    "# 't'는 구조(shape) [1, 2, 1, 3, 1, 1]의 텐서\n",
    "shape(squeeze(t, [2, 4])) ==> [1, 2, 3, 1]\n",
    "\n",
    "#2. tf.stop_gradient: 잘 모르겠..\n",
    "#https://www.tensorflow.org/api_docs/python/tf/stop_gradient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
