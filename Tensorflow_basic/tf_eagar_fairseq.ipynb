{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 10\n",
    "window_size = 3\n",
    "dict_size = 5\n",
    "filter_size = window_size*embed_size\n",
    "\n",
    "def input_fn():\n",
    "    ids = tf.constant([[5, 6, 10, 9, 2], [5, 6, 10, 60, 2]])\n",
    "    label_ids = tf.constant([[4, 22, 163, 649, 2], [4, 22, 163, 649, 2]])\n",
    "    return {'ids': ids,\n",
    "            'init_label': label_ids[:, :window_size],\n",
    "            'label_length': int(label_ids.shape[1])-window_size\n",
    "            }, label_ids[:, window_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 0 0]\n",
      " [0 0 0]], shape=(2, 3), dtype=int32)\n",
      "5\n",
      "tf.Tensor(\n",
      "[[1 3 1 3 2]\n",
      " [2 1 3 4 4]], shape=(2, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "ids = tf.constant([[1, 2, 3, 4, 3, 2, 1], [1, 2, 3, 2, 3, 2, 3]])\n",
    "label_ids = tf.constant([[0, 0, 0, 1, 3, 1, 3, 2], [0, 0, 0, 2, 1, 3, 4, 4]]) #무조건 앞에 3개를... \n",
    "init_label = label_ids[:, :window_size]\n",
    "label_length = int(label_ids.shape[1]) - window_size\n",
    "label_ids = label_ids[:, window_size:]\n",
    "\n",
    "print(init_label)\n",
    "print(label_length)\n",
    "print(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10)\n"
     ]
    }
   ],
   "source": [
    "embeddings = tfe.Variable(tf.truncated_normal([dict_size, embed_size]), trainable=False)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.44031993  0.27558795  1.88564384 -0.26049265  1.04315972 -0.8651666\n",
      "    0.45365837  0.21361713  0.36355501  1.71674705]\n",
      "  [ 0.00447189 -0.20288323 -0.78719151  0.80474299 -0.97947961  0.43934852\n",
      "   -0.00413364  0.87714851 -0.58071738  0.23394325]\n",
      "  [-1.54866028  0.79748154  0.61276042  0.14294747  1.46765685  1.49111998\n",
      "   -1.82410061  0.7496416   0.30667016  1.82444489]\n",
      "  [-1.24891686  1.56246781 -1.05082929 -0.79415643 -0.33950487 -1.42375171\n",
      "   -0.82772219  0.00675092 -0.51040632  1.12121177]\n",
      "  [-1.54866028  0.79748154  0.61276042  0.14294747  1.46765685  1.49111998\n",
      "   -1.82410061  0.7496416   0.30667016  1.82444489]\n",
      "  [ 0.00447189 -0.20288323 -0.78719151  0.80474299 -0.97947961  0.43934852\n",
      "   -0.00413364  0.87714851 -0.58071738  0.23394325]\n",
      "  [-0.44031993  0.27558795  1.88564384 -0.26049265  1.04315972 -0.8651666\n",
      "    0.45365837  0.21361713  0.36355501  1.71674705]]\n",
      "\n",
      " [[-0.44031993  0.27558795  1.88564384 -0.26049265  1.04315972 -0.8651666\n",
      "    0.45365837  0.21361713  0.36355501  1.71674705]\n",
      "  [ 0.00447189 -0.20288323 -0.78719151  0.80474299 -0.97947961  0.43934852\n",
      "   -0.00413364  0.87714851 -0.58071738  0.23394325]\n",
      "  [-1.54866028  0.79748154  0.61276042  0.14294747  1.46765685  1.49111998\n",
      "   -1.82410061  0.7496416   0.30667016  1.82444489]\n",
      "  [ 0.00447189 -0.20288323 -0.78719151  0.80474299 -0.97947961  0.43934852\n",
      "   -0.00413364  0.87714851 -0.58071738  0.23394325]\n",
      "  [-1.54866028  0.79748154  0.61276042  0.14294747  1.46765685  1.49111998\n",
      "   -1.82410061  0.7496416   0.30667016  1.82444489]\n",
      "  [ 0.00447189 -0.20288323 -0.78719151  0.80474299 -0.97947961  0.43934852\n",
      "   -0.00413364  0.87714851 -0.58071738  0.23394325]\n",
      "  [-1.54866028  0.79748154  0.61276042  0.14294747  1.46765685  1.49111998\n",
      "   -1.82410061  0.7496416   0.30667016  1.82444489]]], shape=(2, 7, 10), dtype=float32)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Value for attr 'Tindices' of float is not in the list of allowed values: int32, int64\n\t; NodeDef: ResourceGather = ResourceGather[Tindices=DT_FLOAT, dtype=DT_FLOAT, validate_indices=true](dummy_input, dummy_input); Op<name=ResourceGather; signature=resource:resource, indices:Tindices -> output:dtype; attr=validate_indices:bool,default=true; attr=dtype:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true> [Op:ResourceGather] name: embedding_lookup/embedding_lookup//",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-13b5d4c8ddce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minput_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0minit_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0minit_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0minit_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[0;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[1;32m    323\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m       transform_fn=None)\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[0;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mtransform_fn\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(params, ids, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \"\"\"\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36msparse_read\u001b[0;34m(self, indices, name)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m       value = gen_resource_variable_ops.resource_gather(\n\u001b[0;32m--> 690\u001b[0;31m           self._handle, indices, dtype=self._dtype, name=name)\n\u001b[0m\u001b[1;32m    691\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mresource_gather\u001b[0;34m(resource, indices, dtype, validate_indices, name)\u001b[0m\n\u001b[1;32m    242\u001b[0m               \"Tindices\", _attr_Tindices)\n\u001b[1;32m    243\u001b[0m     _result = _execute.execute(b\"ResourceGather\", 1, inputs=_inputs_flat,\n\u001b[0;32m--> 244\u001b[0;31m                                attrs=_attrs, ctx=_ctx, name=name)\n\u001b[0m\u001b[1;32m    245\u001b[0m   _execute.record_gradient(\n\u001b[1;32m    246\u001b[0m       \"ResourceGather\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Value for attr 'Tindices' of float is not in the list of allowed values: int32, int64\n\t; NodeDef: ResourceGather = ResourceGather[Tindices=DT_FLOAT, dtype=DT_FLOAT, validate_indices=true](dummy_input, dummy_input); Op<name=ResourceGather; signature=resource:resource, indices:Tindices -> output:dtype; attr=validate_indices:bool,default=true; attr=dtype:type; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true> [Op:ResourceGather] name: embedding_lookup/embedding_lookup//"
     ]
    }
   ],
   "source": [
    "# embedding\n",
    "input_embeds = tf.nn.embedding_lookup(embeddings, ids)\n",
    "print(input_embeds)\n",
    "\n",
    "input_flat = tf.layers.flatten(input_embeds)\n",
    "input_flat = tf.expand_dims(input_flat, -1)\n",
    "\n",
    "init_label = tf.nn.embedding_lookup(embeddings, init_label)\n",
    "init_label = tf.layers.flatten(init_label)\n",
    "init_label = tf.expand_dims(init_label, -1)\n",
    "\n",
    "print(input_flat.shape, init_label.shape)\n",
    "\n",
    "label_onehot = tf.one_hot(label_ids, depth=dict_size, dtype=tf.float32)\n",
    "print(label_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 7, 20)\n",
      "(2, 7, 10)\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "encoder_conv = tf.layers.conv1d(\n",
    "        inputs=input_flat,\n",
    "        filters=2*embed_size,\n",
    "        kernel_size=filter_size,\n",
    "        strides=embed_size,\n",
    "        padding='same')\n",
    "\n",
    "print(encoder_conv.shape)\n",
    "#embed_size, 반으로 나눈다.\n",
    "encoder_glu = encoder_conv[:, :, embed_size:]*tf.nn.sigmoid(encoder_conv[:, :, :embed_size])\n",
    "print(encoder_glu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 20)\n",
      "(2, 1, 10)\n",
      "(2, 7, 10)\n",
      "(2, 7, 1)\n",
      "tf.Tensor(\n",
      "[[[ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]]\n",
      "\n",
      " [[ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]]], shape=(2, 7, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#decoder: 그림에서 왼쪽에서 하나의 블록을 구하는 식\n",
    "\n",
    "decoder_conv = tf.layers.conv1d(\n",
    "        inputs=init_label,\n",
    "        filters=2*embed_size,\n",
    "        kernel_size=filter_size,\n",
    "        strides=embed_size)\n",
    "\n",
    "decoder_glu = decoder_conv[:, :, embed_size:]*tf.nn.sigmoid(decoder_conv[:, :, :embed_size])\n",
    "\n",
    "print(decoder_conv.shape)\n",
    "print(decoder_glu.shape)\n",
    "\n",
    "tiled_decoder_glu = tf.tile(decoder_glu, [1, int(encoder_glu.shape[1]), 1]) #decoder 단어 1개, encoder는 7개단어)\n",
    "print(tiled_decoder_glu.shape)\n",
    "\n",
    "dot_prod = tf.matmul(encoder_glu, decoder_glu, transpose_b=True)\n",
    "print(dot_prod.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 7, 1)\n",
      "(2, 7, 10)\n",
      "(2, 7, 10)\n",
      "(2, 10)\n",
      "(2, 10)\n",
      "(2, 5)\n",
      "tf.Tensor(\n",
      "[[ 0.27388293  0.11168921  0.34511361  0.04551498  0.22379932]\n",
      " [ 0.37078524  0.07891139  0.3009578   0.02622434  0.2231212 ]], shape=(2, 5), dtype=float32)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'decoder_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-b0b85f812dec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mnext_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mnext_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_embeds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mnext_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder_input' is not defined"
     ]
    }
   ],
   "source": [
    "attention = tf.nn.softmax(dot_prod, axis=1) #수정해야함\n",
    "#tf.reduce_sum(attention, axis=1)\n",
    "print(attention.shape)\n",
    "\n",
    "z_plus_e = encoder_glu + input_embeds #why???\n",
    "print(z_plus_e.shape)\n",
    "\n",
    "tiled_attention = tf.tile(attention, [1, 1, embed_size])\n",
    "print(tiled_attention.shape)\n",
    "\n",
    "c = tf.reduce_sum(tiled_attention*z_plus_e, axis=1)\n",
    "print(c.shape)\n",
    "\n",
    "decoder_glu = tf.reshape(decoder_glu, [-1, embed_size])\n",
    "print(decoder_glu.shape)\n",
    "\n",
    "logits = tf.layers.dense(c+decoder_glu, dict_size)\n",
    "print(logits.shape)\n",
    "\n",
    "out = tf.nn.softmax(logits)\n",
    "print(out)\n",
    "\n",
    "next_id = tf.argmax(out, axis=1)\n",
    "next_embeds = tf.nn.embedding_lookup(embeddings, next_id)\n",
    "next_embeds = tf.expand_dims(next_embeds, -1)\n",
    "decoder_input = tf.concat([decoder_input[:, embed_size:], next_embeds], axis=1)\n",
    "\n",
    "next_ids.append(next_id)\n",
    "outs.append(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]]\n",
      "\n",
      " [[ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]\n",
      "  [ 1.]]], shape=(2, 7, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c = tf.reduce_sum(tiled_attention*z_plus_e, axis=1)\n",
    "decoder_glu = tf.reshape(decoder_glu, [-1, embed_size])\n",
    "\n",
    "logits = tf.layers.dense(c+decoder_glu, dict_size)\n",
    "\n",
    "out = tf.nn.softmax(logits) #out으로 묶기\n",
    "\n",
    "next_id = tf.argmax(out, axis=1)\n",
    "next_embeds = tf.nn.embedding_lookup(embeddings, next_id)\n",
    "next_embeds = tf.expand_dims(next_embeds, -1)\n",
    "decoder_input = tf.concat([decoder_input[:, embed_size:], next_embeds], axis=1)\n",
    "\n",
    "next_ids.append(next_id)\n",
    "outs.append(logits)\n",
    "\n",
    "outs = tf.stack(outs, axis=1)\n",
    "next_ids = tf.stack(next_ids, axis=1)\n",
    "\n",
    "if TRAIN:\n",
    "    global_step = tf.train.get_global_step()\n",
    "    loss = tf.losses.softmax_cross_entropy(label_onehot, outs)\n",
    "    train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss, global_step)\n",
    "    estimator_spec = tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            train_op=train_op,\n",
    "            loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 1.60669613  0.58048368  4.32952166 -0.75439    -6.57604551]\n",
      "  [ 2.47091866 -1.72941756 -4.18986034  2.36932921  2.30357647]\n",
      "  [ 2.63007236 -6.39063406 -1.39494836 -2.52273297  6.33367109]\n",
      "  [ 6.15402555 -5.65982628 -2.03500366  2.70822239  1.98549938]\n",
      "  [ 0.19027537  5.78388309  2.20064783 -3.58040476  1.57070911]]\n",
      "\n",
      " [[ 0.87436223 -1.44449735  6.2491827   2.25679016 -9.77695084]\n",
      "  [-0.18215799 -1.39346814  0.43136549  5.30196714  1.92673659]\n",
      "  [ 4.68214655 -7.55460072 -5.41391659 -4.77869511  9.60904503]\n",
      "  [ 7.43560982 -3.8282907  -1.15743423  0.38468164 -0.97345304]\n",
      "  [ 1.20879579  9.34952164  1.27624547 -1.68866849 -2.31034088]]], shape=(2, 5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2 0 4 0 1]\n",
      " [2 3 4 0 1]], shape=(2, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#decoder\n",
    "next_ids = []\n",
    "outs = []\n",
    "for l in range(label_length):\n",
    "    if l == 0: decoder_input = init_label\n",
    "\n",
    "    decoder_conv = tf.layers.conv1d(\n",
    "            inputs=init_label,\n",
    "            filters=2*embed_size,\n",
    "            kernel_size=filter_size,\n",
    "            strides=embed_size)\n",
    "\n",
    "\n",
    "    decoder_glu = decoder_conv[:, :, embed_size:]*tf.nn.sigmoid(decoder_conv[:, :, :embed_size])\n",
    "\n",
    "    tiled_decoder_glu = tf.tile(decoder_glu, [1, int(encoder_glu.shape[1]), 1])\n",
    "\n",
    "\n",
    "    dot_prod = tf.matmul(encoder_glu, decoder_glu, transpose_b=True)\n",
    "\n",
    "    attention = tf.nn.softmax(dot_prod)\n",
    "\n",
    "    z_plus_e = encoder_glu + input_embeds\n",
    "\n",
    "    tiled_attention = tf.tile(attention, [1, 1, embed_size])\n",
    "\n",
    "    c = tf.reduce_sum(tiled_attention*z_plus_e, axis=1)\n",
    "    decoder_glu = tf.reshape(decoder_glu, [-1, embed_size])\n",
    "\n",
    "    logits = tf.layers.dense(c+decoder_glu, dict_size)\n",
    "\n",
    "    out = tf.nn.softmax(logits)\n",
    "\n",
    "    next_id = tf.argmax(out, axis=1)\n",
    "    next_embeds = tf.nn.embedding_lookup(embeddings, next_id)\n",
    "    next_embeds = tf.expand_dims(next_embeds, -1)\n",
    "    decoder_input = tf.concat([decoder_input[:, embed_size:], next_embeds], axis=1) #옆에 붙이자, 기존꺼 버리고..\n",
    "\n",
    "    next_ids.append(next_id)\n",
    "    outs.append(logits)\n",
    "\n",
    "outs = tf.stack(outs, axis=1)\n",
    "next_ids = tf.stack(next_ids, axis=1)\n",
    "\n",
    "print(outs)\n",
    "print(next_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`loss` passed to Optimizer.compute_gradients should be a function when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-e143c1799c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_onehot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf_nightly/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    419\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         raise RuntimeError(\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0;34m\"`loss` passed to Optimizer.compute_gradients should \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m             \"be a function when eager execution is enabled.\")\n\u001b[1;32m    423\u001b[0m       \u001b[0;31m# TODO(agarwal): consider passing parameters to the `loss` function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `loss` passed to Optimizer.compute_gradients should be a function when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "loss = tf.losses.softmax_cross_entropy(label_onehot, outs)\n",
    "train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_glu = encoder_conv[:, :, embed_size:]*tf.nn.sigmoid(encoder_conv[:, :, :embed_size])\n",
    "\n",
    "#decoder\n",
    "next_ids = []\n",
    "outs = []\n",
    "for l in range(features['label_length']):\n",
    "    if l == 0: decoder_input = init_label\n",
    "\n",
    "    decoder_conv = tf.layers.conv1d(\n",
    "            inputs=init_label,\n",
    "            filters=2*embed_size,\n",
    "            kernel_size=filter_size,\n",
    "            strides=embed_size)\n",
    "\n",
    "\n",
    "    decoder_glu = decoder_conv[:, :, embed_size:]*tf.nn.sigmoid(decoder_conv[:, :, :embed_size])\n",
    "\n",
    "    tiled_decoder_glu = tf.tile(decoder_glu, [1, int(encoder_glu.shape[1]), 1])\n",
    "\n",
    "\n",
    "    dot_prod = tf.matmul(encoder_glu, decoder_glu, transpose_b=True)\n",
    "\n",
    "    attention = tf.nn.softmax(dot_prod)\n",
    "\n",
    "    z_plus_e = encoder_glu + input_embeds\n",
    "\n",
    "    tiled_attention = tf.tile(attention, [1, 1, embed_size])\n",
    "\n",
    "    c = tf.reduce_sum(tiled_attention*z_plus_e, axis=1)\n",
    "    decoder_glu = tf.reshape(decoder_glu, [-1, embed_size])\n",
    "\n",
    "    logits = tf.layers.dense(c+decoder_glu, dict_size)\n",
    "\n",
    "    out = tf.nn.softmax(logits)\n",
    "\n",
    "    next_id = tf.argmax(out, axis=1)\n",
    "    next_embeds = tf.nn.embedding_lookup(embeddings, next_id)\n",
    "    next_embeds = tf.expand_dims(next_embeds, -1)\n",
    "    decoder_input = tf.concat([decoder_input[:, embed_size:], next_embeds], axis=1) #옆에 붙이자, 기존꺼 버리고..\n",
    "\n",
    "    next_ids.append(next_id)\n",
    "    outs.append(logits)\n",
    "\n",
    "outs = tf.stack(outs, axis=1)\n",
    "next_ids = tf.stack(next_ids, axis=1)\n",
    "\n",
    "if TRAIN:\n",
    "    global_step = tf.train.get_global_step()\n",
    "    loss = tf.losses.softmax_cross_entropy(label_onehot, outs)\n",
    "    train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss, global_step)\n",
    "    estimator_spec = tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            train_op=train_op,\n",
    "            loss=loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FATAL Flags parsing error: Unknown command line flag 'f'\n",
      "Pass --helpshort or --helpfull to see help on flags.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naver/tf_nightly/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def model_fn(mode, features, labels):\n",
    "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
    "\n",
    "    embeddings = tf.Variable(tf.truncated_normal([dict_size, embed_size]), trainable=False)\n",
    "\n",
    "    # embedding\n",
    "    input_embeds = tf.nn.embedding_lookup(embeddings, features['ids'])\n",
    "\n",
    "    input_flat = tf.layers.flatten(input_embeds)\n",
    "    input_flat = tf.expand_dims(input_flat, -1)\n",
    "\n",
    "    init_label = tf.nn.embedding_lookup(embeddings, features['init_label'])\n",
    "    init_label = tf.layers.flatten(init_label)\n",
    "    init_label = tf.expand_dims(init_label, -1)\n",
    "\n",
    "    if not PREDICT:\n",
    "        label_onehot = tf.one_hot(labels, depth=dict_size, dtype=tf.float32)\n",
    "\n",
    "    # encoder\n",
    "    encoder_conv = tf.layers.conv1d(\n",
    "            inputs=input_flat,\n",
    "            filters=2*embed_size,\n",
    "            kernel_size=filter_size,\n",
    "            strides=embed_size,\n",
    "            padding='same')\n",
    "\n",
    "    encoder_glu = encoder_conv[:, :, embed_size:]*tf.nn.sigmoid(encoder_conv[:, :, :embed_size])\n",
    "\n",
    "\n",
    "\n",
    "        decoder_glu = decoder_conv[:, :, embed_size:]*tf.nn.sigmoid(decoder_conv[:, :, :embed_size])\n",
    "\n",
    "        tiled_decoder_glu = tf.tile(decoder_glu, [1, int(encoder_glu.shape[1]), 1])\n",
    "\n",
    "\n",
    "        dot_prod = tf.matmul(encoder_glu, decoder_glu, transpose_b=True)\n",
    "\n",
    "        attention = tf.nn.softmax(dot_prod)\n",
    "\n",
    "        z_plus_e = encoder_glu + input_embeds\n",
    "\n",
    "        tiled_attention = tf.tile(attention, [1, 1, embed_size])\n",
    "\n",
    "        c = tf.reduce_sum(tiled_attention*z_plus_e, axis=1)\n",
    "        decoder_glu = tf.reshape(decoder_glu, [-1, embed_size])\n",
    "\n",
    "        logits = tf.layers.dense(c+decoder_glu, dict_size)\n",
    "\n",
    "        out = tf.nn.softmax(logits)\n",
    "\n",
    "        next_id = tf.argmax(out, axis=1)\n",
    "        next_embeds = tf.nn.embedding_lookup(embeddings, next_id)\n",
    "        next_embeds = tf.expand_dims(next_embeds, -1)\n",
    "        decoder_input = tf.concat([decoder_input[:, embed_size:], next_embeds], axis=1)\n",
    "\n",
    "        next_ids.append(next_id)\n",
    "        outs.append(logits)\n",
    "\n",
    "    outs = tf.stack(outs, axis=1)\n",
    "    next_ids = tf.stack(next_ids, axis=1)\n",
    "\n",
    "    if TRAIN:\n",
    "        global_step = tf.train.get_global_step()\n",
    "        loss = tf.losses.softmax_cross_entropy(label_onehot, outs)\n",
    "        train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss, global_step)\n",
    "        estimator_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                train_op=train_op,\n",
    "                loss=loss)\n",
    "\n",
    "    elif EVAL:\n",
    "        loss = tf.losses.softmax_cross_entropy(label_onehot, outs)\n",
    "        eval_metric_ops = {'acc': tf.metrics.accuracy(labels, next_ids)}\n",
    "        estimator_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=loss,\n",
    "                eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "    elif PREDICT:\n",
    "        estimator_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions={'prediction': next_ids})\n",
    "\n",
    "    else:\n",
    "        raise Exception('estiamtor spec is invalid')\n",
    "\n",
    "    return estimator_spec\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    fairseq = tf.estimator.Estimator(model_fn)\n",
    "    fairseq.train(input_fn, steps=10000)\n",
    "    fairseq.evaluate(input_fn, steps=10)\n",
    "\n",
    "    pred = fairseq.predict(input_fn)\n",
    "    for p in pred:\n",
    "        print('prediction: {}'.format(p['prediction']))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
