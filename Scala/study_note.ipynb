{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaa\n",
    "\n",
    "## 둘다 Play\n",
    "\n",
    "## @Singleton?\n",
    "\n",
    "## Guice\n",
    "\n",
    " - New 대신 사용하는 Guice\n",
    " \n",
    "## injector\n",
    "\n",
    "## Apply\n",
    "\n",
    "## 왜 굳이 Private? 함수 안에 있으면 local function 아님?\n",
    "\n",
    "## Future의 역할?\n",
    "\n",
    "## Await는?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baloo\n",
    "\n",
    "## TrainingSetBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mbools\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mBoolean\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32mtrue\u001b[39m, \u001b[32mfalse\u001b[39m)\n",
       "\u001b[36mres0_1\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mUnit\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m()\u001b[39m, \u001b[32m()\u001b[39m)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// try / catch / case\n",
    "\n",
    "val bools = Seq(true, false)\n",
    "bools.map { x =>\n",
    "x match {\n",
    "  case true => println(\"true\")\n",
    "  case false => println(\"false\")\n",
    "}\n",
    "}\n",
    "\n",
    "val bools = Seq(true, false)\n",
    "bools.map {\n",
    "  case true => println(\"true\")\n",
    "  case false => println(\"false\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mbools\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mBoolean\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32mtrue\u001b[39m, \u001b[32mfalse\u001b[39m)\n",
       "\u001b[36mres1_1\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mUnit\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m()\u001b[39m, \u001b[32m()\u001b[39m)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// flatten\n",
    "\n",
    "// coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-3.0`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.3.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n",
    "\n",
    "val sparkSession = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  // .yarn(\"/etc/hadoop/conf\") // optional, for Spark on YARN - argument is the Hadoop conf directory\n",
    "  // .emr(\"2.6.4\") // on AWS ElasticMapReduce, this adds aws-related to the spark jar list\n",
    "  .master(\"local\") // change to \"yarn-client\" on YARN\n",
    "  // .config(\"spark.executor.instances\", \"10\")\n",
    "  // .config(\"spark.executor.memory\", \"3g\")\n",
    "  // .config(\"spark.hadoop.fs.s3a.access.key\", awsCredentials._1)\n",
    "  // .config(\"spark.hadoop.fs.s3a.secret.key\", awsCredentials._2)\n",
    "  .appName(\"notebook\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{SaveMode, SparkSession}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Properties\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{SaveMode, SparkSession}\n",
    "import java.util.Properties\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mappName\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"dataframeSaveDatabase\"\u001b[39m\n",
       "\u001b[36mmaster\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"local[*]\"\u001b[39m\n",
       "\u001b[36mconf\u001b[39m: \u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@4f4fef3d"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val appName = \"dataframeSaveDatabase\"\n",
    "val master = \"local[*]\"\n",
    "val conf = new SparkConf().setAppName(appName).setMaster(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@324c1ac0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  // .yarn(\"/etc/hadoop/conf\") // optional, for Spark on YARN - argument is the Hadoop conf directory\n",
    "  // .emr(\"2.6.4\") // on AWS ElasticMapReduce, this adds aws-related to the spark jar list\n",
    "  .master(master) // change to \"yarn-client\" on YARN\n",
    "  .appName(appName)\n",
    "  .config(\"spark.some.config.option\", \"config-value\")\n",
    "  // .config(\"spark.executor.instances\", \"10\")\n",
    "  // .config(\"spark.executor.memory\", \"3g\")\n",
    "  // .config(\"spark.hadoop.fs.s3a.access.key\", awsCredentials._1)\n",
    "  // .config(\"spark.hadoop.fs.s3a.secret.key\", awsCredentials._2)\n",
    "  .getOrCreate()\n",
    "\n",
    "val sqlContext = sc.sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msc.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m\"romance\"\u001b[39m, \u001b[32m2016\u001b[39m, \u001b[32m100\u001b[39m),\n",
       "  (\u001b[32m\"romance\"\u001b[39m, \u001b[32m2017\u001b[39m, \u001b[32m200\u001b[39m),\n",
       "  (\u001b[32m\"general\"\u001b[39m, \u001b[32m2015\u001b[39m, \u001b[32m50\u001b[39m),\n",
       "  (\u001b[32m\"general\"\u001b[39m, \u001b[32m2016\u001b[39m, \u001b[32m150\u001b[39m),\n",
       "  (\u001b[32m\"fantasy\"\u001b[39m, \u001b[32m2017\u001b[39m, \u001b[32m50\u001b[39m)\n",
       ")\n",
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [genre: string, year: int ... 1 more field]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// RDD/Dataframe/Dataset 참고자료 https://wikidocs.net/14260\n",
    "\n",
    "import sc.implicits._\n",
    "\n",
    "val data = \n",
    "  Seq(\n",
    "    (\"romance\", 2016, 100),\n",
    "    (\"romance\", 2017, 200),\n",
    "    (\"general\", 2015, 50),\n",
    "    (\"general\", 2016, 150),\n",
    "    (\"fantasy\", 2017, 50))\n",
    "\n",
    "val df = data.toDF(\"genre\", \"year\", \"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- genre: string (nullable = true)\n",
      " |-- year: integer (nullable = false)\n",
      " |-- price: integer (nullable = false)\n",
      "\n",
      "+-------+----+-----+\n",
      "|  genre|year|price|\n",
      "+-------+----+-----+\n",
      "|romance|2016|  100|\n",
      "|romance|2017|  200|\n",
      "|general|2015|   50|\n",
      "|general|2016|  150|\n",
      "|fantasy|2017|   50|\n",
      "+-------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2.생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- genre: string (nullable = true)\n",
      " |-- year: integer (nullable = false)\n",
      " |-- price: integer (nullable = false)\n",
      "\n",
      "+-------+----+-----+\n",
      "|  genre|year|price|\n",
      "+-------+----+-----+\n",
      "|romance|2016|  100|\n",
      "|romance|2017|  200|\n",
      "|general|2015|   50|\n",
      "|general|2016|  150|\n",
      "|fantasy|2017|   50|\n",
      "+-------+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcolNames\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"genre\"\u001b[39m, \u001b[32m\"year\"\u001b[39m, \u001b[32m\"price\"\u001b[39m)\n",
       "\u001b[36mdf2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [genre: string, year: int ... 1 more field]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colNames: Array[String] = df.columns\n",
    "val df2 = data.toDF(colNames: _*)\n",
    "\n",
    "df2.printSchema\n",
    "df2.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- genre: string (nullable = true)\n",
      " |-- year: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.io.IOException: Failed to connect to /10.37.129.2:57196",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$downloadClient(NettyRpcEnv.scala:368)",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply$mcV$sp(NettyRpcEnv.scala:336)",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:339)",
      "\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:661)",
      "\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:485)",
      "\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:755)",
      "\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:747)",
      "\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)",
      "\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)",
      "\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)",
      "\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:130)",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)",
      "\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:747)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:312)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "\tSuppressed: java.lang.NullPointerException",
      "\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1419)",
      "\t\t... 17 more",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Operation timed out: /10.37.129.2:57196",
      "\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)",
      "\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)",
      "\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)",
      "\t... 1 more",
      "Caused by: java.net.ConnectException: Operation timed out",
      "\t... 11 more",
      "",
      "Driver stacktrace:\u001b[39m",
      "  org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1599\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1587\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1586\u001b[39m)",
      "  scala.collection.mutable.ResizableArray$class.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m59\u001b[39m)",
      "  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m48\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1586\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m831\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m831\u001b[39m)",
      "  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m257\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m831\u001b[39m)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1820\u001b[39m)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1769\u001b[39m)",
      "  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1758\u001b[39m)",
      "  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m48\u001b[39m)",
      "  org.apache.spark.scheduler.DAGScheduler.runJob(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m642\u001b[39m)",
      "  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2027\u001b[39m)",
      "  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2048\u001b[39m)",
      "  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2067\u001b[39m)",
      "  org.apache.spark.sql.execution.SparkPlan.executeTake(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m363\u001b[39m)",
      "  org.apache.spark.sql.execution.CollectLimitExec.executeCollect(\u001b[32mlimit.scala\u001b[39m:\u001b[32m38\u001b[39m)",
      "  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3272\u001b[39m)",
      "  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2484\u001b[39m)",
      "  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2484\u001b[39m)",
      "  org.apache.spark.sql.Dataset$$anonfun$52.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3253\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m77\u001b[39m)",
      "  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3252\u001b[39m)",
      "  org.apache.spark.sql.Dataset.head(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2484\u001b[39m)",
      "  org.apache.spark.sql.Dataset.take(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2698\u001b[39m)",
      "  org.apache.spark.sql.Dataset.showString(\u001b[32mDataset.scala\u001b[39m:\u001b[32m254\u001b[39m)",
      "  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m723\u001b[39m)",
      "  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m682\u001b[39m)",
      "  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m691\u001b[39m)",
      "  $sess.cmd19Wrapper$Helper.<init>(\u001b[32mcmd19.sc\u001b[39m:\u001b[32m16\u001b[39m)",
      "  $sess.cmd19Wrapper.<init>(\u001b[32mcmd19.sc\u001b[39m:\u001b[32m501\u001b[39m)",
      "  $sess.cmd19$.<init>(\u001b[32mcmd19.sc\u001b[39m:\u001b[32m349\u001b[39m)",
      "  $sess.cmd19$.<clinit>(\u001b[32mcmd19.sc\u001b[39m:\u001b[32m-1\u001b[39m)",
      "\u001b[31mjava.io.IOException: Failed to connect to /10.37.129.2:57196\u001b[39m",
      "  org.apache.spark.network.client.TransportClientFactory.createClient(\u001b[32mTransportClientFactory.java\u001b[39m:\u001b[32m245\u001b[39m)",
      "  org.apache.spark.network.client.TransportClientFactory.createClient(\u001b[32mTransportClientFactory.java\u001b[39m:\u001b[32m187\u001b[39m)",
      "  org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$downloadClient(\u001b[32mNettyRpcEnv.scala\u001b[39m:\u001b[32m368\u001b[39m)",
      "  org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply$mcV$sp(\u001b[32mNettyRpcEnv.scala\u001b[39m:\u001b[32m336\u001b[39m)",
      "  org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(\u001b[32mNettyRpcEnv.scala\u001b[39m:\u001b[32m335\u001b[39m)",
      "  org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(\u001b[32mNettyRpcEnv.scala\u001b[39m:\u001b[32m335\u001b[39m)",
      "  org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1411\u001b[39m)",
      "  org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(\u001b[32mNettyRpcEnv.scala\u001b[39m:\u001b[32m339\u001b[39m)",
      "  org.apache.spark.util.Utils$.doFetchFile(\u001b[32mUtils.scala\u001b[39m:\u001b[32m661\u001b[39m)",
      "  org.apache.spark.util.Utils$.fetchFile(\u001b[32mUtils.scala\u001b[39m:\u001b[32m485\u001b[39m)",
      "  org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m755\u001b[39m)",
      "  org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m747\u001b[39m)",
      "  scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m733\u001b[39m)",
      "  scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(\u001b[32mHashMap.scala\u001b[39m:\u001b[32m130\u001b[39m)",
      "  scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(\u001b[32mHashMap.scala\u001b[39m:\u001b[32m130\u001b[39m)",
      "  scala.collection.mutable.HashTable$class.foreachEntry(\u001b[32mHashTable.scala\u001b[39m:\u001b[32m236\u001b[39m)",
      "  scala.collection.mutable.HashMap.foreachEntry(\u001b[32mHashMap.scala\u001b[39m:\u001b[32m40\u001b[39m)",
      "  scala.collection.mutable.HashMap.foreach(\u001b[32mHashMap.scala\u001b[39m:\u001b[32m130\u001b[39m)",
      "  scala.collection.TraversableLike$WithFilter.foreach(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m732\u001b[39m)",
      "  org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m747\u001b[39m)",
      "  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m312\u001b[39m)",
      "  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1149\u001b[39m)",
      "  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m624\u001b[39m)",
      "  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m748\u001b[39m)",
      "\u001b[31mio.netty.channel.AbstractChannel$AnnotatedConnectException: Operation timed out: /10.37.129.2:57196\u001b[39m",
      "  sun.nio.ch.SocketChannelImpl.checkConnect(\u001b[32mNative Method\u001b[39m)",
      "  sun.nio.ch.SocketChannelImpl.finishConnect(\u001b[32mSocketChannelImpl.java\u001b[39m:\u001b[32m717\u001b[39m)",
      "  io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(\u001b[32mNioSocketChannel.java\u001b[39m:\u001b[32m323\u001b[39m)",
      "  io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(\u001b[32mAbstractNioChannel.java\u001b[39m:\u001b[32m340\u001b[39m)",
      "  io.netty.channel.nio.NioEventLoop.processSelectedKey(\u001b[32mNioEventLoop.java\u001b[39m:\u001b[32m633\u001b[39m)",
      "  io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(\u001b[32mNioEventLoop.java\u001b[39m:\u001b[32m580\u001b[39m)",
      "  io.netty.channel.nio.NioEventLoop.processSelectedKeys(\u001b[32mNioEventLoop.java\u001b[39m:\u001b[32m497\u001b[39m)",
      "  io.netty.channel.nio.NioEventLoop.run(\u001b[32mNioEventLoop.java\u001b[39m:\u001b[32m459\u001b[39m)",
      "  io.netty.util.concurrent.SingleThreadEventExecutor$5.run(\u001b[32mSingleThreadEventExecutor.java\u001b[39m:\u001b[32m858\u001b[39m)",
      "  io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(\u001b[32mDefaultThreadFactory.java\u001b[39m:\u001b[32m138\u001b[39m)",
      "  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m748\u001b[39m)",
      "\u001b[31mjava.net.ConnectException: Operation timed out\u001b[39m",
      "  sun.nio.ch.SocketChannelImpl.checkConnect(\u001b[32mNative Method\u001b[39m)",
      "  sun.nio.ch.SocketChannelImpl.finishConnect(\u001b[32mSocketChannelImpl.java\u001b[39m:\u001b[32m717\u001b[39m)",
      "  io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(\u001b[32mNioSocketChannel.java\u001b[39m:\u001b[32m323\u001b[39m)",
      "  io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(\u001b[32mAbstractNioChannel.java\u001b[39m:\u001b[32m340\u001b[39m)",
      "  io.netty.channel.nio.NioEventLoop.processSelectedKey(\u001b[32mNioEventLoop.java\u001b[39m:\u001b[32m633\u001b[39m)",
      "  io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(\u001b[32mNioEventLoop.java\u001b[39m:\u001b[32m580\u001b[39m)",
      "  io.netty.channel.nio.NioEventLoop.processSelectedKeys(\u001b[32mNioEventLoop.java\u001b[39m:\u001b[32m497\u001b[39m)",
      "  io.netty.channel.nio.NioEventLoop.run(\u001b[32mNioEventLoop.java\u001b[39m:\u001b[32m459\u001b[39m)",
      "  io.netty.util.concurrent.SingleThreadEventExecutor$5.run(\u001b[32mSingleThreadEventExecutor.java\u001b[39m:\u001b[32m858\u001b[39m)",
      "  io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(\u001b[32mDefaultThreadFactory.java\u001b[39m:\u001b[32m138\u001b[39m)",
      "  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m748\u001b[39m)"
     ]
    }
   ],
   "source": [
    "df.createTempView(\"temp_view\")\n",
    "\n",
    "val dfQ =\n",
    "  sc.sqlContext.sql(\n",
    "  \"\"\"\n",
    "    |select\n",
    "    |  genre,\n",
    "    |  year\n",
    "    |from temp_view\n",
    "    |where genre = 'general'\n",
    "    |  and year in (2015, 2016)\n",
    "    |  and price >= 100\n",
    "  \"\"\".stripMargin)\n",
    "\n",
    "dfQ.printSchema\n",
    "dfQ.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// http://helloino.tistory.com/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msc.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mvalues\u001b[39m: \u001b[32mList\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m((\u001b[32m\"hi\"\u001b[39m, \u001b[32m\"2018-07-05\"\u001b[39m, \u001b[32m\"2018-08-05\"\u001b[39m))\n",
       "\u001b[36mdataFrame\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_id: string, join_date: string ... 1 more field]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val values = List((\"hi\", \"2018-07-05\", \"2018-08-05\"))\n",
    "val dataFrame = values.toDF(\"user_id\", \"join_date\", \"event_date\")\n",
    "\n",
    "val properties = new Properties()\n",
    "properties.put(\"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd27.sc:2: value sparkSession is not a member of org.apache.spark.sql.SparkSession\n",
      "val res27_1 = sparkSession.sparkSession\n",
      "                           ^"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "// import org.apache.spark.SparkContext\n",
    "// import org.apache.spark.SparkConf\n",
    "\n",
    "val sc = sparkSession.sparkContext\n",
    "// sparkSession.sparkSession\n",
    "// import ai.company.cai.khan.utils.Config\n",
    "// import com.soundcloud.lsh.Lsh\n",
    "// import org.apache.spark.SparkContext\n",
    "// import org.apache.spark.mllib.clustering.KMeans\n",
    "// import org.apache.spark.mllib.linalg.Vectors\n",
    "// import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix}\n",
    "// import org.apache.spark.rdd.RDD\n",
    "// import org.apache.spark.sql.{Dataset, SparkSession}\n",
    "// import org.apache.spark.storage.StorageLevel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m)\n",
       "\u001b[36mdistData\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[3] at parallelize at cmd39.sc:2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Array(1, 2, 3, 4, 5)\n",
    "val distData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres40\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[3] at parallelize at cmd39.sc:2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres29\u001b[39m: \u001b[32mClass\u001b[39m[\u001b[32m?0\u001b[39m] = class [I"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.getClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[2] at parallelize at cmd38.sc:1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msparkSession.implicits._\u001b[39m"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparkSession.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres36\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mInt\u001b[39m] = [value: int]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.createDataset(Seq(1,2))\n",
    "\n",
    "// val rows = spark.sparkContext.parallelize(Seq(Row.fromSeq(Seq(1, 2))))\n",
    "// val schema = StructType(Seq(\"col1\", \"col2\").map(col => StructField(col, IntegerType, nullable = false)))\n",
    "// spark.createDataFrame(rows, schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres37\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mInt\u001b[39m] = [value: int]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
