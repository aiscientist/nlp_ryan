{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaa\n",
    "\n",
    "## 둘다 Play\n",
    "\n",
    "## @Singleton?\n",
    "\n",
    "## Guice\n",
    "\n",
    " - New 대신 사용하는 Guice\n",
    " \n",
    "## injector\n",
    "\n",
    "## Apply\n",
    "\n",
    "## 왜 굳이 Private? 함수 안에 있으면 local function 아님?\n",
    "\n",
    "## Future의 역할?\n",
    "\n",
    "## Await는?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baloo\n",
    "\n",
    "## TrainingSetBuilder\n",
    "\n",
    " - case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd0.sc:9: bools is already defined as value bools\n",
      "val bools = Seq(true, false)\n",
      "    ^"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "// try / catch / case\n",
    "\n",
    "val bools = Seq(true, false)\n",
    "\n",
    "bools.map { x =>\n",
    "x match {\n",
    "  case true => println(\"true\")\n",
    "  case false => println(\"false\")\n",
    "}\n",
    "}\n",
    "\n",
    "val bools = Seq(true, false)\n",
    "bools.map {\n",
    "  case true => println(\"true\")\n",
    "  case false => println(\"false\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// flatten\n",
    "\n",
    "// coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36msparkSession\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@2f7ac343"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-3.0`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.3.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n",
    "\n",
    "val sparkSession = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  // .yarn(\"/etc/hadoop/conf\") // optional, for Spark on YARN - argument is the Hadoop conf directory\n",
    "  // .emr(\"2.6.4\") // on AWS ElasticMapReduce, this adds aws-related to the spark jar list\n",
    "  .master(\"local\") // change to \"yarn-client\" on YARN\n",
    "  // .config(\"spark.executor.instances\", \"10\")\n",
    "  // .config(\"spark.executor.memory\", \"3g\")\n",
    "  // .config(\"spark.hadoop.fs.s3a.access.key\", awsCredentials._1)\n",
    "  // .config(\"spark.hadoop.fs.s3a.secret.key\", awsCredentials._2)\n",
    "  .appName(\"notebook\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{SaveMode, SparkSession}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Properties\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mappName\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"dataframeSaveDatabase\"\u001b[39m\n",
       "\u001b[36mmaster\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"local[*]\"\u001b[39m\n",
       "\u001b[36mconf\u001b[39m: \u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@bbf2150\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@2f7ac343\n",
       "\u001b[36msqlContext\u001b[39m: \u001b[32mSQLContext\u001b[39m = org.apache.spark.sql.SQLContext@1a7bb2a4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{SaveMode, SparkSession}\n",
    "import java.util.Properties\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val appName = \"dataframeSaveDatabase\"\n",
    "val master = \"local[*]\"\n",
    "val conf = new SparkConf().setAppName(appName).setMaster(master)\n",
    "\n",
    "val sc = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  // .yarn(\"/etc/hadoop/conf\") // optional, for Spark on YARN - argument is the Hadoop conf directory\n",
    "  // .emr(\"2.6.4\") // on AWS ElasticMapReduce, this adds aws-related to the spark jar list\n",
    "  .master(master) // change to \"yarn-client\" on YARN\n",
    "  .appName(appName)\n",
    "  .config(\"spark.some.config.option\", \"config-value\")\n",
    "  // .config(\"spark.executor.instances\", \"10\")\n",
    "  // .config(\"spark.executor.memory\", \"3g\")\n",
    "  // .config(\"spark.hadoop.fs.s3a.access.key\", awsCredentials._1)\n",
    "  // .config(\"spark.hadoop.fs.s3a.secret.key\", awsCredentials._2)\n",
    "  .getOrCreate()\n",
    "\n",
    "val sqlContext = sc.sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd14.sc:11: value toDF is not a member of Seq[(String, Int, Int)]\n",
      "val df = data.toDF(\"genre\", \"year\", \"price\")\n",
      "              ^cmd14.sc:17: value toDF is not a member of Seq[(String, Int, Int)]\n",
      "val df2 = data.toDF(colNames: _*)\n",
      "               ^"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "// RDD/Dataframe/Dataset 참고자료 https://wikidocs.net/14260\n",
    "\n",
    "import sc.implicits._\n",
    "\n",
    "val data = \n",
    "  Seq(\n",
    "    (\"romance\", 2016, 100),\n",
    "    (\"romance\", 2017, 200),\n",
    "    (\"general\", 2015, 50),\n",
    "    (\"general\", 2016, 150),\n",
    "    (\"fantasy\", 2017, 50))\n",
    "\n",
    "val df = data.toDF(\"genre\", \"year\", \"price\")\n",
    "\n",
    "df.printSchema\n",
    "df.show\n",
    "\n",
    "val colNames: Array[String] = df.columns\n",
    "val df2 = data.toDF(colNames: _*)\n",
    "\n",
    "df2.printSchema\n",
    "df2.show\n",
    "\n",
    "df.createTempView(\"temp_view\")\n",
    "\n",
    "val dfQ =\n",
    "  sc.sqlContext.sql(\n",
    "  \"\"\"\n",
    "    |select\n",
    "    |  genre,\n",
    "    |  year\n",
    "    |from temp_view\n",
    "    |where genre = 'general'\n",
    "    |  and year in (2015, 2016)\n",
    "    |  and price >= 100\n",
    "  \"\"\".stripMargin)\n",
    "\n",
    "dfQ.printSchema\n",
    "dfQ.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                            \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mbreeze.linalg.{DenseMatrix, DenseVector}\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.scalanlp::breeze:0.13.2`\n",
    "\n",
    "import breeze.linalg.{DenseMatrix, DenseVector}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmain\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fullpath = s\"/Users/user/NCSDrive/Work/dssm/dssm_tf/data_in/model=dssm/table=CIDSMATRIXNORMS_dssm.kryo/part-1\"\n",
    "val param = sc.read.parquet(fullpath).as[(Array[String], DenseMatrix[Double], DenseVector[Double])].collect().map(x =>println(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package ai.company\n",
    "\n",
    "import java.io.{File, FileInputStream, FileNotFoundException}\n",
    "import java.net.{HttpURLConnection, URL}\n",
    "import java.nio.file.{Files, Paths}\n",
    "import java.util.concurrent.atomic.AtomicLong\n",
    "\n",
    "import ai.company.akka.SystemInfoStateParam\n",
    "import ai.company.ari.repo.chat.{BuildEntity, ChatcontentEntity, ChatsessionEntity, ChatutterancesetEntity}\n",
    "import ai.company.ari.repo.domain.Parameters\n",
    "import ai.company.baloo.env.{BPaths, SystemInfo}\n",
    "import ai.company.baloo.env.spark.Spark\n",
    "import ai.company.baloo.kaa.AfterTrainingWithPB\n",
    "import ai.company.baloo.nlp.NLPUtil\n",
    "import ai.company.baloo.tf.{Seq2Seq, TensorFlowProvider, TrainingSetBuilder}\n",
    "import ai.company.baloo.tf.TrainingSetBuilder.{NHotData, QAGroup, TrainingSetPreparation, df2ds, loadC3Text, log, nhotProcess, reservedTerms, schema, udfAttachEOS, zipWithIndex}\n",
    "import ai.company.baloo.util.SeriDeseri\n",
    "import breeze.linalg.{DenseMatrix, DenseVector}\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SaveMode}\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "import scala.concurrent.Future\n",
    "\n",
    "object ReadPB {\n",
    " def main(args: Array[String]): Unit = {\n",
    "   val spark = Spark.spark\n",
    "   import spark.implicits._\n",
    "\n",
    "   val fullPath = \"/home1/irteam/user/companyai/data/db/host=dev2.chatbot.naver.com/schema=mogli2tool/domainId=5008/dt=2018-07-23T06-34-11.104Z/nlp=kmatapidssm/model=dssm\"\n",
    "\n",
    "   val parameters = Spark.spark.read.parquet(fullPath + \"/table=PARAMETERS\").as[Parameters].collect()\n",
    "   println(\":>>>>>>>>>>>>>>>>>>\", parameters.head.outputLayerName)\n",
    "\n",
    "   val datePath = fullPath.slice(fullPath.lastIndexOf('/') + 4, fullPath.length)\n",
    "\n",
    "   val nhots = parameters.head.use_nhot\n",
    "\n",
    "   val vocabTapi: Map[String, Array[Float]] = Spark.spark.read.text(fullPath + \"/table=VOCABQIDTAPI.text\").as[String].map(x => {\n",
    "     val spl = x.split(\"\\u241D\")\n",
    "     (spl.head, spl.last.split(\"\\u241E\").map(x => x.toDouble.toFloat))\n",
    "   }).collect.toMap\n",
    "\n",
    "   val filterTarget = TrainingSetBuilder.loadC3Text(fullPath.substring(0, fullPath.lastIndexOf(\"/nlp=\")), \"ANSWERQIDS.text\")\n",
    "   val cidVocab = {\n",
    "     val cv: DataFrame = Spark.spark.read.text(fullPath + \"/table=CIDVOCABQIDS.text\").as[String].map(x => {\n",
    "       val spl = x.split(\"\\u241D\")\n",
    "       (spl.head, spl.last)\n",
    "     }).toDF(\"cid\", \"tvec\")\n",
    "\n",
    "     if (filterTarget.size > 0) {\n",
    "       println(s\"ANSWERQID exists ${filterTarget.size}\")\n",
    "       log.info(s\"ANSWERQID exists ${filterTarget.size}\")\n",
    "       cv.join(filterTarget.toDF(\"cid\"), \"cid\")\n",
    "     } else {\n",
    "       println(\"ANSWERQID NOT exists\")\n",
    "       log.info(\"ANSWERQID NOT exists\")\n",
    "       cv\n",
    "     }\n",
    "   }\n",
    "\n",
    "   val cidNHOTS: Array[DataFrame] = nhots.map(x => Spark.spark.read.text(fullPath + s\"/table=CID${x}QIDS.text\").as[String].map(x => {\n",
    "     val spl = x.split(\"\\u241D\")\n",
    "     (spl.head, spl.last)\n",
    "   }).toDF(\"cid\", s\"${x}vec\"))\n",
    "   val total: DataFrame = cidNHOTS.foldLeft(cidVocab) { (prev, next) => prev.join(next, \"cid\").drop(next(\"cid\")) }\n",
    "\n",
    "   val pbPath = \"/Users/user/Desktop/sim_model.pb\"\n",
    "\n",
    "   val pbByte = Files.readAllBytes(Paths.get(pbPath))\n",
    "\n",
    "   val seq = new Seq2Seq(pbByte, 1, 128)\n",
    "   val tf = new TensorFlowProvider(seq)\n",
    "\n",
    "   //    println(seq)\n",
    "   //    println(tf)\n",
    "\n",
    "   val qvec = total.collect().par.map(x => {\n",
    "     val cid = x.getAs[String](\"cid\")\n",
    "     val tvec: Array[Array[Float]] = x.getAs[String](\"tvec\").split(\"\\u241E\").map(y => vocabTapi(y))\n",
    "     val nvec = nhots.map(n => x.getAs[String](s\"${n}vec\").split(\"\\u241E\").map(y => y.toInt))\n",
    "     (cid, AfterTrainingWithPB.predictDssm(tvec, nvec, tf, parameters.head))\n",
    "   }).seq.toArray\n",
    "   val qvecText = qvec.map(x => s\"${x._1}\\u241D${x._2.mkString(\"\\u241E\")}\")\n",
    "\n",
    "   val qvecCol: Array[(String, Array[Double])] = qvec\n",
    "\n",
    "\n",
    "   AfterTrainingWithPB.featureTable(qvec, s\"$fullPath/table=CIDSMATRIXNORMS_dssm.kryo\")\n",
    "   println(s\"$fullPath/table=CIDSMATRIXNORMS_dssm.kryo is finished ${qvec.size} \")\n",
    "   log.info(s\"$fullPath/table=CIDSMATRIXNORMS_dssm.kryo is finished ${qvec.size} \")\n",
    "\n",
    " }\n",
    "}\n",
    "\n",
    "\n",
    "object Debuger {\n",
    " System.setProperty(\"db\", \"mogli2tool\")\n",
    " val spark = Spark.spark\n",
    "\n",
    " import spark.implicits._\n",
    "\n",
    " spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    " def main(args: Array[String]): Unit = {\n",
    "   println(\"hello\")\n",
    "\n",
    "\n",
    "   //    val fullpath = s\"/home1/irteam/user/companyai/data/db/host=dev2.chatbot.naver.com/schema=mogli2tool/domainId=4724/dt=2018-07-26T09-11-44.845Z/nlp=kmatapidssm/model=dssm/table=CIDSMATRIXNORMS_dssm.kryo/part-1\"\n",
    "   //    Spark.spark.read.parquet(fullpath).as[(Array[String], DenseMatrix[Double], DenseVector[Double])].collect().map(x =>println(x))\n",
    "   //    val file = new File(fullpath)\n",
    "   //    val in = new FileInputStream(file)\n",
    "   //    val a = SeriDeseri.loadKryo(in)\n",
    "   //    println(a.get)\n",
    "   //    println(a.mkString(\"\\t\"))\n",
    "   //    val fullPath = s\"/user/companyai/data/db/host=dev2.chatbot.naver.com/schema=mogli2tool/domainId=5004/dt=2018-07-20T09-38-39.799Z\"\n",
    "   //    val fullPath = s\"/user/companyai/data/db/host=dev2.chatbot.naver.com/schema=mogli2tool/domainId=5004/dt=2018-07-20T09-38-39.799Z/table=QIDTAPI_tapivec\"\n",
    "   // c3v download\n",
    "   //    val checkFile = Files.exists(Paths.get(s\"$fullPath\"))\n",
    "   //    if (!checkFile) {\n",
    "   //      if (Spark.isLocal) {\n",
    "   //        import sys.process._\n",
    "   //        val cmd = s\"\"\"/bin/bash c3v2local.sh $fullPath \"\"\".!!\n",
    "   //        println(cmd)\n",
    "   //        log.info(cmd)\n",
    "   //      }\n",
    "   //    }\n",
    "   //    val tapiRawFile = spark.read.parquet(fullPath).as[(String, Array[Double])].map(x => {\n",
    "   //      println(\"--------------x._1\", x._1)\n",
    "   //      (x._1.toInt, x._2)\n",
    "   //    }).rdd\n",
    "   //    tapiRawFile.collect().map(x => {\n",
    "   //      println(\"-0-\")\n",
    "   //      println(x._1)\n",
    "   //      x\n",
    "   //    })\n",
    "   //    try {\n",
    "   //      val e = Files.exists(Paths.get(\"/home1/irteam/user/companyai/data/db/host=dev2.chatbot.naver.com/schema=mogli2tool/domainId=5004/dt=2018-07-20T05-52-18.978Z/table=QIDTAPI_tapivec\"))\n",
    "   //      println(\"e\", e)\n",
    "   //\n",
    "   //      val a=Spark.spark.read.parquet(\"home\")\n",
    "   //      println(\"ok\")\n",
    "   //    } catch {\n",
    "   //      case e: Exception =>\n",
    "   //        println(\"error\")\n",
    "   //    }\n",
    "   val p = s\"/home1/irteam/user/companyai/data/db/host=dev2.chatbot.naver.com/schema=mogli2tool/domainId=5004/dt=2018-07-20T05-52-18.978Z\"\n",
    "   val domainId = 5004\n",
    "\n",
    "   val allParam = SystemInfoStateParam(SystemInfo(), BuildEntity(None, None, None, None, Some(domainId), Some(p), \"REQUESTED\", None), \"REQUESTED\")\n",
    "   val atgcv = allParam.buildState.fileName.get\n",
    "   val fullPath = allParam.buildState.fileName.get\n",
    "   val domainId = allParam.buildState.domainId.get\n",
    "   val dtPath = fullPath.drop(fullPath.lastIndexOf('/') + 1)\n",
    "   val params = Spark.spark.read.parquet(BPaths.tablePath(\"PARAMETERS\", fullPath)).as[Parameters].collect()\n",
    "\n",
    "   val source = params.headOption.map(x => x.source).getOrElse(\"mogli2tool\")\n",
    "\n",
    "   val source = params.headOption.map(x => x.source).getOrElse(\"mogli2tool\")\n",
    "   readC3Text(bs, params.head.c3Path.get)\n",
    "\n",
    "\n",
    "   val trainingSetPreparation = readDBParquet(fullPath, domainId)\n",
    "\n",
    "   println(trainingSetPreparation)\n",
    "\n",
    "   val qag: QAGroup = qaGroup(trainingSetPreparation)\n",
    "   println(qag)\n",
    "   println(qag.a.take(1).length)\n",
    "   println(qag.q.take(1).length)\n",
    "   println(qag.total.take(1).length)\n",
    "\n",
    "   if (qag.a.take(1).size == 0 || qag.q.take(1).size == 0 || qag.total.take(1).size == 0) {\n",
    "     println(s\"throw IllegalStateException\")\n",
    "   } else {\n",
    "     println(s\"normal\")\n",
    "   }\n",
    "\n",
    "   qidaids(qag, fullPath, domainId)\n",
    "   val params = Spark.spark.read.parquet(BPaths.tablePath(\"PARAMETERS\", fullPath)).as[Parameters].collect()\n",
    "   val nHots = makenHot(trainingSetPreparation, params.head, domainId, fullPath, qag)\n",
    "\n",
    "   nHot2QAFlat(nHots, trainingSetPreparation, 0)\n",
    " }\n",
    "\n",
    "\n",
    " def qaGroup(sourceData: TrainingSetPreparation) = {\n",
    "   val joinedQ = sourceData.CHATUTTERANCESET.withColumnRenamed(\"id\", \"gid\").join(sourceData.CHATCONTENT.withColumnRenamed(\"id\", \"qid\"), sourceData.CHATUTTERANCESET(\"chatQSessionId\") === sourceData.CHATCONTENT(\"chatsessionId\"), \"left\")\n",
    "   val filteredChatcontentA = sourceData.CHATCONTENT.select(sourceData.CHATCONTENT(\"id\").as(\"aid\"), sourceData.CHATCONTENT(\"chatsessionId\"), sourceData.CHATCONTENT(\"chat\").as(\"achat\"))\n",
    "   val joinedA = sourceData.CHATUTTERANCESET.withColumnRenamed(\"id\", \"gid\").join(filteredChatcontentA, sourceData.CHATUTTERANCESET(\"chatASessionId\") === filteredChatcontentA(\"chatsessionId\"), \"left\")\n",
    "\n",
    "   val filteredJoinA = joinedA.select(joinedA(\"gid\"), joinedA(\"aid\"))\n",
    "   val groupedA = filteredJoinA.groupBy(filteredJoinA(\"gid\")).agg(collect_list(filteredJoinA(\"aid\")).as(\"aids\"))\n",
    "   val filteredJoinQ = joinedQ.select(joinedQ(\"gid\"), joinedQ(\"qid\"))\n",
    "   val groupedQ = filteredJoinQ.groupBy(filteredJoinQ(\"gid\")).agg(collect_list(filteredJoinQ(\"qid\")).as(\"qids\"))\n",
    "\n",
    "   val total = groupedQ.join(groupedA, \"gid\")\n",
    "   val totalFiltered = total.where(size(total(\"qids\")) > 0 && size(total(\"aids\")) > 0)\n",
    "   val totalFilteredTarget = total.where(size(total(\"qids\")) <= 0 || size(total(\"aids\")) <= 0)\n",
    "   val removeTargetsQ = totalFilteredTarget.select(explode(totalFilteredTarget(\"qids\")).as(\"qid\"))\n",
    "   val removeTargetsA = totalFilteredTarget.select(explode(totalFilteredTarget(\"aids\")).as(\"aid\"))\n",
    "\n",
    "   val removedQ = joinedQ.where(!joinedQ(\"qid\").isin(removeTargetsQ.collect().map(x => x.getAs[Long](\"qid\")): _*))\n",
    "   val removedA = joinedA.where(!joinedA(\"aid\").isin(removeTargetsA.collect().map(x => x.getAs[Long](\"aid\")): _*))\n",
    "\n",
    "   val gidQidsAids = totalFiltered.select(concat(total(\"gid\"), lit(\"\\u241D\"), concat_ws(\"\\u241E\", total(\"qids\")), lit(\"\\u241D\"), concat_ws(\"\\u241E\", total(\"aids\"))))\n",
    "   QAGroup(gidQidsAids, totalFiltered, remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msc.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mvalues\u001b[39m: \u001b[32mList\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m((\u001b[32m\"hi\"\u001b[39m, \u001b[32m\"2018-07-05\"\u001b[39m, \u001b[32m\"2018-08-05\"\u001b[39m))\n",
       "\u001b[36mdataFrame\u001b[39m: \u001b[32mDataFrame\u001b[39m = [user_id: string, join_date: string ... 1 more field]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val values = List((\"hi\", \"2018-07-05\", \"2018-08-05\"))\n",
    "val dataFrame = values.toDF(\"user_id\", \"join_date\", \"event_date\")\n",
    "\n",
    "val properties = new Properties()\n",
    "properties.put(\"user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd27.sc:2: value sparkSession is not a member of org.apache.spark.sql.SparkSession\n",
      "val res27_1 = sparkSession.sparkSession\n",
      "                           ^"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "// import org.apache.spark.SparkContext\n",
    "// import org.apache.spark.SparkConf\n",
    "\n",
    "val sc = sparkSession.sparkContext\n",
    "// sparkSession.sparkSession\n",
    "// import ai.company.cai.khan.utils.Config\n",
    "// import com.soundcloud.lsh.Lsh\n",
    "// import org.apache.spark.SparkContext\n",
    "// import org.apache.spark.mllib.clustering.KMeans\n",
    "// import org.apache.spark.mllib.linalg.Vectors\n",
    "// import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix}\n",
    "// import org.apache.spark.rdd.RDD\n",
    "// import org.apache.spark.sql.{Dataset, SparkSession}\n",
    "// import org.apache.spark.storage.StorageLevel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mInt\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m)\n",
       "\u001b[36mdistData\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[3] at parallelize at cmd39.sc:2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Array(1, 2, 3, 4, 5)\n",
    "val distData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres40\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[3] at parallelize at cmd39.sc:2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres29\u001b[39m: \u001b[32mClass\u001b[39m[\u001b[32m?0\u001b[39m] = class [I"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.getClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[2] at parallelize at cmd38.sc:1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = sc.parallelize(1 to 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msparkSession.implicits._\u001b[39m"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparkSession.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres36\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mInt\u001b[39m] = [value: int]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkSession.createDataset(Seq(1,2))\n",
    "\n",
    "// val rows = spark.sparkContext.parallelize(Seq(Row.fromSeq(Seq(1, 2))))\n",
    "// val schema = StructType(Seq(\"col1\", \"col2\").map(col => StructField(col, IntegerType, nullable = false)))\n",
    "// spark.createDataFrame(rows, schema).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres37\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mInt\u001b[39m] = [value: int]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector값 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.pom.sha1\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/commons-math3-3.2.pom.sha1\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/commons-math3-3.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/commons-math3-3.2.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/commons-math3-3.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-parent/1.7.5/slf4j-parent-1.7.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-parent/1.7.5/slf4j-parent-1.7.5.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-parent/1.7.5/slf4j-parent-1.7.5.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-parent/1.7.5/slf4j-parent-1.7.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-parent/1.7.5/\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-parent/1.7.5/\n",
      "Downloading https://repo1.maven.org/maven2/junit/junit/4.8.2/junit-4.8.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/junit/junit/4.8.2/junit-4.8.2.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/junit/junit/4.8.2/junit-4.8.2.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/junit/junit/4.8.2/junit-4.8.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/junit/junit/4.8.2/\n",
      "Downloaded https://repo1.maven.org/maven2/junit/junit/4.8.2/\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/commons-math3-3.2.jar\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/commons-math3-3.2.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/junit/junit/4.8.2/junit-4.8.2.jar\n",
      "Downloading https://repo1.maven.org/maven2/junit/junit/4.8.2/junit-4.8.2.jar.sha1\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/commons-math3-3.2.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/junit/junit/4.8.2/junit-4.8.2.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar\n",
      "https://repo1.maven.org/maven2/junit/junit/4.8.2/junit-4.8.2.j… (82.61 %, 19606…\n",
      "https://repo1.maven.org/maven2/org/apache/commons/commons-math… (19.33 %, 32714…\n",
      "\n",
      "Downloaded https://repo1.maven.org/maven2/junit/junit/4.8.2/junit-4.8.2.jar\n",
      "https://repo1.maven.org/maven2/org/apache/commons/commons-math… (35.78 %, 60566…\n",
      "\n",
      "https://repo1.maven.org/maven2/org/apache/commons/commons-math… (57.07 %, 96611…\n",
      "\n",
      "https://repo1.maven.org/maven2/org/apache/commons/commons-math… (80.30 %, 13593…\n",
      "\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/commons-math3-3.2.jar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                             // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.scalanlp::breeze:0.13.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mbreeze.linalg.{DenseVector, norm}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.nio.{FloatBuffer, IntBuffer}\u001b[39m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Random\n",
    "import breeze.linalg.{DenseVector, norm}\n",
    "import java.nio.{FloatBuffer, IntBuffer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mvec\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mArray\u001b[39m[\u001b[32mFloat\u001b[39m]] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    \u001b[32m0.0011953712F\u001b[39m,\n",
       "    \u001b[32m0.8542268F\u001b[39m,\n",
       "    \u001b[32m0.7088155F\u001b[39m,\n",
       "    \u001b[32m0.5209682F\u001b[39m,\n",
       "    \u001b[32m0.41408342F\u001b[39m,\n",
       "    \u001b[32m0.019472957F\u001b[39m,\n",
       "    \u001b[32m0.4475488F\u001b[39m,\n",
       "    \u001b[32m0.58119166F\u001b[39m,\n",
       "    \u001b[32m0.14844549F\u001b[39m,\n",
       "    \u001b[32m0.74071723F\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mres24_1\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mq_vec\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mArray\u001b[39m[\u001b[32mFloat\u001b[39m]] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    \u001b[32m0.0011953712F\u001b[39m,\n",
       "    \u001b[32m0.8542268F\u001b[39m,\n",
       "    \u001b[32m0.7088155F\u001b[39m,\n",
       "    \u001b[32m0.5209682F\u001b[39m,\n",
       "    \u001b[32m0.41408342F\u001b[39m,\n",
       "    \u001b[32m0.019472957F\u001b[39m,\n",
       "    \u001b[32m0.4475488F\u001b[39m,\n",
       "    \u001b[32m0.58119166F\u001b[39m,\n",
       "    \u001b[32m0.14844549F\u001b[39m,\n",
       "    \u001b[32m0.74071723F\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36msim_vec\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mArray\u001b[39m[\u001b[32mFloat\u001b[39m]] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[33mArray\u001b[39m(\n",
       "    \u001b[32m0.0011953712F\u001b[39m,\n",
       "    \u001b[32m0.8542268F\u001b[39m,\n",
       "    \u001b[32m0.7088155F\u001b[39m,\n",
       "    \u001b[32m0.5209682F\u001b[39m,\n",
       "    \u001b[32m0.41408342F\u001b[39m,\n",
       "    \u001b[32m0.019472957F\u001b[39m,\n",
       "    \u001b[32m0.4475488F\u001b[39m,\n",
       "    \u001b[32m0.58119166F\u001b[39m,\n",
       "    \u001b[32m0.14844549F\u001b[39m,\n",
       "    \u001b[32m0.74071723F\u001b[39m,\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vec = (1 to 10).map(i => {\n",
    "  val vec_emb = (1 to 128).map(j => {\n",
    "    Random.nextFloat()\n",
    "  })\n",
    "  vec_emb.toArray\n",
    "}).toArray\n",
    "\n",
    "vec.size //size 구하기\n",
    "val q_vec = vec\n",
    "val sim_vec = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mq_flatten\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m0.26099400189356137\u001b[39m,\n",
       "  \u001b[32m0.05614817000656136\u001b[39m,\n",
       "  \u001b[32m0.4897788341840936\u001b[39m,\n",
       "  \u001b[32m0.4618345327224451\u001b[39m,\n",
       "  \u001b[32m0.6519115452165767\u001b[39m,\n",
       "  \u001b[32m0.5937805677791962\u001b[39m,\n",
       "  \u001b[32m0.2540285198353831\u001b[39m,\n",
       "  \u001b[32m0.2106753505411656\u001b[39m,\n",
       "  \u001b[32m0.40865374862968884\u001b[39m,\n",
       "  \u001b[32m0.6296643317233559\u001b[39m,\n",
       "  \u001b[32m0.846535678963581\u001b[39m,\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36msim_flatten\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m0.26099400189356137\u001b[39m,\n",
       "  \u001b[32m0.05614817000656136\u001b[39m,\n",
       "  \u001b[32m0.4897788341840936\u001b[39m,\n",
       "  \u001b[32m0.4618345327224451\u001b[39m,\n",
       "  \u001b[32m0.6519115452165767\u001b[39m,\n",
       "  \u001b[32m0.5937805677791962\u001b[39m,\n",
       "  \u001b[32m0.2540285198353831\u001b[39m,\n",
       "  \u001b[32m0.2106753505411656\u001b[39m,\n",
       "  \u001b[32m0.40865374862968884\u001b[39m,\n",
       "  \u001b[32m0.6296643317233559\u001b[39m,\n",
       "  \u001b[32m0.846535678963581\u001b[39m,\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val q_flatten = q_vec.flatten\n",
    "val sim_flatten = sim_vec.flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36msimliarity\u001b[39m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simliarity(userVector: Array[Double], answerVector: Array[Double]) = {\n",
    "  // cos sim with simple test\n",
    "\n",
    "  val q_dense = DenseVector(userVector)\n",
    "  val sim_q_dense = DenseVector(answerVector)\n",
    "\n",
    "  val cos_top = q_dense dot sim_q_dense\n",
    "  val cos_bottom = norm(q_dense) * norm(sim_q_dense)\n",
    "\n",
    "  val cos_sim = cos_top / cos_bottom\n",
    "\n",
    "  cos_sim\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres21\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m1.0000000000000004\u001b[39m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simliarity(q_flatten, sim_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres23\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m0.26099400189356137\u001b[39m,\n",
       "  \u001b[32m0.05614817000656136\u001b[39m,\n",
       "  \u001b[32m0.4897788341840936\u001b[39m,\n",
       "  \u001b[32m0.4618345327224451\u001b[39m,\n",
       "  \u001b[32m0.6519115452165767\u001b[39m,\n",
       "  \u001b[32m0.5937805677791962\u001b[39m,\n",
       "  \u001b[32m0.2540285198353831\u001b[39m,\n",
       "  \u001b[32m0.2106753505411656\u001b[39m,\n",
       "  \u001b[32m0.40865374862968884\u001b[39m,\n",
       "  \u001b[32m0.6296643317233559\u001b[39m,\n",
       "  \u001b[32m0.846535678963581\u001b[39m,\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_vec.flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mshape\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mLong\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m1L\u001b[39m, \u001b[32m10L\u001b[39m, \u001b[32m128L\u001b[39m)\n",
       "\u001b[36mids\u001b[39m: \u001b[32mFloatBuffer\u001b[39m = java.nio.HeapFloatBuffer[pos=0 lim=1280 cap=1280]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val shape = Array[Long](1, 10, 128)\n",
    "val ids = FloatBuffer.wrap(q_vec.flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36minputLayer\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"q_input\"\u001b[39m\n",
       "\u001b[36ma\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mString\u001b[39m, (\u001b[32mArray\u001b[39m[\u001b[32mLong\u001b[39m], \u001b[32mFloatBuffer\u001b[39m))] = \u001b[33mList\u001b[39m(\n",
       "  (\n",
       "    \u001b[32m\"q_input\"\u001b[39m,\n",
       "    (\u001b[33mArray\u001b[39m(\u001b[32m1L\u001b[39m, \u001b[32m10L\u001b[39m, \u001b[32m128L\u001b[39m), java.nio.HeapFloatBuffer[pos=0 lim=1280 cap=1280])\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputLayer = \"q_input\"\n",
    "val a = Seq(inputLayer -> (shape, ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mshapeForSizing\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mLong\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m1L\u001b[39m)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val shapeForSizing: Array[Long] = Array[Long](1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
